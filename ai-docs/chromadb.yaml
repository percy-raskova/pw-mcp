# ChromaDB Reference for pw-mcp
# Source: Context7 research 2024-12, updated 2025-12
# Purpose: Token-efficient AI reference for vector DB operations
# Embedding: OpenAI text-embedding-3-large (1536 dimensions)

# =============================================================================
# CLIENT SETUP
# =============================================================================

client:
  persistent:
    code: |
      import chromadb
      client = chromadb.PersistentClient(path="./chroma_data")
    notes:
      - data survives process restarts
      - path relative to working directory
      - not for production (use HttpClient for scale)

  max_batch_size:
    access: client.max_batch_size
    typical: 5000  # pw-mcp default in ChromaDBConfig
    use: chunk large ingestions

# =============================================================================
# COLLECTION CONFIGURATION
# =============================================================================

collection:
  create:
    code: |
      collection = client.create_collection(
          name="prolewiki",
          metadata={"hnsw:space": "cosine"},
          configuration={
              "hnsw": {
                  "space": "cosine",        # distance metric
                  "ef_construction": 200,   # index build quality (higher=better recall, slower build)
                  "ef_search": 100,         # search quality (higher=better recall, slower query)
              }
          }
      )

  get_or_create:
    code: |
      collection = client.get_or_create_collection(
          name="prolewiki",
          metadata={"hnsw:space": "cosine"}
      )
    notes: idempotent, safe for repeated runs

  distance_metrics:
    # -------------------------------------------------------------------------
    # IMPORTANT: For OpenAI embeddings (unit-normalized), all three metrics
    # produce IDENTICAL rankings. Choice is about interpretability/convention.
    # See chromadb-schema.yaml for full rationale.
    # -------------------------------------------------------------------------
    cosine:
      config: {"hnsw:space": "cosine"}
      formula: 1 - cos(θ)
      range: 0-2 (0=identical, 1=orthogonal, 2=opposite)
      use: semantic similarity (normalized vectors)
      recommended: true  # OpenAI recommends, NLP standard
      interpretation: |
        0.0 = identical direction
        0.3 = very similar (~85% cosine similarity)
        0.5 = moderately similar (~75%)
        1.0 = unrelated (orthogonal)
    l2:
      config: {"hnsw:space": "l2"}
      formula: √Σ(aᵢ - bᵢ)²
      range: 0-2 (for unit vectors)
      use: euclidean distance
      notes: equivalent ranking to cosine for normalized vectors, less intuitive values
    ip:
      config: {"hnsw:space": "ip"}
      formula: -Σ(aᵢ × bᵢ)  # ChromaDB negates for "smaller=closer"
      range: -1 to 1
      use: inner product (dot product)
      notes: equivalent ranking, but negative values are confusing

  hnsw_tuning:
    ef_construction:
      default: 100
      higher: better recall, slower index build
      recommendation: 200 for quality corpus
    ef_search:
      default: 100
      higher: better recall, slower queries
      tunable: post-creation via rebuild
    m:
      default: 16
      purpose: connections per node in graph
      higher: better recall, more memory

# =============================================================================
# DATA OPERATIONS
# =============================================================================

add:
  with_precomputed_embeddings:
    code: |
      collection.add(
          ids=["chunk_001", "chunk_002"],
          embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...]],  # 1536-dim for text-embedding-3-large
          documents=["text content 1", "text content 2"],
          metadatas=[
              {"article": "Five-Year Plans", "namespace": "Main", "section": "Overview"},
              {"article": "Five-Year Plans", "namespace": "Main", "section": "Implementation"}
          ]
      )
    notes:
      - ids must be unique strings
      - embeddings dimensions must match collection
      - documents stored for retrieval (optional if only doing vector search)

  auto_embed:
    code: |
      # If collection has embedding_function configured
      collection.add(
          ids=["id1"],
          documents=["text to embed automatically"]
      )
    notes: slower, requires embedding function on collection

upsert:
  code: |
    collection.upsert(
        ids=["chunk_001"],
        embeddings=[[...]],
        documents=["updated content"],
        metadatas=[{"article": "Updated Title"}]
    )
  notes: creates if not exists, updates if exists

update:
  code: |
    collection.update(
        ids=["chunk_001"],
        metadatas=[{"section": "New Section"}]
    )
  notes: only updates specified fields

delete:
  by_ids:
    code: collection.delete(ids=["chunk_001", "chunk_002"])
  by_filter:
    code: 'collection.delete(where={"namespace": "ProleWiki"})'

# =============================================================================
# QUERYING
# =============================================================================

query:
  semantic_search:
    code: |
      results = collection.query(
          query_embeddings=[[0.1, 0.2, ...]],  # precomputed query embedding
          n_results=10,
          include=["documents", "metadatas", "distances"]
      )
    returns:
      ids: list of matched chunk IDs
      documents: original text content
      metadatas: associated metadata dicts
      distances: similarity scores (lower=closer for cosine)

  with_text:
    code: |
      # Requires embedding function on collection
      results = collection.query(
          query_texts=["What were the Five-Year Plans?"],
          n_results=5
      )

get:
  by_ids:
    code: |
      results = collection.get(
          ids=["chunk_001", "chunk_002"],
          include=["documents", "metadatas", "embeddings"]
      )

  with_filter:
    code: |
      results = collection.get(
          where={"namespace": "Main"},
          limit=100,
          offset=0
      )

  pagination:
    code: |
      batch_size = 100
      for offset in range(0, collection.count(), batch_size):
          batch = collection.get(
              limit=batch_size,
              offset=offset,
              include=["documents", "metadatas"]
          )

# =============================================================================
# METADATA FILTERING
# =============================================================================

operators:
  comparison:
    $eq: {"field": {"$eq": "value"}}      # equals (default, can omit)
    $ne: {"field": {"$ne": "value"}}      # not equals
    $gt: {"count": {"$gt": 10}}           # greater than (numbers only)
    $gte: {"count": {"$gte": 10}}         # greater than or equal
    $lt: {"count": {"$lt": 10}}           # less than
    $lte: {"count": {"$lte": 10}}         # less than or equal

  set_membership:
    $in: {"namespace": {"$in": ["Main", "Library"]}}
    $nin: {"namespace": {"$nin": ["ProleWiki"]}}

  string:
    $contains: {"$contains": "keyword"}   # document content only via where_document
    notes: metadata string search not yet supported

  logical:
    $and:
      code: |
        {"$and": [
            {"namespace": "Main"},
            {"chunk_index": {"$gte": 0}}
        ]}
    $or:
      code: |
        {"$or": [
            {"namespace": "Main"},
            {"namespace": "Library"}
        ]}
    nested:
      code: |
        {"$and": [
            {"$or": [
                {"namespace": "Main"},
                {"namespace": "Essays"}
            ]},
            {"has_refs": True}
        ]}

document_filtering:
  where_document:
    code: |
      collection.query(
          query_embeddings=[[...]],
          where_document={"$contains": "Stalin"},
          n_results=10
      )
    operators:
      - $contains
      - $not_contains
      - $regex (FTS)

combined_filtering:
  code: |
    collection.query(
        query_embeddings=[[...]],
        where={"namespace": "Main"},
        where_document={"$contains": "industrialization"},
        n_results=10
    )

# =============================================================================
# OPENAI EMBEDDING INTEGRATION
# =============================================================================

openai:
  embedding_function:
    code: |
      from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

      openai_ef = OpenAIEmbeddingFunction(
          api_key=os.getenv("OPENAI_API_KEY"),
          model_name="text-embedding-3-large",  # 1536-dim (configurable 256-3072)
      )

      # Generate embeddings directly
      embeddings = openai_ef(["text to embed", "another text"])

  with_collection:
    code: |
      collection = client.create_collection(
          name="prolewiki",
          embedding_function=openai_ef,
          metadata={"hnsw:space": "cosine"}
      )
      # Now add() can auto-embed documents
      collection.add(ids=["id1"], documents=["auto-embedded text"])

  custom_function:
    code: |
      from chromadb import Documents, EmbeddingFunction, Embeddings
      from openai import OpenAI

      class OpenAIEmbedder(EmbeddingFunction):
          def __init__(self, model: str = "text-embedding-3-large", dimensions: int = 1536):
              self.model = model
              self.dimensions = dimensions
              self.client = OpenAI()  # Uses OPENAI_API_KEY env var

          def __call__(self, input: Documents) -> Embeddings:
              response = self.client.embeddings.create(
                  model=self.model,
                  input=list(input),
                  dimensions=self.dimensions,
              )
              return [item.embedding for item in response.data]

# =============================================================================
# LEGACY: OLLAMA EMBEDDING (for local development)
# =============================================================================
# NOTE: Production uses OpenAI. Ollama available for local testing.

ollama_legacy:
  status: DEPRECATED - use OpenAI for production
  embedding_function:
    code: |
      from chromadb.utils.embedding_functions.ollama_embedding_function import (
          OllamaEmbeddingFunction,
      )

      ollama_ef = OllamaEmbeddingFunction(
          url="http://localhost:11434",
          model_name="embeddinggemma",  # 768-dim
      )

# =============================================================================
# BATCH OPERATIONS
# =============================================================================

batching:
  utility:
    code: |
      from chromadb.utils.batch_utils import create_batches

      batches = create_batches(
          api=client,
          ids=all_ids,
          documents=all_docs,
          embeddings=all_embeddings,
          metadatas=all_metadatas
      )

      for batch in batches:
          collection.add(
              ids=batch[0],
              embeddings=batch[1],
              metadatas=batch[2],
              documents=batch[3]
          )

  manual:
    code: |
      BATCH_SIZE = 5000  # below max_batch_size
      for i in range(0, len(chunks), BATCH_SIZE):
          batch = chunks[i:i + BATCH_SIZE]
          collection.add(
              ids=[c.id for c in batch],
              embeddings=[c.embedding for c in batch],
              documents=[c.text for c in batch],
              metadatas=[c.metadata for c in batch]
          )

  performance_debugging:
    code: |
      import time

      # Measure embedding time separately
      start = time.perf_counter()
      embeddings = openai_ef(documents)
      embed_time = time.perf_counter() - start

      # Measure add time
      start = time.perf_counter()
      collection.add(ids=ids, embeddings=embeddings, documents=documents)
      add_time = time.perf_counter() - start

      print(f"Embed: {embed_time:.2f}s, Add: {add_time:.2f}s")

# =============================================================================
# PROLEWIKI METADATA SCHEMA (MVP - Phase A)
# =============================================================================
# These 13 fields are output by chunker.py and ready for ChromaDB loading.

pw_schema:
  embedding:
    provider: openai
    model: text-embedding-3-large
    dimensions: 1536

  chunk_metadata:
    # --- Core identification ---
    chunk_id:
      type: str
      format: "{namespace}/{article_title}#{chunk_index}"
      example: "Main/Five-Year_Plans#0"
      purpose: ChromaDB document ID (unique, deterministic)

    text:
      type: str
      purpose: chunk content (ChromaDB "document" field, gets embedded)
      max_tokens: ~1000

    article_title:
      type: str
      example: "Five-Year Plans"
      filterable: true
      purpose: group chunks by source article

    namespace:
      type: str
      values: ["Main", "Library", "Essays", "ProleWiki"]
      filterable: true
      purpose: filter by content type

    section:
      type: str | null
      example: "Implementation"
      source: '"== Header ==" in MediaWiki'
      purpose: sub-article navigation

    chunk_index:
      type: int
      example: 0
      filterable: true
      purpose: ordering chunks within article

    line_range:
      type: str  # stored as "start-end"
      example: "45-52"
      purpose: precise citation back to source text
      notes: tuple not supported in metadata, use string

    word_count:
      type: int
      example: 342
      filterable: true
      purpose: filter by content density

    # --- Article-level metadata (same for all chunks in article) ---
    categories:
      type: list[str]
      storage: native list (ChromaDB supports)
      example: '["Soviet economy", "Stalin era"]'
      purpose: topic classification from [[Category:...]]

    internal_links:
      type: list[str]
      storage: native list
      example: '["Joseph Stalin", "USSR"]'
      purpose: wiki-style interlinking

    # --- Quality flags ---
    is_stub:
      type: bool
      example: false
      filterable: true
      purpose: article marked as incomplete ({{Stub}} template)
      query_pattern: |
        # Exclude stubs from results
        where={"is_stub": False}

    citation_needed_count:
      type: int
      example: 0
      filterable: true
      purpose: number of {{Citation needed}} markers
      query_pattern: |
        # Find well-cited articles
        where={"citation_needed_count": {"$eq": 0}}

    has_blockquote:
      type: bool
      example: true
      filterable: true
      purpose: chunk contains <blockquote> (often primary source)

  id_format:
    pattern: "{namespace}/{article_title}#{chunk_index}"
    examples:
      - "Main/Five-Year_Plans#0"
      - "Library/Capital_Vol1#127"
      - "Essays/On_Imperialism#3"
    notes: URL-safe (spaces → underscores), deterministic for upsert

  # --- FUTURE ENHANCEMENTS (not in MVP) ---
  future_fields:
    source_file:
      status: PLANNED
      type: str
      example: "Main/Five-Year Plans.txt"
      purpose: provenance tracking back to source file

    has_refs:
      status: PLANNED
      type: bool
      purpose: filter scholarly content with citations
      notes: requires linking refs to specific chunks

    backlinks:
      status: DEFERRED
      type: list[str]
      purpose: articles that link TO this article
      notes: requires graph computation pass

# =============================================================================
# Q&A TRAINING RECORD SCHEMA (aligned with pw_schema)
# =============================================================================
# JSON Schema: training_data/schema/qa_record.schema.json
# Purpose: Training data for GRPO fine-tuning, linked to RAG corpus

qa_schema:
  description: |
    Q&A training records share field semantics with pw_schema chunk metadata.
    This enables bidirectional linking: Q&A records can reference source chunks,
    and training provenance is traceable back to the ProleWiki corpus.

  id_format:
    pattern: "{namespace}/{title}#{index}"
    examples:
      - "Library/Lenin_Revisionism#001"
      - "Library/Jackson_Blood_My_Eye#015"
      - "Training/Synthetic_Antisemitism#042"
    notes: same format as pw_schema.chunk_metadata.chunk_id

  shared_fields:
    # These fields have identical semantics to pw_schema.chunk_metadata
    namespace:
      values: ["Main", "Library", "Essays", "ProleWiki", "Training"]
      notes: Training namespace added for synthetic data
    categories:
      type: list[str]
      purpose: topic tags (should align with ProleWiki categories)
    is_stub:
      type: bool
      purpose: source marked as incomplete
    citation_needed_count:
      type: int
      purpose: number of claims needing citation
    has_blockquote:
      type: bool
      purpose: contains primary source quotation
    line_range:
      type: str
      format: "start-end"
      purpose: precise citation back to source file

  training_specific_fields:
    source:
      author: primary author of source material
      work: full title of source work
      chunk_ids: list of ChromaDB chunk IDs this Q&A was derived from
      section: section within the source work
    classification:
      tradition: ["ML", "MLM", "general", "contested"]
      subtradition: specific tendency (Hoxhaist, Third-Worldist, etc.)
      historical_period: Russian Revolution, Cultural Revolution, etc.
      geographic_focus: Soviet Union, China, Palestine, etc.
    quality:
      human_verified: bool
      confidence: ["high", "medium", "low"]
      notes: quality issues or caveats
    provenance:
      created_date: ISO 8601 date
      created_by: ["human", "claude-opus", "claude-sonnet", "other-llm", "automated"]
      version: record version number
    training:
      iteration: which training iteration added this record
      correction_for: failure modes this corrects (hallucination, both-sidesing, etc.)
      difficulty: ["basic", "intermediate", "advanced", "adversarial"]
      response_style: ["educational", "firm-rejection", "theoretical", etc.]

  example_record:
    code: |
      {
        "qa_id": "Library/Lenin_Revisionism#001",
        "instruction": "What is revisionism in the Marxist sense?",
        "response": "Revisionism refers to attempts to revise...",
        "source": {
          "namespace": "Library",
          "article_title": "Marxism and Revisionism",
          "author": "Vladimir Lenin",
          "chunk_ids": ["Library/Lenin_Revisionism#14", "Library/Lenin_Revisionism#15"],
          "line_range": "45-52"
        },
        "classification": {
          "categories": ["revisionism", "marxist-theory", "leninism"],
          "tradition": "ML",
          "historical_period": "Second International"
        },
        "quality": {
          "has_blockquote": true,
          "human_verified": true,
          "confidence": "high"
        },
        "provenance": {
          "created_date": "2025-12-17",
          "created_by": "claude-opus",
          "version": 1
        }
      }

# =============================================================================
# INTERLINKING PATTERNS (WIKI-STYLE)
# =============================================================================

interlinking:
  concept: |
    ProleWiki articles link to each other via [[Internal Links]].
    Store these relationships in metadata to enable:
    1. "See also" recommendations
    2. Graph traversal queries
    3. Related content discovery

  store_outbound_links:
    code: |
      # Extract [[links]] during parsing
      links = ["Joseph Stalin", "USSR", "Industrialization"]

      collection.add(
          ids=["Main/Five-Year_Plans#0"],
          metadatas=[{
              "article_title": "Five-Year Plans",
              "internal_links": links  # native list (ChromaDB supports)
          }]
      )

  find_related:
    code: |
      # Find all chunks that link TO a specific article
      # Note: requires searching document text or post-filtering
      results = collection.get(
          where_document={"$contains": "Joseph Stalin"}
      )

  # --- FUTURE ENHANCEMENT ---
  build_link_index:
    status: DEFERRED
    strategy: |
      Maintain separate collection or external index:
      1. Parse all articles for [[links]]
      2. Build adjacency list: article -> [linked_articles]
      3. Invert: article -> [articles_linking_here]
      4. Store backlinks in metadata for fast lookup

    code: |
      # During ingestion, track bidirectional links
      links_to = defaultdict(set)    # article -> articles it links to
      links_from = defaultdict(set)  # article -> articles that link to it

      for article in articles:
          for link in article.internal_links:
              links_to[article.title].add(link)
              links_from[link].add(article.title)

      # Add backlinks to metadata (future enhancement)
      metadata["backlinks"] = list(links_from[article.title])

  category_graph:
    concept: |
      Categories form a hierarchy/graph.
      Store category relationships for faceted search.

    code: |
      # Category membership stored as native list
      metadatas=[{
          "categories": ["Soviet history", "Economic policy"],
      }]

      # Query by category requires document search
      collection.query(
          query_embeddings=[...],
          where_document={"$contains": "Soviet history"}
      )

# =============================================================================
# MAINTENANCE
# =============================================================================

maintenance:
  count:
    code: collection.count()

  rebuild_hnsw:
    command: chops hnsw rebuild ./chroma_data --collection prolewiki
    options:
      - "--construction-ef 200"
      - "--search-ef 100"
      - "--m 32"

  copy_collection:
    code: |
      source = client.get_collection("prolewiki")
      target = client.get_or_create_collection("prolewiki_backup")

      batch_size = 1000
      for i in range(0, source.count(), batch_size):
          batch = source.get(
              limit=batch_size,
              offset=i,
              include=["documents", "metadatas", "embeddings"]
          )
          target.add(
              ids=batch["ids"],
              documents=batch["documents"],
              metadatas=batch["metadatas"],
              embeddings=batch["embeddings"]
          )
