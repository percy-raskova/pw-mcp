# Embedding Documentation - Phase 5
# Purpose: Vector embedding generation for semantic search
# Status: IMPLEMENTED - Phase 5 COMPLETE (OpenAI primary, Ollama fallback)
# Dependencies: Phase 4 (chunks/*.jsonl)
# Last updated: 2025-12-16

overview:
  goal: Generate vector embeddings for all chunks
  input: chunks/{namespace}/{title}.jsonl
  output: embeddings/{namespace}/{title}.npy
  architecture: Dual-provider (OpenAI primary, Ollama fallback)
  primary_model: OpenAI text-embedding-3-large (1536-dim)
  fallback_model: Ollama embeddinggemma (768-dim)

# =============================================================================
# MODEL SPECIFICATION (DUAL-PROVIDER)
# =============================================================================

models:
  primary:
    name: text-embedding-3-large
    provider: OpenAI (API)
    dimensions: 1536
    max_tokens: 8191
    normalization: L2-normalized (unit vectors)
    cost: ~$0.13 per million tokens

    setup: |
      # Set API key in .env file
      OPENAI_API_KEY=sk-...

      # Or export directly
      export OPENAI_API_KEY=sk-...

    why_openai: |
      - Superior quality: Latest OpenAI embedding model
      - High dimensions: 1536 captures more semantic nuance
      - Proven performance: Well-benchmarked on retrieval tasks
      - Configurable: Can reduce to 256-3072 dimensions
      - Fast API: High throughput with batch support

  fallback:
    name: embeddinggemma
    provider: Ollama (local)
    dimensions: 768
    max_tokens: 2048
    normalization: L2-normalized (unit vectors)
    params: 307M (Gemma3-based)

    setup: |
      # Pull the model (one-time setup)
      ollama pull embeddinggemma

      # Verify it's available
      ollama list | grep embeddinggemma

    why_ollama_fallback: |
      - Local/private: No data leaves the machine
      - Free: No API costs
      - Offline: Works without internet connection
      - Good quality: Gemma3-based, solid semantic understanding

  selection_guidance: |
    Use OpenAI (--provider openai) when:
    - Quality is paramount (production, final corpus)
    - API costs are acceptable
    - Internet connection available

    Use Ollama (--provider ollama) when:
    - Data privacy is critical
    - Offline operation required
    - Development/testing (free, fast iteration)
    - API quota concerns

# =============================================================================
# CONFIGURATION
# =============================================================================

config:
  dataclass: EmbedConfig
  frozen: true
  fields:
    provider:
      type: str
      default: "openai"
      enum: ["openai", "ollama"]
      description: Embedding provider (openai or ollama)

    model:
      type: str
      default: "text-embedding-3-large"  # or "embeddinggemma" for Ollama
      description: Model name for embeddings (provider-specific)

    dimensions:
      type: int
      default: 1536  # OpenAI default; Ollama uses 768
      description: Expected embedding dimensions (for validation)

    batch_size:
      type: int
      default: 32
      description: Number of texts per API call
      notes: |
        OpenAI: Can handle larger batches efficiently
        Ollama: 32 is good balance for GPU memory
        Reduce if OOM errors occur

    ollama_host:
      type: str
      default: "http://localhost:11434"
      description: Ollama server URL (only used when provider=ollama)

    max_retries:
      type: int
      default: 3
      description: Retry attempts on API failure

    retry_delay:
      type: float
      default: 1.0
      description: Initial delay between retries (exponential backoff)

# =============================================================================
# DATA STRUCTURES
# =============================================================================

dataclasses:
  EmbeddedArticle:
    description: Article with generated embeddings
    fields:
      article_title:
        type: str
        description: Article title (from JSONL)

      namespace:
        type: str
        description: ProleWiki namespace

      num_chunks:
        type: int
        description: Number of chunks embedded

      embeddings:
        type: np.ndarray
        shape: "(num_chunks, dim)"  # 1536 for OpenAI, 768 for Ollama
        dtype: float32
        description: Embedding vectors in chunk order

# =============================================================================
# PUBLIC API
# =============================================================================

public_api:
  functions:
    embed_texts:
      signature: "def embed_texts(texts: list[str], config: EmbedConfig) -> np.ndarray"
      description: Embed a batch of texts using configured provider
      returns: "np.ndarray of shape (len(texts), dim)"  # 1536 OpenAI, 768 Ollama
      notes: |
        - Handles batching internally based on config.batch_size
        - Returns empty array for empty input
        - Dispatches to OpenAI or Ollama based on config.provider
        - Raises EmbeddingError if provider unavailable

    embed_article_chunks:
      signature: "def embed_article_chunks(jsonl_path: Path, config: EmbedConfig) -> EmbeddedArticle"
      description: Embed all chunks from a JSONL file
      notes: |
        - Reads JSONL, extracts "text" field from each line
        - Preserves chunk order (important for Stage 6 loading)
        - Extracts namespace from path structure

    write_embeddings_npy:
      signature: "def write_embeddings_npy(article: EmbeddedArticle, output_path: Path) -> None"
      description: Save embeddings to .npy file
      notes: |
        - Creates parent directories if needed
        - Uses np.save() with allow_pickle=False
        - Shape depends on provider (num_chunks, 1536 or 768)

    check_provider_ready:
      signature: "def check_provider_ready(config: EmbedConfig) -> bool"
      description: Pre-flight check for embedding provider
      notes: |
        - For OpenAI: Verifies API key and test embedding
        - For Ollama: Checks server and model availability
        - Verifies model returns expected dimensions
        - Returns False on any error

  exceptions:
    EmbeddingError:
      description: Base exception for embedding errors

    OpenAIError:
      description: OpenAI API error (auth, rate limit, etc.)

    OllamaConnectionError:
      description: Ollama server not reachable

    OllamaModelError:
      description: Ollama model not available or wrong dimensions

# =============================================================================
# OPENAI INTEGRATION (PRIMARY)
# =============================================================================

openai_integration:
  library: openai (Python package)
  install: "uv add openai"

  usage_pattern: |
    from openai import OpenAI

    client = OpenAI()  # Uses OPENAI_API_KEY env var

    response = client.embeddings.create(
        model="text-embedding-3-large",
        input=["text1", "text2", "text3"]  # Batch support!
    )
    embeddings = [e.embedding for e in response.data]
    # len(embeddings) == 3
    # len(embeddings[0]) == 1536

  async_pattern: |
    from openai import AsyncOpenAI

    async def embed_async(texts: list[str]) -> list[list[float]]:
        client = AsyncOpenAI()
        response = await client.embeddings.create(
            model="text-embedding-3-large",
            input=texts
        )
        return [e.embedding for e in response.data]

  dimension_reduction: |
    # OpenAI supports reducing dimensions (trade-off: speed vs quality)
    response = client.embeddings.create(
        model="text-embedding-3-large",
        input=texts,
        dimensions=768  # Reduce from 1536 to 768
    )

# =============================================================================
# OLLAMA INTEGRATION (FALLBACK)
# =============================================================================

ollama_integration:
  library: ollama (Python package)
  install: "uv add ollama"

  usage_pattern: |
    from ollama import embed

    response = embed(
        model='embeddinggemma',
        input=['text1', 'text2', 'text3']  # Batch support!
    )
    embeddings = response.embeddings  # List of lists
    # len(embeddings) == 3
    # len(embeddings[0]) == 768

  async_pattern: |
    from ollama import AsyncClient

    async def embed_async(texts: list[str]) -> list[list[float]]:
        client = AsyncClient()
        response = await client.embed(
            model='embeddinggemma',
            input=texts
        )
        return response.embeddings

  api_endpoint: |
    POST /api/embed
    {
      "model": "embeddinggemma",
      "input": ["text1", "text2", ...]
    }

    Response:
    {
      "embeddings": [[0.1, 0.2, ...], [0.3, 0.4, ...]]
    }

# =============================================================================
# OUTPUT FORMAT
# =============================================================================

output_format:
  file_structure: embeddings/{namespace}/{title}.npy

  npy_spec:
    format: NumPy binary (.npy)
    dtype: float32
    shape: "(num_chunks, dim)"  # 1536 for OpenAI, 768 for Ollama
    notes: |
      - Chunk order matches JSONL line order
      - embeddings[i] corresponds to chunk_index i
      - Can be loaded with: np.load(path)
      - Memory-mappable with: np.load(path, mmap_mode='r')
      - Dimension depends on provider used during generation

  example_openai:
    input: chunks/Main/Five-Year_Plans.jsonl (5 chunks)
    output: embeddings/Main/Five-Year_Plans.npy
    shape: (5, 1536)
    size: ~30KB (5 * 1536 * 4 bytes)

  example_ollama:
    input: chunks/Main/Five-Year_Plans.jsonl (5 chunks)
    output: embeddings/Main/Five-Year_Plans.npy
    shape: (5, 768)
    size: ~15KB (5 * 768 * 4 bytes)

  alignment_with_chunks: |
    CRITICAL: Embedding order must match JSONL line order!

    chunks/Main/Article.jsonl:
      Line 0: {"chunk_index": 0, "text": "..."}
      Line 1: {"chunk_index": 1, "text": "..."}
      Line 2: {"chunk_index": 2, "text": "..."}

    embeddings/Main/Article.npy:
      embeddings[0] = vector for chunk_index 0
      embeddings[1] = vector for chunk_index 1
      embeddings[2] = vector for chunk_index 2

    Stage 6 loading relies on this alignment!

# =============================================================================
# CLI INTEGRATION
# =============================================================================

cli:
  subcommand: embed
  description: Generate vector embeddings for chunks

  options:
    input:
      flags: [-i, --input]
      type: Path
      default: chunks/
      description: Input directory with JSONL chunk files

    output:
      flags: [-o, --output]
      type: Path
      default: embeddings/
      description: Output directory for NPY embedding files

    provider:
      flags: [--provider]
      type: str
      default: openai
      choices: [openai, ollama]
      description: Embedding provider (openai or ollama)

    sample:
      flags: [--sample]
      type: int
      default: null
      description: Process only N random files for testing

    no_progress:
      flags: [--no-progress]
      type: bool
      default: false
      description: Disable progress display

    model:
      flags: [--model]
      type: str
      default: "(provider-specific)"
      description: Model name (text-embedding-3-large or embeddinggemma)

    batch_size:
      flags: [--batch-size]
      type: int
      default: 32
      description: Chunks per API call

    host:
      flags: [--host]
      type: str
      default: http://localhost:11434
      description: Ollama server URL (only used with --provider ollama)

  usage_examples:
    # OpenAI (default, recommended)
    openai_sample: pw-ingest embed --sample 10 --provider openai
    openai_full: pw-ingest embed -i chunks/ -o embeddings/ --provider openai
    # Ollama (local fallback)
    ollama_sample: pw-ingest embed --sample 10 --provider ollama
    ollama_full: pw-ingest embed -i chunks/ -o embeddings/ --provider ollama
    # Legacy (defaults to openai)
    check_sample: pw-ingest embed --sample 10
    full_corpus: pw-ingest embed -i chunks/ -o embeddings/
    custom_batch: pw-ingest embed --batch-size 64
    remote_ollama: pw-ingest embed --provider ollama --host http://gpu-server:11434

  exit_codes:
    0: Success
    1: Error (provider not available, no files found)
    130: Keyboard interrupt

# =============================================================================
# PROCESSING LOGIC
# =============================================================================

processing:
  algorithm: |
    1. Pre-flight check: verify Ollama is running with correct model
    2. Discover all .jsonl files in input directory
    3. For each file (with progress bar):
       a. Check if corresponding .npy exists â†’ skip (resume support)
       b. Read JSONL, extract "text" field from each line
       c. Batch texts into groups of batch_size
       d. For each batch:
          - Call ollama.embed(model, input=batch)
          - Collect embedding vectors
          - Handle retries on failure
       e. Stack all embeddings into numpy array
       f. Save as .npy in output directory (preserving namespace)
    4. Report summary statistics

  resume_support: |
    If embeddings/{namespace}/{title}.npy exists:
      - Skip processing that article
      - Log "Skipping (already embedded)"

    This enables:
      - Resuming interrupted runs
      - Incremental updates (re-run after adding new chunks)

  error_handling: |
    Connection Error:
      - Fail fast with clear message: "Ollama not available at {host}"
      - Suggest: "Run 'ollama serve' or check --host parameter"

    Model Error:
      - Fail fast: "Model 'embeddinggemma' not found"
      - Suggest: "Run 'ollama pull embeddinggemma'"

    Per-Article Error:
      - Log error, skip article, continue processing
      - Report failed articles at end
      - Return exit code 1 if any failures

# =============================================================================
# TESTING STRATEGY
# =============================================================================

testing:
  structure:
    unit:
      - tests/unit/embedding/__init__.py
      - tests/unit/embedding/test_embedder.py
      - tests/unit/embedding/test_embed_cli.py
    slow:
      - tests/slow/test_embedding_slow.py
    fixtures:
      - tests/fixtures/embedding/sample_chunks.jsonl

  test_categories:
    config_tests:
      count: 4
      tests:
        - test_embed_config_defaults
        - test_embed_config_custom_values
        - test_embed_config_frozen
        - test_embed_config_validation

    embed_texts_tests:
      count: 8
      tests:
        - test_embed_texts_single
        - test_embed_texts_batch
        - test_embed_texts_batching_at_limit
        - test_embed_texts_empty_list
        - test_embed_texts_returns_numpy
        - test_embed_texts_correct_shape
        - test_embed_texts_correct_dtype
        - test_embed_texts_retry_on_failure

    embed_article_tests:
      count: 5
      tests:
        - test_embed_article_reads_jsonl
        - test_embed_article_extracts_text
        - test_embed_article_preserves_order
        - test_embed_article_extracts_metadata
        - test_embed_article_handles_empty

    write_tests:
      count: 4
      tests:
        - test_write_embeddings_creates_file
        - test_write_embeddings_correct_shape
        - test_write_embeddings_loadable
        - test_write_embeddings_creates_dirs

    cli_tests:
      count: 10
      tests:
        - test_embed_subcommand_exists
        - test_embed_input_default
        - test_embed_output_default
        - test_embed_model_default
        - test_embed_batch_size_default
        - test_embed_sample_limits_files
        - test_embed_skip_existing_npy
        - test_embed_validates_ollama
        - test_embed_progress_reports
        - test_embed_preserves_namespace

    integration_tests:
      count: 4
      marker: "@pytest.mark.slow"
      requires: Ollama running with embeddinggemma
      tests:
        - test_embed_real_chunk
        - test_embedding_dimensions
        - test_embedding_normalized
        - test_embedding_deterministic

  mock_strategy: |
    Use unittest.mock.patch to mock ollama.embed:

    @patch('pw_mcp.ingest.embedder.embed')
    def test_embed_texts(mock_embed):
        mock_embed.return_value = MagicMock(
            embeddings=[[0.1] * 768, [0.2] * 768]
        )
        result = embed_texts(["a", "b"], config)
        assert result.shape == (2, 768)
        mock_embed.assert_called_once()

  fixtures:
    sample_chunks_jsonl: |
      {"chunk_id": "Main/Test#0", "text": "First chunk text.", "chunk_index": 0}
      {"chunk_id": "Main/Test#1", "text": "Second chunk text.", "chunk_index": 1}
      {"chunk_id": "Main/Test#2", "text": "Third chunk text.", "chunk_index": 2}

# =============================================================================
# IMPLEMENTATION PHASES (TDD)
# =============================================================================

implementation:
  phase_5_1:
    name: Test Infrastructure (Red Phase)
    status: COMPLETE
    completed_date: 2025-12-15
    tasks:
      - Create tests/unit/embedding/ directory structure
      - Create test_embedder.py with 21 tests (pytest.skip)
      - Create test_embed_cli.py with 10 tests (pytest.skip)
      - Create tests/fixtures/embedding/sample_chunks.jsonl
      - Update conftest.py with 6 embedding fixtures
    deliverables:
      - tests/unit/embedding/__init__.py
      - tests/unit/embedding/test_embedder.py (21 tests)
      - tests/unit/embedding/test_embed_cli.py (10 tests)
      - tests/fixtures/embedding/sample_chunks.jsonl (5 chunks)

  phase_5_2:
    name: Core Embedder Module (Green Phase)
    status: COMPLETE
    completed_date: 2025-12-15
    tests: 21 passing
    tasks:
      - Add ollama>=0.4.0 dependency to pyproject.toml
      - Create src/pw_mcp/ingest/embedder.py
      - Implement EmbedConfig dataclass (frozen, validated)
      - Implement EmbeddedArticle dataclass
      - Implement embed_texts() with batching and retry logic
      - Implement embed_article_chunks()
      - Implement write_embeddings_npy()
      - Implement check_ollama_ready()
      - Custom exceptions (OllamaConnectionError, OllamaModelError)
      - All 21 unit tests pass (with mocked Ollama)
    deliverables:
      - src/pw_mcp/ingest/embedder.py (complete)

  phase_5_3:
    name: CLI Integration
    status: COMPLETE
    completed_date: 2025-12-15
    tests: 10 passing
    tasks:
      - Add "embed" subcommand to cli.py (7 options)
      - Implement pre-flight Ollama health check
      - Implement progress display ([n/total] format)
      - Implement resume support (skip existing .npy)
      - Preserve namespace directory structure
      - All 10 CLI tests pass
    deliverables:
      - Updated src/pw_mcp/ingest/cli.py

  phase_5_4:
    name: Integration Tests + Documentation
    status: COMPLETE
    completed_date: 2025-12-15
    tests: 4 passing (slow)
    tasks:
      - Add slow integration tests (requires live Ollama)
      - Update ai-docs/ with final status
    deliverables:
      - tests/slow/test_embedding_slow.py (4 tests)
      - tests/conftest.py (require_ollama_server fixture)
      - Updated ai-docs/ documentation

# =============================================================================
# SUCCESS CRITERIA
# =============================================================================

success_criteria:
  functionality:
    - All 35 tests pass (21 embedder + 10 CLI + 4 slow/integration)
    - Can process full corpus without errors
    - Output NPY files match chunk count
    - Resume support works correctly

  quality:
    - Pre-commit hooks pass (ruff, mypy)
    - No type errors
    - Documentation complete

  performance:
    - Full corpus embedded in < 15 minutes
    - Memory usage < 2GB during processing
    - Batch processing provides speedup over sequential

# =============================================================================
# DEPENDENCIES
# =============================================================================

dependencies:
  required:
    - Phase 4 chunking (chunks/*.jsonl files)
    - Ollama server running
    - embeddinggemma model pulled

  python_packages:
    - ollama  # Ollama Python client
    - numpy   # Already in project

  system:
    - Ollama installed and running
    - GPU recommended but CPU works

# =============================================================================
# MISE TASKS
# =============================================================================

mise_tasks:
  # OpenAI (primary)
  embed:
    description: Embed full corpus with OpenAI (recommended)
    command: uv run pw-ingest embed -i chunks/ -o embeddings/ --provider openai

  # Ollama (fallback)
  embed-ollama:
    description: Embed full corpus with local Ollama
    command: uv run pw-ingest embed -i chunks/ -o embeddings/ --provider ollama

  embed-check:
    description: Verify embedding providers are available
    command: |
      echo "OpenAI: $(python3 -c 'import os; print("Ready" if os.getenv("OPENAI_API_KEY") else "Set key")')"
      echo "Ollama: $(ollama list 2>/dev/null | grep -q embeddinggemma && echo 'Ready' || echo 'Pull model')"

  embed-sample:
    description: Test embedding with sample files (OpenAI)
    command: uv run pw-ingest embed --sample 10 --provider openai

  embed-sample-ollama:
    description: Test embedding with sample files (Ollama)
    command: uv run pw-ingest embed --sample 10 --provider ollama

# =============================================================================
# SAMPLE PIPELINE TASKS
# =============================================================================

sample_pipeline_tasks:
  sample-embed:
    description: Embed sample pipeline chunks with OpenAI
    command: uv run pw-ingest embed -i sample-pipeline/chunks -o sample-pipeline/embeddings --provider openai

  sample-embed-ollama:
    description: Embed sample pipeline chunks with Ollama (local)
    command: uv run pw-ingest embed -i sample-pipeline/chunks -o sample-pipeline/embeddings --provider ollama
