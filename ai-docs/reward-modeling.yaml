# Reward Modeling for GRPO Fine-Tuning
# Token-efficient reference for AI assistants
# Research-backed approaches to defeat reward hacking

status: IMPLEMENTED - multi-layer coherence rewards with wandb logging
purpose: Robust reward functions for Marxist-Leninist GRPO training
module: src/pw_mcp/ai_training/
files:
  grpo_rewards: Reward functions for GRPO training
  wandb_logging: Weights & Biases integration for training observability

# =============================================================================
# OVERVIEW
# =============================================================================

overview:
  goal: |
    Train DeepSeek-R1-0528-Qwen3-8B on ProleWiki Q&A corpus using GRPO
    with reward functions that cannot be gamed by "word soup" or other
    adversarial strategies.

  problem: |
    Naive reward functions (substring matching, keyword counting) are
    vulnerable to reward hacking. A model could maximize reward by
    outputting random Marxist terminology without coherent meaning.

  solution: |
    Multi-layer reward combining:
    1. NLI (Natural Language Inference) - checks logical consistency
    2. Self-consistency - checks for internal contradictions
    3. Structural coherence - checks syntactic structure via spaCy

  research_basis:
    - "arxiv.org/abs/2508.18212 - NLI as reward modeling paradigm"
    - "arxiv.org/abs/2509.22047 - MO-GRPO mitigating reward hacking"
    - "arxiv.org/abs/2508.05170 - Posterior-GRPO process rewards"

# =============================================================================
# IDEOLOGICAL BIAS CONSIDERATION
# =============================================================================

ideological_bias:
  concern: |
    NLI models trained on predominantly liberal/capitalist media might
    encode bourgeois assumptions, potentially marking valid Marxist claims
    as "contradiction" or "neutral".

  empirical_finding: |
    Testing shows BART-large-MNLI performs LOGICAL inference, not
    ideological judgment:
    - "Capitalism exploits workers" → "Workers are exploited": entailment (0.998)
    - "Class struggle drives history" vs "Great individuals drive history": contradiction (0.998)
    - Word soup → coherent claim: neutral (0.932)

  why_it_works: |
    We compare Marxist response against Marxist ground truth (from ProleWiki).
    The model isn't judging if Marxism is "true" - it's checking if the
    response logically follows from the expected answer.

  mitigation_strategy: |
    Self-consistency check avoids external ideology entirely - it only
    checks if the response contradicts ITSELF, not external "truth".

# =============================================================================
# REWARD FUNCTIONS
# =============================================================================

reward_functions:

  # ---------------------------------------------------------------------------
  # FORMAT REWARDS (from original notebook)
  # ---------------------------------------------------------------------------

  match_format_exactly:
    purpose: Encourage proper <think>...</think> format
    scoring:
      has_think_end_tag: "+3.0"
      no_tag: "0.0"
    notes: Checks for </think> tag presence

  match_format_approximately:
    purpose: Reward partial format compliance
    scoring:
      one_think_start: "+0.5"
      one_think_end: "+0.5"
      multiple_or_missing: "-1.0"
    notes: Penalizes malformed tag structure

  # ---------------------------------------------------------------------------
  # SEMANTIC REWARDS
  # ---------------------------------------------------------------------------

  semantic_similarity_reward:
    purpose: Reward responses semantically similar to ground truth
    model: sentence-transformers/all-MiniLM-L6-v2
    scoring:
      similarity_gt_0.75: "+5.0"
      similarity_gt_0.60: "+3.0"
      similarity_gt_0.45: "+1.0"
      similarity_gt_0.30: "-1.0"
      similarity_le_0.30: "-3.0"
    notes: |
      Uses cosine similarity of embeddings.
      Good for soft matching but doesn't catch contradictions.

  # ---------------------------------------------------------------------------
  # NLI-BASED REWARDS (Research-backed)
  # ---------------------------------------------------------------------------

  nli_coherence_reward:
    purpose: Check if response ENTAILS ground truth
    model: facebook/bart-large-mnli
    scoring:
      entailment: "+3.0 (response supports/implies ground truth)"
      neutral: "-1.0 (off-topic or incoherent)"
      contradiction: "-3.0 (contradicts ground truth)"
    defeats:
      - "Word soup (random terms → neutral)"
      - "Contradictory claims"
      - "Off-topic responses"
    gpu_memory: "~1.6GB"
    research: "arxiv.org/abs/2508.18212"

  self_consistency_reward:
    purpose: Check for internal contradictions (no external ideology)
    method: |
      Parse response into sentences using spaCy.
      Check adjacent sentence pairs for NLI contradiction.
    scoring:
      no_contradictions: "+1.0"
      has_contradiction: "-2.0"
    notes: |
      Only checks within-document coherence, avoiding any external
      ideological bias from the NLI model's training data.
    research: "arxiv.org/abs/2508.05170"

  structural_coherence_reward:
    purpose: Verify proper linguistic structure (defeats word soup)
    model: spacy en_core_web_sm
    checks:
      - "Marxist terms in subject/object syntactic positions"
      - "Presence of discourse connectives (therefore, because, etc.)"
      - "Complete sentence structure"
    scoring:
      term_in_syntactic_role: "+0.3 per term (max +1.5)"
      discourse_connective: "+0.2 per connective (max +1.0)"
      no_sentences: "-1.0"
    syntactic_roles:
      - "nsubj (nominal subject)"
      - "nsubjpass (passive subject)"
      - "dobj (direct object)"
      - "pobj (object of preposition)"
      - "attr (attribute)"
      - "appos (appositional modifier)"
    discourse_connectives:
      - "because, therefore, thus, hence, consequently"
      - "however, although, whereas, nevertheless, moreover"
      - "furthermore, additionally, specifically, namely"
      - "as a result, due to, in order to, so that"
      - "on the other hand, in contrast, similarly, likewise"

  # ---------------------------------------------------------------------------
  # TOPIC RELEVANCE REWARD (Question-Answer Alignment)
  # ---------------------------------------------------------------------------

  topic_relevance_reward:
    purpose: Ensure answer addresses what the question asked about
    method: |
      Implements f(A) ⊆ f(Q) check where f extracts semantic topics:
      1. Extract core topics from question Q using dependency parsing
      2. Expand Q topics with Marxist concept synonyms
      3. Extract topics from answer A
      4. Compute coverage: how many Q topics are addressed in A
    scoring:
      gt_80_coverage: "+2.0 (answer fully addresses question topics)"
      gt_60_coverage: "+1.5 (answer mostly on-topic)"
      gt_40_coverage: "+1.0 (answer partially on-topic)"
      gt_20_coverage: "0.0 (answer tangentially related)"
      le_20_coverage: "-1.5 (answer off-topic)"
    defeats:
      - "Off-topic coherent text (coherent Marxist text about wrong subject)"
      - "Topic drift during response"
    spacy_model: "en_core_web_trf (transformer-based, best semantic understanding)"
    topic_extraction:
      question: "ROOT verb children with nsubj/dobj/attr/nsubjpass dependency"
      answer: "Noun chunks + named entities (determiners stripped)"
      synonym_expansion: "CONCEPT_EQUIVALENCES dict maps bourgeoisie ↔ capitalist class"

  # ---------------------------------------------------------------------------
  # COMBINED REWARDS
  # ---------------------------------------------------------------------------

  robust_coherence_reward:
    purpose: Multi-layer coherence check
    layers:
      1: "NLI coherence - Does response entail ground truth?"
      2: "Self-consistency - Does response contradict itself?"
      3: "Structural coherence - Are terms used in proper syntax?"
    scoring:
      max_score: "+5.5 (entailment + consistent + structured)"
      contradiction_floor: "-3.0 (NLI contradiction)"
      inconsistency_penalty: "-2.0 (internal contradiction)"
    combination_logic: |
      if nli_score <= -3.0:
          return -3.0  # Contradiction dominates
      elif consistency_score <= -2.0:
          return -2.0  # Internal contradiction
      else:
          return nli_score + (consistency * 0.5) + (structure * 0.5)

  full_coherence_reward:
    purpose: Complete coherence check (RECOMMENDED - maximum robustness)
    layers:
      1: "NLI coherence - Does response entail ground truth?"
      2: "Self-consistency - Does response contradict itself?"
      3: "Structural coherence - Are terms used in proper syntax?"
      4: "Topic relevance - Does answer address what was asked?"
      5: "Interconnection depth - Rewards deep analysis, penalizes buzzword salad"
    scoring:
      max_score: "+7.0 (all checks pass with deep analysis)"
      off_topic_penalty: "-2.0 (severely off-topic)"
      buzzword_salad_penalty: "-1.5 (shallow buzzword listing)"
      inherits: "robust_coherence_reward penalties for NLI/consistency failures"
    combination_logic: |
      if relevance <= -1.5:
          return -2.0  # Severely off-topic
      elif robust <= -2.0:
          return robust  # Robust check failed
      elif depth <= -1.5:
          return -1.5  # Buzzword salad detected
      else:
          return robust + (relevance * 0.4) + (depth * 0.3)
    use_when: "Maximum robustness against reward hacking is needed"

  # ---------------------------------------------------------------------------
  # INTERCONNECTION DEPTH REWARD (Anti-Buzzword-Salad)
  # ---------------------------------------------------------------------------

  interconnection_depth_reward:
    purpose: Distinguish deep analysis from shallow buzzword salad
    method: |
      Rewards meaningful interconnections while penalizing superficial
      concept-dropping. Distinguishes:
      - GOOD: "Surplus value relates to imperialism BECAUSE capital export..."
      - BAD: "Surplus value intersects with imperialism, colonialism, patriarchy..."
    signals:
      depth_ratio:
        description: "Words per unique Marxist concept"
        gt_20: "+1.0 (deep analysis - few concepts well-explained)"
        range_10_20: "+0.5 (adequate depth)"
        range_5_10: "-0.5 (shallow)"
        lt_5: "-1.5 (severe buzzword soup)"
      hollow_buzzwords:
        description: "Activist jargon without substance"
        threshold: "> 2 hollow phrases"
        penalty: "-0.3 per additional (max -1.5)"
      depth_markers:
        description: "Historical specificity, citations, examples"
        bonus: "+0.3 each (max +1.5)"
      explanation_ratio:
        description: "Explanatory phrases per concept"
        gt_50_percent: "+0.5 (well-explained)"
        lt_10_percent_many_concepts: "-0.5 (unexplained concept soup)"
    scoring:
      range: "-2.5 to +3.0"
    defeats:
      - "Buzzword salad (many concepts, no explanation)"
      - "Activist jargon (performative language without analysis)"
      - "Intersectionality word soup (mentioning everything without depth)"

  # ---------------------------------------------------------------------------
  # DEPRECATED/SHALLOW REWARDS
  # ---------------------------------------------------------------------------

  terminology_reward:
    status: DEPRECATED - use structural_coherence_reward instead
    purpose: Reward Marxist terminology (SHALLOW - can be gamed)
    scoring: "+0.3 per term (max +2.0)"
    warning: |
      This reward can be gamed with "word soup" - random Marxist
      terms without coherent meaning. Use nli_coherence_reward or
      structural_coherence_reward for robust evaluation.

# =============================================================================
# MARXIST TERMINOLOGY
# =============================================================================

marxist_terms:
  core_concepts:
    - "dialectical, materialism, historical materialism, dialectical materialism"
  classes:
    - "bourgeoisie, proletariat, petty bourgeois, lumpenproletariat"
    - "working class, ruling class"
  class_struggle:
    - "class struggle, class consciousness, class war, class conflict"
  political_economy:
    - "surplus value, commodity, use value, exchange value"
    - "labor power, means of production, relations of production"
    - "forces of production, mode of production, primitive accumulation"
    - "exploitation, capital accumulation"
  imperialism:
    - "imperialism, colonialism, neo-colonialism, settler colonialism"
    - "national liberation, self-determination"
  state_revolution:
    - "dictatorship of the proletariat, vanguard, vanguard party"
    - "democratic centralism, withering away of the state"
  ideology:
    - "hegemony, superstructure, base, ideology, false consciousness"
  revisionism:
    - "revisionism, opportunism, reformism, social democracy, ultra-leftism"
  alienation:
    - "alienation, fetishism, commodity fetishism, reification"
  historical:
    - "paris commune, october revolution, bolshevik, menshevik"
  anti_colonial:
    - "decolonization, third world, global south, national bourgeoisie, comprador"

# =============================================================================
# HOLLOW BUZZWORDS (Activist Jargon to Penalize)
# =============================================================================

hollow_buzzwords:
  description: |
    Phrases that signal superficial analysis when used without substantive
    explanation. These are NOT Marxist technical terms - they are activist
    jargon that often substitutes for actual analysis.
  vague_connectors:
    - "interconnected, interrelated, intersects with"
    - "it's all connected, everything is connected, systemic"
  performative_language:
    - "centered, centering, uplift, uplifting"
    - "do the work, the work, unpack, unpacking"
    - "unlearn, unlearning, hold space, sit with, lean into"
    - "problematic, harmful, toxic"
  vague_abstractions:
    - "in a way, sort of, kind of, essentially, basically"
    - "generally speaking, broadly"
  misused_terms:
    - "praxis (when used without explanation)"
    - "material conditions (when used as hand-wave)"
    - "structural, structurally (when mechanism not specified)"
  note: |
    The penalty applies when hollow buzzword DENSITY is high AND
    depth ratio is low. Legitimate use with explanation is not penalized.

# =============================================================================
# EXPLANATORY PHRASES (Depth Markers)
# =============================================================================

explanatory_phrases:
  description: "Phrases indicating concept is being explained, not just dropped"
  causal:
    - "because the, because of, this is because, since the"
    - "due to the, as a result of, results from, caused by"
    - "leads to, results in, enables, produces"
  definitional:
    - "is defined as, refers to, means that, denotes"
    - "that is, in other words, namely, i.e."
  elaboration:
    - "specifically, in particular, for example, such as"
    - "this means, which means, this implies, therefore"
  mechanism:
    - "this occurs when, this happens because, the mechanism"
    - "through the process of, by means of, works by"

depth_markers:
  description: "Phrases indicating analytical depth (historical specificity, citations)"
  historical:
    - "in 1, in 2, during the, after the, before the"
  citations:
    - "marx argued, lenin wrote, engels noted, gramsci"
    - "according to, as marx, as lenin"
  examples:
    - "for example, such as, in the case of, consider"
  definitions:
    - "defined as, meaning, specifically"

# =============================================================================
# USAGE IN TRAINING
# =============================================================================

training_usage:

  full_reward_set:
    description: "RECOMMENDED - Maximum robustness including depth analysis"
    functions:
      - "match_format_exactly (+3.0 for </think>)"
      - "match_format_approximately (tag validation)"
      - "full_coherence_reward (NLI + structure + topic + depth)"
      - "completeness_reward (length comparison)"
      - "debug_print_reward (monitoring)"
    notes: "full_coherence_reward now includes interconnection_depth_reward"

  robust_reward_set:
    description: "Balanced set that defeats word soup attacks"
    functions:
      - "match_format_exactly (+3.0 for </think>)"
      - "match_format_approximately (tag validation)"
      - "robust_coherence_reward (NLI + self-consistency + structure)"
      - "completeness_reward (length comparison)"
      - "debug_print_reward (monitoring)"

  legacy_reward_set:
    description: "Original set (VULNERABLE to word soup)"
    functions:
      - "match_format_exactly"
      - "match_format_approximately"
      - "semantic_similarity_reward"
      - "terminology_reward  # VULNERABLE"
      - "completeness_reward"
      - "debug_print_reward"

  grpo_trainer_example: |
    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        reward_funcs=[
            match_format_exactly,
            match_format_approximately,
            full_coherence_reward,  # RECOMMENDED: NLI + structure + topic
            completeness_reward,
            debug_print_reward,
        ],
        args=training_args,
        train_dataset=dataset,
    )

# =============================================================================
# DEPENDENCIES
# =============================================================================

dependencies:
  required:
    - "sentence-transformers  # for semantic_similarity_reward"
    - "transformers  # for NLI pipeline (bart-large-mnli)"
    - "spacy  # for topic extraction and structural coherence"
    - "spacy-curated-transformers  # for en_core_web_trf"
    - "numpy  # for embeddings"

  models_downloaded:
    - "sentence-transformers/all-MiniLM-L6-v2 (~90MB)"
    - "facebook/bart-large-mnli (~1.6GB)"
    - "en_core_web_trf (~436MB, transformer-based, RECOMMENDED)"
    - "en_core_web_md (~40MB, word vectors, fallback)"
    - "en_core_web_sm (~12MB, no vectors, last resort)"

  gpu_memory:
    embedder: "~200MB"
    nli_model: "~1.6GB"
    spacy_trf: "~500MB (RoBERTa-based transformer)"
    total: "~2.5GB additional (on top of training model)"

# =============================================================================
# WANDB LOGGING INTEGRATION
# =============================================================================

wandb_logging:
  purpose: |
    Comprehensive logging for GRPO training observability via Weights & Biases.
    Provides debugging visibility into reward function behavior, sample outputs,
    and per-step metrics.

  module: src/pw_mcp/ai_training/wandb_logging.py

  components:

    init_wandb_logging:
      purpose: Initialize W&B run with configuration
      signature: |
        init_wandb_logging(
            project: str,
            config: dict[str, Any],
            name: str | None = None,
            tags: list[str] | None = None,
            notes: str | None = None,
            mode: str = "online",  # or "offline", "disabled"
        ) -> wandb.Run | None
      example: |
        run = init_wandb_logging(
            project="marxist-grpo",
            config={
                "model": "DeepSeek-R1-0528-Qwen3-8B",
                "learning_rate": 5e-6,
                "batch_size": 2,
            },
            tags=["grpo", "marxist", "v1"],
        )

    WandbSampleLogger:
      purpose: Accumulate and log sample tables for debugging
      fields:
        log_every_n_steps: "int = 10 (log table every N training steps)"
        max_samples_per_log: "int = 4 (samples per table)"
      methods:
        add_sample: "Add sample with question/response/rewards to buffer"
        should_log: "Check if current step should trigger table log"
        log_table: "Log accumulated samples as wandb.Table"
        clear: "Clear sample buffer"
      table_columns:
        - "step, question, response, ground_truth"
        - "format_exact, format_approx, nli_coherence"
        - "topic_relevance, depth, completeness, total"

    create_logging_reward:
      purpose: Factory for GRPOTrainer-compatible logging reward function
      signature: |
        create_logging_reward(
            sample_logger: WandbSampleLogger | None = None,
            compute_all_rewards: bool = True,
        ) -> Callable[..., list[float]]
      behavior: |
        1. Computes ALL reward functions internally (if compute_all_rewards=True)
        2. Logs aggregated metrics to wandb (per-reward mean/min/max)
        3. Logs sample tables at configured intervals
        4. Returns [0.0] * len(completions) (no training effect)
      usage: |
        sample_logger = WandbSampleLogger(log_every_n_steps=10)
        logging_reward = create_logging_reward(sample_logger)

        trainer = GRPOTrainer(
            reward_funcs=[..., logging_reward],  # Add to reward_funcs
            ...
        )

    log_reward_metrics:
      purpose: Log per-reward metrics to wandb
      signature: |
        log_reward_metrics(
            step: int,
            reward_scores: dict[str, list[float]],
        ) -> None
      logs:
        - "rewards/{name} - mean score"
        - "rewards/{name}_min - minimum score"
        - "rewards/{name}_max - maximum score"
        - "rewards/total - sum of all reward means"

    finish_wandb_logging:
      purpose: Finish run with optional summary statistics
      signature: |
        finish_wandb_logging(
            summary: dict[str, Any] | None = None,
        ) -> None

    log_model_checkpoint:
      purpose: Log checkpoint as wandb artifact
      signature: |
        log_model_checkpoint(
            checkpoint_path: str,
            metadata: dict[str, Any] | None = None,
        ) -> None

  graceful_degradation: |
    All functions work gracefully when wandb is not installed:
    - is_wandb_available() returns False
    - Logging functions print fallback messages or no-op
    - create_logging_reward returns valid reward function (prints to stdout)

  integration_example: |
    from pw_mcp.ai_training import (
        init_wandb_logging,
        WandbSampleLogger,
        create_logging_reward,
        finish_wandb_logging,
        match_format_exactly,
        full_coherence_reward,
    )

    # Initialize wandb
    run = init_wandb_logging(
        project="marxist-grpo",
        config={"model": "DeepSeek-R1", "lr": 5e-6, "steps": 250},
    )

    # Create sample logger and logging reward
    sample_logger = WandbSampleLogger(log_every_n_steps=10, max_samples_per_log=4)
    logging_reward = create_logging_reward(sample_logger, compute_all_rewards=True)

    # Train with logging
    trainer = GRPOTrainer(
        model=model,
        reward_funcs=[
            match_format_exactly,
            full_coherence_reward,
            logging_reward,  # Logs all metrics + samples
        ],
        args=training_args,
        train_dataset=dataset,
    )
    trainer.train()

    # Finish with summary
    finish_wandb_logging(summary={"final_loss": trainer.state.loss})

# =============================================================================
# TESTING
# =============================================================================

testing:

  test_word_soup:
    input: "bourgeoisie proletariat dialectical materialism surplus value"
    expected_nli: "neutral (0.93)"
    expected_structure: "low (no syntactic roles)"
    expected_topic: "fails - no proper sentence to extract topics from"

  test_good_response:
    input: "The bourgeoisie extracts surplus value from the proletariat through exploitation of labor power."
    expected_nli: "entailment (depends on ground truth)"
    expected_structure: "high (terms in subject/object positions)"
    expected_topic: "high (bourgeoisie, surplus value, proletariat in proper roles)"

  test_contradiction:
    input: "Capitalism benefits everyone. Workers are exploited under capitalism."
    expected_self_consistency: "-2.0 (internal contradiction)"

  test_off_topic:
    question: "What is revisionism?"
    answer: "Imperialism is the highest stage of capitalism characterized by monopolies."
    expected_topic_relevance: "-1.5 (off-topic - discusses imperialism not revisionism)"

  test_synonym_recognition:
    question: "What is the bourgeoisie?"
    answer: "The capitalist class owns the means of production."
    expected_topic_relevance: "+2.0 (synonym 'capitalist class' recognized)"

# =============================================================================
# REFERENCES
# =============================================================================

references:
  papers:
    - "Better LM-Based Judging Reward Modeling: arxiv.org/abs/2508.18212"
    - "MO-GRPO Mitigating Reward Hacking: arxiv.org/abs/2509.22047"
    - "Posterior-GRPO Process Rewards: arxiv.org/abs/2508.05170"
    - "MENLI NLI Evaluation Metrics: doi.org/10.1162/tacl_a_00576"

  models:
    - "BART-large-MNLI: huggingface.co/facebook/bart-large-mnli"
    - "all-MiniLM-L6-v2: huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
    - "spaCy en_core_web_trf: spacy.io/models/en#en_core_web_trf (RECOMMENDED)"
    - "spaCy en_core_web_md: spacy.io/models/en#en_core_web_md (fallback)"

  related_docs:
    - "ai-docs/finetune.yaml - overall fine-tuning strategy"
    - "ai-docs/chatbot-ideology.yaml - training data design"
    - "ai-docs/runpod.yaml - GPU setup for training"
