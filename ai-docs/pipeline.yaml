# ProleWiki Ingestion Pipeline Architecture
# Status: Stages 1-3 IMPLEMENTED, Stage 4 NEXT
# Last updated: 2025-12-16
#
# MAJOR CHANGE (2025-12-16): Removed sembr ML pipeline, replaced with tiktoken
# Pipeline is now 4 stages instead of 5 (~6000 lines of code removed)
#
# Pipeline Structure (simplified):
#   Stage 1: Extraction  ✓  - Parse MediaWiki → extracted/articles/*.json
#   Stage 2: Chunking    ✓  - Tiktoken-based chunking → chunks/*.jsonl
#   Stage 3: Embedding   ✓  - Generate vectors → embeddings/*.npy (OpenAI/Ollama)
#   Stage 4: Loading        - Load into ChromaDB → chroma_data/
#   (Optional) Graph        - Compute backlinks → graph/*.jsonl
#
# Sample Pipeline: sample-pipeline/ contains 10 public domain works
# demonstrating full workflow with all outputs committed to repo

# =============================================================================
# DESIGN PRINCIPLES
# =============================================================================

principles:
  normalization: |
    References are normalized into a single source of truth.
    Multiple documents citing the same work → same reference record.
    Reference metadata is enriched from ALL citations of that work.

  debuggability: |
    Each stage outputs intermediate files for inspection.
    Pipeline can be re-run from any stage.

  backlinks: |
    Wiki-style bidirectional linking is core to the architecture.
    Every link A→B also creates a backlink B←A.

  quote_preservation: |
    Quoted text from references is semantically valuable.
    Quotes are extracted and kept inline in clean text.
    Quote source is tracked in metadata.

# =============================================================================
# DATA FLOW OVERVIEW
# =============================================================================

data_flow: |
  prolewiki-exports/
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 1: EXTRACTION                                   [✓]  │
  │  Parse MediaWiki, extract structured data + clean text      │
  │                                                             │
  │  Outputs:                                                   │
  │    extracted/articles/{namespace}/{title}.json              │
  │    - clean_text (templates removed, ready for chunking)     │
  │    - categories, internal_links, infobox, library_work      │
  │    - is_stub, citation_needed_count, has_blockquote         │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 2: CHUNKING (tiktoken-based)                    [✓]  │
  │  Split text into token-aware chunks (100-1000 tokens)       │
  │  - Accurate token counting with tiktoken (cl100k_base)      │
  │  - Oversized line splitting at sentence/word boundaries     │
  │  - Section header detection and overlap support             │
  │  Attach metadata from extraction                            │
  │                                                             │
  │  Outputs:                                                   │
  │    chunks/{namespace}/{title}.jsonl                         │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 3: EMBEDDING                                    [✓]  │
  │  Generate vectors (OpenAI 1536-dim or Ollama 768-dim)       │
  │  Provider: --provider openai (default) or --provider ollama │
  │  Token-aware batching for OpenAI API limits                 │
  │                                                             │
  │  Outputs:                                                   │
  │    embeddings/{namespace}/{title}.npy                       │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 4: CHROMADB LOADING                                  │
  │  Load chunks + embeddings + metadata into ChromaDB          │
  │                                                             │
  │  Outputs:                                                   │
  │    chroma_data/                                             │
  └─────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────┐
  │  (OPTIONAL) GRAPH COMPUTATION                               │
  │  Compute backlinks, link counts from extracted data         │
  │  Can be added anytime - not blocking core pipeline          │
  │                                                             │
  │  Outputs:                                                   │
  │    graph/backlinks.jsonl      # Inverted link index (B←A)   │
  │    graph/link_counts.json     # In-degree/out-degree        │
  │    graph/category_graph.json  # Category co-occurrence      │
  │                                                             │
  │  Benefits:                                                  │
  │    - Boost important articles in search ranking             │
  │    - "Related articles" MCP tool                            │
  │    - Context expansion for better answers                   │
  └─────────────────────────────────────────────────────────────┘

# =============================================================================
# STAGE 1: EXTRACTION - Detailed
# =============================================================================

stage_1_extraction:
  status: IMPLEMENTED
  implementation: src/pw_mcp/ingest/extraction.py
  tests: 160 passing (6 test files)
  purpose: |
    Parse MediaWiki markup and extract all structured data.
    This is the most complex stage - everything else is simpler.

  input:
    source: prolewiki-exports/{namespace}/{title}.txt
    format: MediaWiki markup

  outputs:
    article_json:
      path: extracted/articles/{namespace}/{title}.json
      schema:
        title: str
        namespace: str  # Main, Library, Essays, ProleWiki
        sections: list[Section]
        clean_text: str  # Markup removed, quotes preserved, infobox removed
        ref_ids: list[str]  # References used in this article
        link_targets: list[str]  # Articles this links to (includes infobox links)
        categories: list[str]
        word_count: int
        has_quotes: bool
        infobox: InfoboxData | null  # Extracted infobox metadata

      infobox_schema:
        type: str  # politician, country, political_party, etc.
        raw_fields: dict[str, str]  # All fields as-is before parsing
        # Parsed fields (type-dependent):
        subject_name: str | null
        native_name: str | null
        birth_date: str | null  # ISO format
        birth_year: int | null
        death_date: str | null
        death_year: int | null
        birth_place: str | null
        death_place: str | null
        nationality: str | null
        death_cause: str | null
        political_orientation: list[str]
        political_party: list[str]
        founded_date: str | null
        founded_year: int | null
        dissolved_date: str | null
        dissolved_year: int | null
        life_span: str | null
        successor: str | null
        predecessor: str | null
        capital: str | null
        government_type: str | null
        essay_author: str | null
        essay_date: str | null
        essay_excerpt: str | null
        conflict_date: str | null
        conflict_result: str | null
        extracted_links: list[str]  # Links found within infobox values

    references_jsonl:
      path: extracted/references.jsonl
      purpose: Normalized reference table (single source of truth)
      schema:
        ref_id: str  # Deterministic hash of normalized citation
        ref_type: str  # Citation, Web, Book, Journal
        authors: list[str]
        year: int | null
        title: str
        source_publication: str | null  # Journal, encyclopedia, etc.
        urls: list[str]  # All URLs (mia, pdf, url, archive)
        quotes: list[QuoteInstance]  # All quotes from this ref
        citing_articles: list[str]  # Articles that cite this ref
        first_seen_in: str  # Article where first encountered

      quote_instance_schema:
        text: str
        article: str
        section: str | null

    links_jsonl:
      path: extracted/links.jsonl
      purpose: Forward link graph
      schema:
        source_article: str  # "Main/Atheism"
        target_article: str  # "Main/Marxism–Leninism"
        link_text: str  # Display text if different from target
        section: str | null  # Section where link appears
        count: int  # Times this link appears in source

    categories_jsonl:
      path: extracted/categories.jsonl
      schema:
        article: str
        categories: list[str]

  extraction_rules:
    references:
      named_ref: |
        <ref name="...">...</ref>
        - Extract name as local identifier
        - Parse inner Citation/Web citation template
        - Generate deterministic ref_id from normalized fields

      inline_ref: |
        <ref>...</ref>
        - No name, single use
        - Still normalize and deduplicate

      ref_reuse: |
        <ref name="..." />
        - Points to earlier named ref
        - Record as citation without re-extracting

      citation_template: |
        {{Citation|author=...|year=...|title=...|...}}
        Fields to extract:
          - author (may be [[linked]])
          - year
          - title
          - chapter
          - url, mia, pdf, chapter-url, archive-url
          - quote (IMPORTANT: preserve this text!)
          - volume, page, pages
          - publisher, city
          - trans-title, trans-lang

      web_citation_template: |
        {{Web citation|...}}
        Additional fields:
          - newspaper
          - archive-date
          - access-date

    internal_links:
      simple: '"[[Target]]" -> target="Target", text="Target"'
      piped: '"[[Target|Display]]" -> target="Target", text="Display"'
      sectioned: '"[[Target#Section]]" -> target="Target", section="Section"'
      category: '"[[Category:Name]]" -> category, not link'
      file: '"[[File:...]]" -> ignore'
      interwiki: '"[[ru:...]]" -> ignore (or separate table?)'

    quotes:
      strategy: |
        1. Extract quote text from |quote= parameter
        2. Track which reference it came from
        3. Insert quote inline where <ref> was (in clean_text)
        4. Format: "quoted text" (attributed to ref_id)

    sections:
      pattern: "^(=+)\\s*(.+?)\\s*\\1$"
      extract:
        level: int  # Number of = signs
        title: str
        line_start: int
        line_end: int

    # -----------------------------------------------------------------
    # INFOBOX EXTRACTION
    # -----------------------------------------------------------------
    infoboxes:
      pattern: |
        {{Infobox TYPE
        |field1=value1
        |field2=value2
        ...
        }}

        May also appear as single-line:
        {{Infobox TYPE|field1=value1|field2=value2|...}}

      types_found:
        # Frequency from corpus analysis:
        politician: 621      # People in politics
        country: 408         # Nations, states
        political_party: 386  # Parties, movements
        person: 228          # Generic people
        revolutionary: 97    # Revolutionary figures
        essay: 50            # ProleWiki essays
        philosopher: 46      # Philosophers
        company: 36          # Companies, corporations
        settlement: 26       # Cities, towns
        military_person: 19  # Military figures
        organization: 17     # Generic orgs
        guerilla_organization: 16
        youtuber: 12
        military_conflict: 12
        book: 8
        religion: 4
        website: 2
        others: "<10 each"   # transcript, library_work, film, etc.

      field_mapping:
        # Fields to EXTRACT (valuable metadata)
        extract:
          # Identity
          - name -> subject_name
          - native_name -> native_name

          # Biographical (people)
          - birth_date -> parse_date -> birth_date, birth_year
          - death_date -> parse_date -> death_date, death_year
          - birth_place -> birth_place (also extract [[links]])
          - death_place -> death_place (also extract [[links]])
          - nationality -> nationality (also extract [[links]])
          - death_cause -> death_cause

          # Political/Ideological
          - political_orientation -> political_orientation (parse [[links]] to list)
          - political_line -> political_orientation (alias)
          - political_party -> political_party (parse [[links]] to list)

          # Organization lifecycle
          - founded -> parse_date -> founded_date, founded_year
          - dissolved -> parse_date -> dissolved_date, dissolved_year
          - life_span -> life_span
          - successor -> successor (extract [[link]])
          - split -> predecessor (extract [[link]])
          - predecessor -> predecessor

          # Geographic
          - capital -> capital
          - government_type -> government_type

          # Essay-specific
          - author -> essay_author
          - date -> essay_date
          - excerpt -> essay_excerpt

          # Conflict-specific
          - date -> conflict_date (for military_conflict type)
          - result -> conflict_result

        # Fields to DISCARD (not useful for search)
        discard:
          - image
          - image_size
          - image_upright
          - image_alt
          - caption
          - logo
          - image_flag
          - honorific_prefix
          - honorific_suffix
          - signature
          - signature_size
          - combatant1, combatant2  # Too complex, [[links]] extracted separately
          - commander1, commander2  # Same

      date_parsing:
        templates:
          - '"{{birth date|YYYY|MM|DD}}" -> "YYYY-MM-DD"'
          - '"{{death date|YYYY|MM|DD}}" -> "YYYY-MM-DD"'
          - '"{{death date and age|YYYY|MM|DD|...}}" -> "YYYY-MM-DD"'
          - '"DD Month YYYY" -> "YYYY-MM-DD"'
          - '"Month DD, YYYY" -> "YYYY-MM-DD"'
          - '"YYYY" -> year only (birth_year/death_year)'
          - '"YYYY–YYYY" -> life_span (keep as string)'

      link_extraction:
        purpose: |
          Many infobox values contain [[internal links]].
          These are valuable for the link graph!

        examples:
          - input: "birth_place=[[Moscow]], [[Soviet Union]]"
            extract: '["Moscow", "Soviet Union"]'
            action: Add to article link_targets

          - input: "political_party=[[CPSU]]"
            extract: '["CPSU"]'
            action: Store in political_party field AND link_targets

          - input: "political_orientation=[[Islamic socialism]]<br>[[Arab socialism]]"
            extract: '["Islamic socialism", "Arab socialism"]'
            action: Store as list in political_orientation field

      removal_from_text:
        purpose: |
          After extraction, the entire infobox is REMOVED from clean_text.
          This improves sembr processing and embedding quality.

        pattern: |
          Remove from start of {{Infobox to matching }}
          Handle nested {{ }} carefully

        result: |
          Infobox metadata → stored in article.infobox
          Infobox text → NOT in clean_text (no sembr, no embedding)
          Infobox links → added to link_targets

    # -----------------------------------------------------------------
    # ADDITIONAL TEMPLATES (beyond infoboxes)
    # -----------------------------------------------------------------
    additional_templates:

      # =================================================================
      # LIBRARY WORK TEMPLATE - Rich metadata for Library namespace
      # =================================================================
      library_work:
        pattern: |
          {{Library work
          |title=...
          |author=...
          |published_date=...
          |type=...
          |source=...
          }}

        frequency: "~15+ in sample, prevalent in Library/ namespace"

        field_mapping:
          extract:
            - title -> library_work_title
            - author -> library_work_author
            - written_in -> library_work_written_date
            - published_date -> library_work_published_date
            - published_location -> library_work_location
            - edition_date -> library_work_edition_date
            - type -> library_work_type  # Book, Speech, Research paper, etc.
            - source -> library_work_source_url
            - translated_by -> library_work_translator
            - original_language -> library_work_original_language
            - publisher -> library_work_publisher
            - audiobook -> library_work_audiobook_url

          discard:
            - image
            - image_size
            - image_upright
            - image_alt
            - caption

        action: |
          1. Extract all fields to article.library_work metadata
          2. Remove entire template from clean_text
          3. Add to link_targets if author/translator contain [[links]]

      # =================================================================
      # FOREWORD TEMPLATE - Rich semantic content
      # =================================================================
      foreword:
        pattern: "{{Foreword|foreword=...}}"
        frequency: "Rare but semantically valuable"

        action: |
          1. Extract foreword text
          2. KEEP in clean_text (valuable for embedding)
          3. Tag section as foreword in metadata
          4. Format as block quote with "Foreword:" prefix

      # =================================================================
      # NAVIGATION TEMPLATES (Library chapters)
      # =================================================================
      book_navigation:
        pattern: |
          {{Book_Navigation
          |previous=Library:...
          |previous_title=...
          |next=Library:...
          |next_title=...
          |current=...
          }}

        frequency: "~7+ in Library chapters"

        field_mapping:
          extract:
            - previous -> chapter_previous  # Link to previous chapter
            - next -> chapter_next  # Link to next chapter
            - current -> chapter_current  # Current chapter title

        action: |
          1. Extract navigation metadata
          2. Add previous/next to link_targets
          3. Remove template from clean_text (navigation noise)

      book_toc:
        pattern: "{{Book_TOC|...}}"
        frequency: "~7 in Library chapters"

        action: |
          1. Extract chapter list as structured data
          2. Remove from clean_text (table of contents noise)
          3. Store as book_chapters metadata (list of chapter links)

      chapter_change_title:
        pattern: "{{Chapter_change_title}}"
        action: "Remove from clean_text (display control only)"

      # =================================================================
      # MAIN ARTICLE LINKS (valuable for graph!)
      # =================================================================
      main_article:
        pattern: "{{Main article|Target Article}}"
        frequency: "~10+ across corpus"

        action: |
          1. Extract target article as related_main_article
          2. Add to link_targets (important relationship!)
          3. Remove template from clean_text
          4. Store association: this section → main article

        note: |
          These indicate "this section summarizes, see full article"
          High-value signal for related content discovery

      # =================================================================
      # STATUS MARKERS
      # =================================================================
      stub_markers:
        patterns:
          - "{{Stub}}"
          - "{{Message box/Stub}}"

        frequency: "~9 in sample"

        action: |
          1. Set article.is_stub = true
          2. Remove template from clean_text
          3. Potentially lower ranking in search results

      citation_needed:
        pattern: "{{Citation needed}}"
        frequency: "4 in sample"

        action: |
          1. Count occurrences → article.citation_needed_count
          2. Remove from clean_text
          3. Track location for potential future editorial work

      # =================================================================
      # CITATION TEMPLATE VARIANTS
      # =================================================================
      citation_variants:
        types:
          web_citation:
            pattern: "{{Web citation|...}}"
            frequency: "71 in sample (most common)"
            fields: [author, newspaper, title, url, date, archive-url, archive-date]

          news_citation:
            pattern: "{{News citation|...}}"
            frequency: "39 in sample"
            fields: [author, newspaper, title, url, date]

          library_citation:
            pattern: "{{Library citation|link=...}}"
            frequency: "Rare, links to Library namespace"
            note: "Internal link to Library work - valuable for graph!"

          video_citation:
            pattern: "{{Video citation|url=...|channel=...|title=...}}"
            frequency: "Rare"
            fields: [url, channel, title]

          youtube_citation:
            pattern: "{{YouTube citation|...}}"
            frequency: "~3 in sample"
            note: "Variant of video citation"

        action: |
          All citation templates are handled within <ref> tags.
          Parse type-specific fields and normalize to reference table.
          Distinguish ref_type: Web, News, Library, Video, etc.

      # =================================================================
      # DISPLAY CONTROL (remove, no extraction)
      # =================================================================
      display_control:
        templates:
          DEFAULTSORT:
            pattern: "{{DEFAULTSORT:Sort Key}}"
            action: "Remove from clean_text (affects category sorting only)"

          DISPLAYTITLE:
            pattern: "{{DISPLAYTITLE:Title with formatting}}"
            action: "Remove from clean_text (display only)"

          TOC_limit:
            pattern: "{{TOC limit|N}}"
            action: "Remove (table of contents depth control)"

      # =================================================================
      # SIDEBARS AND NAVBOXES (remove, extract links)
      # =================================================================
      sidebars:
        patterns:
          - "{{Marxism–Leninism sidebar}}"
          - "{{TKNsidebar}}"
          - "{{Capital, vol. I sidebar}}"
          - "{{*sidebar}}"  # Generic pattern

        action: |
          1. Identify sidebar type
          2. Remove from clean_text (navigation clutter)
          3. Optionally: set article.sidebar_type for categorization

      navboxes:
        patterns:
          - "{{Navbox CPC Constitution}}"
          - "{{Navbox *}}"  # Generic pattern

        action: |
          1. Remove from clean_text
          2. Navboxes often appear at article end (after References)
          3. No metadata extraction needed

      # =================================================================
      # TRANSCLUSION MARKERS
      # =================================================================
      transclusion:
        noinclude:
          pattern: "<noinclude>...</noinclude>"
          content: "Navigation, footnotes, categories (for transcluded pages)"

          action: |
            1. For Library chapters: noinclude often contains Book_Navigation
            2. Extract navigation metadata BEFORE removing
            3. Remove noinclude wrapper and content from clean_text
            4. Exception: keep content if it's Footnotes section

        onlyinclude:
          pattern: "<onlyinclude>...</onlyinclude>"
          action: |
            1. This IS the main content (for transcluded pages)
            2. Keep content, remove wrapper tags
            3. Common in Library chapters

    # -----------------------------------------------------------------
    # STRUCTURAL SECTIONS
    # -----------------------------------------------------------------
    structural_sections:
      see_also:
        pattern: "== See also =="
        frequency: "Common in Main/ articles"

        action: |
          1. Extract all [[links]] from this section
          2. Store as article.see_also (valuable related articles)
          3. Add to link_targets
          4. Optionally: exclude section from embedding (just links)

      external_links:
        pattern: "== External links =="
        action: |
          1. Extract URLs
          2. Remove section from clean_text (not valuable for embedding)
          3. Store as article.external_links

      references_section:
        patterns:
          - "== References =="
          - "== Footnotes =="
          - "== Notes =="
          - "<references />"
          - "<references group=\"lower-alpha\" />"

        action: |
          1. Remove entire section from clean_text
          2. References already extracted from <ref> tags
          3. <references /> just renders them (no new content)

    # -----------------------------------------------------------------
    # BLOCKQUOTES
    # -----------------------------------------------------------------
    blockquotes:
      pattern: "<blockquote>...</blockquote>"
      frequency: "Common in Essays and Library"

      action: |
        1. KEEP in clean_text (semantically valuable!)
        2. Extract attribution if present (often at end: "— Author")
        3. Format for embedding: preserve quote structure
        4. Tag chunk as has_blockquote = true

      attribution_patterns:
        - "— Author Name"
        - "-- [[Author Name]]"
        - "- Author, Title"

# =============================================================================
# STAGE 2: CHUNKING - Detailed (tiktoken-based)
# =============================================================================
# NOTE: This replaced the sembr stage as of 2025-12-16
# The sembr ML pipeline was removed (~6000 lines) in favor of tiktoken

stage_2_chunking:
  status: IMPLEMENTED
  completed_date: 2025-12-16
  reference_doc: ai-docs/chunking.yaml
  tests: 74 passing (67 chunker + 7 oversized line handling)

  purpose: |
    Split clean text into token-aware chunks using tiktoken.
    Attach all metadata from extraction stage.
    No longer requires sembr preprocessing - reads directly from extracted/.

  key_features:
    - Accurate token counting with tiktoken (cl100k_base encoding)
    - Oversized line handling (splits at sentence/word boundaries)
    - Section header detection (== ... ==) for hard breaks
    - Paragraph awareness for soft breaks
    - Chunk overlap support for context continuity
    - No external server or GPU required

  implementation:
    module: src/pw_mcp/ingest/chunker.py (~600 lines)
    cli: src/pw_mcp/ingest/cli.py (chunk subcommand)
    classes:
      ChunkConfig:
        target_tokens: 350
        min_tokens: 100
        max_tokens: 1000
        overlap_tokens: 50
        frozen: true
      Chunk:
        text: str
        chunk_index: int
        section: str | None
        line_start: int
        line_end: int
        word_count: int
        estimated_tokens: int
      ChunkedArticle:
        article_title: str
        namespace: str
        chunks: list[Chunk]
        categories: list[str]
        internal_links: list[str]
        infobox: dict | None
        library_work: dict | None
        is_stub: bool
        citation_needed_count: int
        has_blockquote: bool
    functions:
      - count_tokens(text) -> int  # tiktoken-based accurate counting
      - is_section_header(line) -> bool
      - extract_section_title(line) -> str
      - generate_chunk_id(namespace, title, index) -> str
      - chunk_text_tiktoken(text, config) -> list[Chunk]  # main algorithm
      - _split_oversized_text(text, max_tokens) -> list[str]  # handles long lines
      - _find_text_split_position(text, max_tokens) -> int  # finds best split point
      - chunk_article(extracted_path, config) -> ChunkedArticle
      - write_chunks_jsonl(article, output_path) -> None

  cli_usage: |
    # Check chunk subcommand help
    pw-ingest chunk --help

    # Process with sample files
    pw-ingest chunk --sample 10

    # Process full corpus (reads from extracted/ directly)
    pw-ingest chunk -i extracted/ -o chunks/

    # Custom token configuration
    pw-ingest chunk --target-tokens 400 --max-tokens 800

  input:
    article_json: extracted/articles/{namespace}/{title}.json  # clean_text field
    note: Reads directly from extraction output - no sembr preprocessing needed

  output:
    path: chunks/{namespace}/{title}.jsonl
    schema:
      chunk_id: str  # "{namespace}/{title}#{index}"
      text: str

      # Article-level metadata
      article_title: str
      namespace: str
      categories: str  # JSON array
      internal_links: str  # JSON array (links FROM this article)
      backlinks: str  # JSON array (links TO this article)

      # Chunk-level metadata
      section: str | null
      chunk_index: int
      line_range: str  # "start-end"
      word_count: int

      # Reference-derived metadata
      cited_ref_ids: str  # JSON array of ref_ids cited in this chunk
      cited_authors: str  # JSON array (aggregated from refs)
      cited_years: str  # JSON array
      has_refs: bool
      ref_count: int
      has_quotes: bool

  chunking_algorithm:
    # REVISED: Larger chunks to leverage EmbeddingGemma's 2048 token limit
    # while maintaining retrieval precision
    target_tokens: 600  # was 350
    min_tokens: 200     # was 100
    max_tokens: 1000    # was 500

    boundaries:
      hard:
        - Section headers (== ... ==)
        - Never split mid-section unless section > max_tokens
      soft:
        - Paragraph breaks (blank lines)
        - Prefer breaking at paragraph boundaries

    grouping: |
      1. Parse sembr output into semantic lines
      2. Group lines into paragraphs (separated by blank lines)
      3. Group paragraphs into chunks respecting token limits
      4. If section header encountered, start new chunk
      5. Track line numbers for line_range metadata

    token_estimation: |
      word_count × 1.3 for English text approximation.
      EmbeddingGemma supports 2048 tokens - larger chunks provide
      more context while staying well under limit.

# =============================================================================
# STAGE 3: EMBEDDING - Detailed
# =============================================================================

stage_3_embedding:
  status: IMPLEMENTED
  completed_date: 2025-12-15
  reference_doc: ai-docs/embedding.yaml
  implementation: src/pw_mcp/ingest/embedder.py
  tests: 31 passing (21 embedder + 10 CLI)
  purpose: Generate vector embeddings for each chunk

  input: chunks/{namespace}/{title}.jsonl
  output: embeddings/{namespace}/{title}.npy

  model:
    provider: ollama (local)
    name: embeddinggemma
    dimensions: 768
    max_tokens: 2048
    normalization: L2-normalized
    params: 307M (Gemma3-based)
    base_url: "http://localhost:11434"

  processing:
    library: ollama Python package
    batch_size: 32  # Chunks per API call
    retry_count: 3  # On transient failures
    resume_support: true  # Skip existing .npy files

    code_pattern: |
      from ollama import embed
      response = embed(model='embeddinggemma', input=texts)
      embeddings = np.array(response.embeddings, dtype=np.float32)

  output_format:
    format: NumPy binary (.npy)
    dtype: float32
    shape: (num_chunks, 768)
    alignment: |
      CRITICAL: Embedding order matches JSONL line order!
      embeddings[i] corresponds to chunk_index i
      Stage 5 loading relies on this alignment.

  cli:
    command: pw-ingest embed
    options: [-i, -o, --sample, --no-progress, --model, --batch-size, --host]

  performance:
    estimated_throughput: 50-100 chunks/second (batch of 32)
    estimated_corpus_time: 3-10 minutes for ~26,000 chunks
    note: Requires benchmarking during implementation

# =============================================================================
# STAGE 4: CHROMADB LOADING - Detailed
# =============================================================================

stage_4_loading:
  status: PLANNED
  purpose: Load chunks + embeddings into ChromaDB

  # Schema is phased - see chromadb-schema.yaml for full details
  schema_reference:
    file: chromadb-schema.yaml
    mvp_phase: A
    mvp_fields: 13
    note: |
      Phase A (MVP): 13 fields already output by chunker - no changes needed
      Phase B: Rich metadata (infobox, library_work) - requires chunker extension
      Phase C: Reference metadata - requires extraction enhancement
      Phase D: Graph metadata - requires graph computation (DEFERRED)

  input:
    chunks: chunks/**/*.jsonl
    embeddings: embeddings/**/*.npy
    # references: extracted/references.jsonl  # Phase C - deferred

  output: chroma_data/

  collections:
    prolewiki_chunks:
      purpose: Main searchable collection
      hnsw_config:
        space: cosine
        ef_construction: 200
        ef_search: 100

      mvp_fields:
        description: Phase A schema - ready to implement
        fields:
          - chunk_id          # "{namespace}/{title}#{index}"
          - text              # embedded content
          - article_title
          - namespace
          - section
          - chunk_index
          - line_range
          - word_count
          - categories        # JSON array
          - internal_links    # JSON array
          - is_stub
          - citation_needed_count
          - has_blockquote

    prolewiki_references:
      status: DEFERRED to Phase C
      purpose: Reference lookup table (optional separate collection)
      note: |
        Defer until citation-based queries are needed.
        For MVP, references stay embedded in chunk text.

  loading_strategy:
    batch_size: 5000  # Below max_batch_size
    upsert: true  # Allow re-running without duplicates

    id_format: |
      Chunks: "{namespace}/{title}#{chunk_index}"
      Refs: "ref:{ref_id}"

      Deterministic IDs enable idempotent loading

# =============================================================================
# OPTIONAL: GRAPH COMPUTATION
# =============================================================================

optional_graph:
  status: DEFERRED
  blocking: false  # Not required for core RAG functionality
  can_add_later: true  # Easy to compute from existing extracted data

  purpose: |
    Compute derived graph data from extracted forward links.
    Provides backlinks, importance metrics, and category relationships.
    NOT required for basic semantic search - can be added as enhancement.

  when_to_implement: |
    After MVP is working (Stages 1-5 + MCP Server):
    - If search ranking needs improvement (boost important articles)
    - If "related articles" feature is desired
    - If context expansion improves answer quality

  outputs:
    backlinks_jsonl:
      path: graph/backlinks.jsonl
      purpose: "Which articles link TO this article? (inverse of forward links)"
      schema:
        target_article: str
        source_articles: list[str]
        total_count: int

    link_counts_json:
      path: graph/link_counts.json
      purpose: "PageRank-style importance metrics"
      schema:
        article: str
        in_degree: int   # How many articles link here (popularity)
        out_degree: int  # How many articles this links to (connectedness)

    category_graph_json:
      path: graph/category_graph.json
      purpose: "Category co-occurrence for related categories"
      schema:
        category: str
        article_count: int
        co_occurs_with: dict[str, int]

  implementation:
    pseudocode: |
      def compute_backlinks(extracted_dir: Path) -> dict[str, list[str]]:
          """Invert forward links to get backlinks."""
          backlinks = defaultdict(list)

          for article_path in extracted_dir.rglob("*.json"):
              article = json.loads(article_path.read_text())
              source_title = article["title"]

              for link in article.get("internal_links", []):
                  target = link["target"]
                  backlinks[target].append(source_title)

          return dict(backlinks)

      def compute_degrees(extracted_dir: Path, backlinks: dict) -> dict:
          """Compute in-degree and out-degree per article."""
          degrees = {}
          for article_path in extracted_dir.rglob("*.json"):
              article = json.loads(article_path.read_text())
              title = article["title"]
              degrees[title] = {
                  "out_degree": len(article.get("internal_links", [])),
                  "in_degree": len(backlinks.get(title, []))
              }
          return degrees

  mcp_tools_enabled:
    search_with_boost: |
      @mcp.tool()
      async def search(query: str, boost_popular: bool = True) -> str:
          results = chromadb.query(query, n=20)
          if boost_popular:
              results = sorted(results, key=lambda x: x['in_degree'], reverse=True)
          return results[:10]

    get_related: |
      @mcp.tool()
      async def get_related(title: str) -> str:
          """What articles discuss this topic? (backlinks)"""
          return backlinks[title]

    explore_context: |
      @mcp.tool()
      async def explore_context(title: str) -> str:
          """Get broader context: what links here + what this links to."""
          return {
              "links_to": forward_links[title],
              "linked_from": backlinks[title]
          }

# =============================================================================
# INTERMEDIATE FILE FORMATS
# =============================================================================

file_formats:
  jsonl: |
    One JSON object per line
    Enables streaming processing
    Easy to inspect with head/tail/grep

  json: |
    Single JSON object
    For small aggregate data (link_counts, category_graph)

  txt: |
    Plain text
    For cleaned and sembr'd content

  npy: |
    NumPy binary format
    Efficient for large embedding arrays

# =============================================================================
# DIRECTORY STRUCTURE
# =============================================================================

directory_structure: |
  pw-mcp/
  ├── prolewiki-exports/     # Source corpus (gitignored)
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  ├── extracted/             # Stage 1: Extraction output
  │   └── articles/
  │       ├── Main/
  │       ├── Library/
  │       ├── Essays/
  │       └── ProleWiki/
  │
  ├── chunks/                # Stage 2: Chunking output (tiktoken-based)
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  ├── embeddings/            # Stage 3: Embedding output
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  ├── chroma_data/           # Stage 4: ChromaDB (gitignored)
  │
  └── graph/                 # (Optional) Graph computation output
      ├── backlinks.jsonl
      ├── link_counts.json
      └── category_graph.json

# =============================================================================
# CLI INTERFACE (proposed)
# =============================================================================

cli:
  commands:
    # Core pipeline (Stages 1-4)
    extract:
      usage: "pw-ingest extract -i prolewiki-exports/ -o extracted/ [--flat]"
      description: "Stage 1: Parse MediaWiki → extracted/articles/*.json"
      status: "implemented"
      options:
        --flat: |
          Process flat directory (infer namespace from filename prefix):
            Essay_* → Essays
            Library_* → Library
            ProleWiki_* → ProleWiki
            (no prefix) → Main
          Used for sample-pipeline/ and similar flat structures.

    chunk:
      usage: "pw-ingest chunk -i extracted/ -o chunks/"
      description: "Stage 2: Create chunks → chunks/*.jsonl (tiktoken-based)"
      status: "implemented"
      note: "Reads directly from extracted/ - no sembr preprocessing needed"

    embed:
      usage: "pw-ingest embed -i chunks/ -o embeddings/ [--provider openai|ollama]"
      description: "Stage 3: Generate embeddings → embeddings/*.npy"
      status: "implemented"
      options:
        --provider: |
          Embedding provider:
            openai (default) - text-embedding-3-large (1536-dim)
            ollama - embeddinggemma (768-dim, local)

    load:
      usage: "pw-ingest load -c chunks/ -e embeddings/ -o chroma_data/"
      description: "Stage 4: Load into ChromaDB"
      status: "planned"

    # Full pipeline
    all:
      usage: "pw-ingest all --source prolewiki-exports/ --output chroma_data/"
      description: "Run full pipeline (Stages 1-4)"
      status: "planned"

    # Optional enhancement
    graph:
      usage: "pw-ingest graph --input extracted/ --output graph/"
      description: "(Optional) Compute backlinks and link counts"
      status: "deferred"
      note: "Not required for core RAG - add after MVP if needed"

    # Utility commands
    validate:
      usage: "pw-ingest validate --stage N --path PATH"
      description: "Validate output of a specific stage"

    stats:
      usage: "pw-ingest stats --path extracted/"
      description: "Print statistics about extracted data"

  # Sample pipeline commands
  sample_pipeline:
    description: "10 public domain works demonstrating full RAG workflow"
    commands:
      sample-status: "mise run sample-status"
      sample-extract: "mise run sample-extract  # Uses --flat flag"
      sample-chunk: "mise run sample-chunk"
      sample-embed: "mise run sample-embed       # OpenAI default"
      sample-embed-ollama: "mise run sample-embed-ollama"
      sample-pipeline: "mise run sample-pipeline # Full workflow"
      sample-clean: "mise run sample-clean"

# =============================================================================
# OPEN QUESTIONS / TODO
# =============================================================================

open_questions:
  - question: "How to handle articles that don't exist yet (red links)?"
    options:
      - Track as separate "missing articles" list
      - Create placeholder entries in link graph
      - Ignore

  - question: "Should Library books be chunked differently?"
    context: "Library contains full books (Capital, etc.) - much longer"
    options:
      - Same chunking, just more chunks
      - Larger chunk size for books
      - Chapter-level metadata

  - question: "Quote formatting in clean text?"
    options:
      - '"Inline quote" - Author, Year'
      - Block quote on separate lines
      - Just quote text, attribution in metadata only

resolved_questions:
  - question: "How to handle templates beyond Citation?"
    resolution: |
      RESOLVED (2025-12-14): Use mwparserfromhell generic template removal.
      - Specific parsers extract metadata from known templates
      - _strip_all_templates() removes ALL 948 template types from clean_text
      - 100% removal rate verified across 5,222 files (20,069 templates)
    implementation: src/pw_mcp/ingest/extraction.py

todo:
  - Define deterministic ref_id generation (hash of normalized fields)
  - Decide on quote formatting for clean_text
  - Performance optimization for Library namespace
  - Incremental update strategy (delta ingestion)

completed:
  - Handle edge cases (malformed refs, nested templates) - mwparserfromhell handles these
  - Template-specific handlers implemented for: Citation (7 types), Infobox (19 types), Library work, Quote
