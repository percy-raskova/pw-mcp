# ProleWiki Ingestion Pipeline Architecture
# Status: Stages 1 & 4 IMPLEMENTED, Stages 2-3 & 5-7 DRAFT
# Last updated: 2025-12-15

# =============================================================================
# DESIGN PRINCIPLES
# =============================================================================

principles:
  normalization: |
    References are normalized into a single source of truth.
    Multiple documents citing the same work → same reference record.
    Reference metadata is enriched from ALL citations of that work.

  debuggability: |
    Each stage outputs intermediate files for inspection.
    Pipeline can be re-run from any stage.

  backlinks: |
    Wiki-style bidirectional linking is core to the architecture.
    Every link A→B also creates a backlink B←A.

  quote_preservation: |
    Quoted text from references is semantically valuable.
    Quotes are extracted and kept inline in clean text.
    Quote source is tracked in metadata.

# =============================================================================
# DATA FLOW OVERVIEW
# =============================================================================

data_flow: |
  prolewiki-exports/
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 1: EXTRACTION                                        │
  │  Parse MediaWiki, extract structured data                   │
  │                                                             │
  │  Outputs:                                                   │
  │    extracted/                                               │
  │    ├── articles/           # Per-article extracted data     │
  │    │   └── {namespace}/{title}.json                         │
  │    ├── references.jsonl    # Normalized reference table     │
  │    ├── links.jsonl         # Link graph (A→B edges)         │
  │    └── categories.jsonl    # Category assignments           │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 2: GRAPH COMPUTATION                                 │
  │  Compute backlinks, category hierarchies, link weights      │
  │                                                             │
  │  Outputs:                                                   │
  │    graph/                                                   │
  │    ├── backlinks.jsonl     # Inverted link index (B←A)      │
  │    ├── link_counts.json    # In-degree/out-degree per article│
  │    └── category_graph.json # Category relationships         │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 3: TEXT CLEANING                                     │
  │  Strip markup, preserve quotes, output plain text           │
  │                                                             │
  │  Outputs:                                                   │
  │    cleaned/                                                 │
  │    └── {namespace}/{title}.txt   # Clean plain text         │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 4: SEMANTIC LINEBREAKING (sembr)                     │
  │  Apply sembr model for 1-line-1-idea structure              │
  │                                                             │
  │  Outputs:                                                   │
  │    sembr/                                                   │
  │    └── {namespace}/{title}.txt   # Sembr'd text             │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 5: CHUNKING                                          │
  │  Group semantic lines into chunks (~200-500 tokens)         │
  │  Attach metadata from earlier stages                        │
  │                                                             │
  │  Outputs:                                                   │
  │    chunks/                                                  │
  │    └── {namespace}/{title}.jsonl  # Chunks with metadata    │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 6: EMBEDDING                                         │
  │  Generate vectors via Ollama embeddinggemma                 │
  │                                                             │
  │  Outputs:                                                   │
  │    embeddings/                                              │
  │    └── {namespace}/{title}.npy   # Embedding arrays         │
  └─────────────────────────────────────────────────────────────┘
       │
       ▼
  ┌─────────────────────────────────────────────────────────────┐
  │  Stage 7: CHROMADB LOADING                                  │
  │  Load chunks + embeddings + metadata into ChromaDB          │
  │                                                             │
  │  Outputs:                                                   │
  │    chroma_data/            # ChromaDB persistence directory │
  └─────────────────────────────────────────────────────────────┘

# =============================================================================
# STAGE 1: EXTRACTION - Detailed
# =============================================================================

stage_1_extraction:
  status: IMPLEMENTED
  implementation: src/pw_mcp/ingest/extraction.py
  tests: 160 passing (6 test files)
  purpose: |
    Parse MediaWiki markup and extract all structured data.
    This is the most complex stage - everything else is simpler.

  input:
    source: prolewiki-exports/{namespace}/{title}.txt
    format: MediaWiki markup

  outputs:
    article_json:
      path: extracted/articles/{namespace}/{title}.json
      schema:
        title: str
        namespace: str  # Main, Library, Essays, ProleWiki
        sections: list[Section]
        clean_text: str  # Markup removed, quotes preserved, infobox removed
        ref_ids: list[str]  # References used in this article
        link_targets: list[str]  # Articles this links to (includes infobox links)
        categories: list[str]
        word_count: int
        has_quotes: bool
        infobox: InfoboxData | null  # Extracted infobox metadata

      infobox_schema:
        type: str  # politician, country, political_party, etc.
        raw_fields: dict[str, str]  # All fields as-is before parsing
        # Parsed fields (type-dependent):
        subject_name: str | null
        native_name: str | null
        birth_date: str | null  # ISO format
        birth_year: int | null
        death_date: str | null
        death_year: int | null
        birth_place: str | null
        death_place: str | null
        nationality: str | null
        death_cause: str | null
        political_orientation: list[str]
        political_party: list[str]
        founded_date: str | null
        founded_year: int | null
        dissolved_date: str | null
        dissolved_year: int | null
        life_span: str | null
        successor: str | null
        predecessor: str | null
        capital: str | null
        government_type: str | null
        essay_author: str | null
        essay_date: str | null
        essay_excerpt: str | null
        conflict_date: str | null
        conflict_result: str | null
        extracted_links: list[str]  # Links found within infobox values

    references_jsonl:
      path: extracted/references.jsonl
      purpose: Normalized reference table (single source of truth)
      schema:
        ref_id: str  # Deterministic hash of normalized citation
        ref_type: str  # Citation, Web, Book, Journal
        authors: list[str]
        year: int | null
        title: str
        source_publication: str | null  # Journal, encyclopedia, etc.
        urls: list[str]  # All URLs (mia, pdf, url, archive)
        quotes: list[QuoteInstance]  # All quotes from this ref
        citing_articles: list[str]  # Articles that cite this ref
        first_seen_in: str  # Article where first encountered

      quote_instance_schema:
        text: str
        article: str
        section: str | null

    links_jsonl:
      path: extracted/links.jsonl
      purpose: Forward link graph
      schema:
        source_article: str  # "Main/Atheism"
        target_article: str  # "Main/Marxism–Leninism"
        link_text: str  # Display text if different from target
        section: str | null  # Section where link appears
        count: int  # Times this link appears in source

    categories_jsonl:
      path: extracted/categories.jsonl
      schema:
        article: str
        categories: list[str]

  extraction_rules:
    references:
      named_ref: |
        <ref name="...">...</ref>
        - Extract name as local identifier
        - Parse inner Citation/Web citation template
        - Generate deterministic ref_id from normalized fields

      inline_ref: |
        <ref>...</ref>
        - No name, single use
        - Still normalize and deduplicate

      ref_reuse: |
        <ref name="..." />
        - Points to earlier named ref
        - Record as citation without re-extracting

      citation_template: |
        {{Citation|author=...|year=...|title=...|...}}
        Fields to extract:
          - author (may be [[linked]])
          - year
          - title
          - chapter
          - url, mia, pdf, chapter-url, archive-url
          - quote (IMPORTANT: preserve this text!)
          - volume, page, pages
          - publisher, city
          - trans-title, trans-lang

      web_citation_template: |
        {{Web citation|...}}
        Additional fields:
          - newspaper
          - archive-date
          - access-date

    internal_links:
      simple: '"[[Target]]" -> target="Target", text="Target"'
      piped: '"[[Target|Display]]" -> target="Target", text="Display"'
      sectioned: '"[[Target#Section]]" -> target="Target", section="Section"'
      category: '"[[Category:Name]]" -> category, not link'
      file: '"[[File:...]]" -> ignore'
      interwiki: '"[[ru:...]]" -> ignore (or separate table?)'

    quotes:
      strategy: |
        1. Extract quote text from |quote= parameter
        2. Track which reference it came from
        3. Insert quote inline where <ref> was (in clean_text)
        4. Format: "quoted text" (attributed to ref_id)

    sections:
      pattern: "^(=+)\\s*(.+?)\\s*\\1$"
      extract:
        level: int  # Number of = signs
        title: str
        line_start: int
        line_end: int

    # -----------------------------------------------------------------
    # INFOBOX EXTRACTION
    # -----------------------------------------------------------------
    infoboxes:
      pattern: |
        {{Infobox TYPE
        |field1=value1
        |field2=value2
        ...
        }}

        May also appear as single-line:
        {{Infobox TYPE|field1=value1|field2=value2|...}}

      types_found:
        # Frequency from corpus analysis:
        politician: 621      # People in politics
        country: 408         # Nations, states
        political_party: 386  # Parties, movements
        person: 228          # Generic people
        revolutionary: 97    # Revolutionary figures
        essay: 50            # ProleWiki essays
        philosopher: 46      # Philosophers
        company: 36          # Companies, corporations
        settlement: 26       # Cities, towns
        military_person: 19  # Military figures
        organization: 17     # Generic orgs
        guerilla_organization: 16
        youtuber: 12
        military_conflict: 12
        book: 8
        religion: 4
        website: 2
        others: "<10 each"   # transcript, library_work, film, etc.

      field_mapping:
        # Fields to EXTRACT (valuable metadata)
        extract:
          # Identity
          - name -> subject_name
          - native_name -> native_name

          # Biographical (people)
          - birth_date -> parse_date -> birth_date, birth_year
          - death_date -> parse_date -> death_date, death_year
          - birth_place -> birth_place (also extract [[links]])
          - death_place -> death_place (also extract [[links]])
          - nationality -> nationality (also extract [[links]])
          - death_cause -> death_cause

          # Political/Ideological
          - political_orientation -> political_orientation (parse [[links]] to list)
          - political_line -> political_orientation (alias)
          - political_party -> political_party (parse [[links]] to list)

          # Organization lifecycle
          - founded -> parse_date -> founded_date, founded_year
          - dissolved -> parse_date -> dissolved_date, dissolved_year
          - life_span -> life_span
          - successor -> successor (extract [[link]])
          - split -> predecessor (extract [[link]])
          - predecessor -> predecessor

          # Geographic
          - capital -> capital
          - government_type -> government_type

          # Essay-specific
          - author -> essay_author
          - date -> essay_date
          - excerpt -> essay_excerpt

          # Conflict-specific
          - date -> conflict_date (for military_conflict type)
          - result -> conflict_result

        # Fields to DISCARD (not useful for search)
        discard:
          - image
          - image_size
          - image_upright
          - image_alt
          - caption
          - logo
          - image_flag
          - honorific_prefix
          - honorific_suffix
          - signature
          - signature_size
          - combatant1, combatant2  # Too complex, [[links]] extracted separately
          - commander1, commander2  # Same

      date_parsing:
        templates:
          - '"{{birth date|YYYY|MM|DD}}" -> "YYYY-MM-DD"'
          - '"{{death date|YYYY|MM|DD}}" -> "YYYY-MM-DD"'
          - '"{{death date and age|YYYY|MM|DD|...}}" -> "YYYY-MM-DD"'
          - '"DD Month YYYY" -> "YYYY-MM-DD"'
          - '"Month DD, YYYY" -> "YYYY-MM-DD"'
          - '"YYYY" -> year only (birth_year/death_year)'
          - '"YYYY–YYYY" -> life_span (keep as string)'

      link_extraction:
        purpose: |
          Many infobox values contain [[internal links]].
          These are valuable for the link graph!

        examples:
          - input: "birth_place=[[Moscow]], [[Soviet Union]]"
            extract: '["Moscow", "Soviet Union"]'
            action: Add to article link_targets

          - input: "political_party=[[CPSU]]"
            extract: '["CPSU"]'
            action: Store in political_party field AND link_targets

          - input: "political_orientation=[[Islamic socialism]]<br>[[Arab socialism]]"
            extract: '["Islamic socialism", "Arab socialism"]'
            action: Store as list in political_orientation field

      removal_from_text:
        purpose: |
          After extraction, the entire infobox is REMOVED from clean_text.
          This improves sembr processing and embedding quality.

        pattern: |
          Remove from start of {{Infobox to matching }}
          Handle nested {{ }} carefully

        result: |
          Infobox metadata → stored in article.infobox
          Infobox text → NOT in clean_text (no sembr, no embedding)
          Infobox links → added to link_targets

    # -----------------------------------------------------------------
    # ADDITIONAL TEMPLATES (beyond infoboxes)
    # -----------------------------------------------------------------
    additional_templates:

      # =================================================================
      # LIBRARY WORK TEMPLATE - Rich metadata for Library namespace
      # =================================================================
      library_work:
        pattern: |
          {{Library work
          |title=...
          |author=...
          |published_date=...
          |type=...
          |source=...
          }}

        frequency: "~15+ in sample, prevalent in Library/ namespace"

        field_mapping:
          extract:
            - title -> library_work_title
            - author -> library_work_author
            - written_in -> library_work_written_date
            - published_date -> library_work_published_date
            - published_location -> library_work_location
            - edition_date -> library_work_edition_date
            - type -> library_work_type  # Book, Speech, Research paper, etc.
            - source -> library_work_source_url
            - translated_by -> library_work_translator
            - original_language -> library_work_original_language
            - publisher -> library_work_publisher
            - audiobook -> library_work_audiobook_url

          discard:
            - image
            - image_size
            - image_upright
            - image_alt
            - caption

        action: |
          1. Extract all fields to article.library_work metadata
          2. Remove entire template from clean_text
          3. Add to link_targets if author/translator contain [[links]]

      # =================================================================
      # FOREWORD TEMPLATE - Rich semantic content
      # =================================================================
      foreword:
        pattern: "{{Foreword|foreword=...}}"
        frequency: "Rare but semantically valuable"

        action: |
          1. Extract foreword text
          2. KEEP in clean_text (valuable for embedding)
          3. Tag section as foreword in metadata
          4. Format as block quote with "Foreword:" prefix

      # =================================================================
      # NAVIGATION TEMPLATES (Library chapters)
      # =================================================================
      book_navigation:
        pattern: |
          {{Book_Navigation
          |previous=Library:...
          |previous_title=...
          |next=Library:...
          |next_title=...
          |current=...
          }}

        frequency: "~7+ in Library chapters"

        field_mapping:
          extract:
            - previous -> chapter_previous  # Link to previous chapter
            - next -> chapter_next  # Link to next chapter
            - current -> chapter_current  # Current chapter title

        action: |
          1. Extract navigation metadata
          2. Add previous/next to link_targets
          3. Remove template from clean_text (navigation noise)

      book_toc:
        pattern: "{{Book_TOC|...}}"
        frequency: "~7 in Library chapters"

        action: |
          1. Extract chapter list as structured data
          2. Remove from clean_text (table of contents noise)
          3. Store as book_chapters metadata (list of chapter links)

      chapter_change_title:
        pattern: "{{Chapter_change_title}}"
        action: "Remove from clean_text (display control only)"

      # =================================================================
      # MAIN ARTICLE LINKS (valuable for graph!)
      # =================================================================
      main_article:
        pattern: "{{Main article|Target Article}}"
        frequency: "~10+ across corpus"

        action: |
          1. Extract target article as related_main_article
          2. Add to link_targets (important relationship!)
          3. Remove template from clean_text
          4. Store association: this section → main article

        note: |
          These indicate "this section summarizes, see full article"
          High-value signal for related content discovery

      # =================================================================
      # STATUS MARKERS
      # =================================================================
      stub_markers:
        patterns:
          - "{{Stub}}"
          - "{{Message box/Stub}}"

        frequency: "~9 in sample"

        action: |
          1. Set article.is_stub = true
          2. Remove template from clean_text
          3. Potentially lower ranking in search results

      citation_needed:
        pattern: "{{Citation needed}}"
        frequency: "4 in sample"

        action: |
          1. Count occurrences → article.citation_needed_count
          2. Remove from clean_text
          3. Track location for potential future editorial work

      # =================================================================
      # CITATION TEMPLATE VARIANTS
      # =================================================================
      citation_variants:
        types:
          web_citation:
            pattern: "{{Web citation|...}}"
            frequency: "71 in sample (most common)"
            fields: [author, newspaper, title, url, date, archive-url, archive-date]

          news_citation:
            pattern: "{{News citation|...}}"
            frequency: "39 in sample"
            fields: [author, newspaper, title, url, date]

          library_citation:
            pattern: "{{Library citation|link=...}}"
            frequency: "Rare, links to Library namespace"
            note: "Internal link to Library work - valuable for graph!"

          video_citation:
            pattern: "{{Video citation|url=...|channel=...|title=...}}"
            frequency: "Rare"
            fields: [url, channel, title]

          youtube_citation:
            pattern: "{{YouTube citation|...}}"
            frequency: "~3 in sample"
            note: "Variant of video citation"

        action: |
          All citation templates are handled within <ref> tags.
          Parse type-specific fields and normalize to reference table.
          Distinguish ref_type: Web, News, Library, Video, etc.

      # =================================================================
      # DISPLAY CONTROL (remove, no extraction)
      # =================================================================
      display_control:
        templates:
          DEFAULTSORT:
            pattern: "{{DEFAULTSORT:Sort Key}}"
            action: "Remove from clean_text (affects category sorting only)"

          DISPLAYTITLE:
            pattern: "{{DISPLAYTITLE:Title with formatting}}"
            action: "Remove from clean_text (display only)"

          TOC_limit:
            pattern: "{{TOC limit|N}}"
            action: "Remove (table of contents depth control)"

      # =================================================================
      # SIDEBARS AND NAVBOXES (remove, extract links)
      # =================================================================
      sidebars:
        patterns:
          - "{{Marxism–Leninism sidebar}}"
          - "{{TKNsidebar}}"
          - "{{Capital, vol. I sidebar}}"
          - "{{*sidebar}}"  # Generic pattern

        action: |
          1. Identify sidebar type
          2. Remove from clean_text (navigation clutter)
          3. Optionally: set article.sidebar_type for categorization

      navboxes:
        patterns:
          - "{{Navbox CPC Constitution}}"
          - "{{Navbox *}}"  # Generic pattern

        action: |
          1. Remove from clean_text
          2. Navboxes often appear at article end (after References)
          3. No metadata extraction needed

      # =================================================================
      # TRANSCLUSION MARKERS
      # =================================================================
      transclusion:
        noinclude:
          pattern: "<noinclude>...</noinclude>"
          content: "Navigation, footnotes, categories (for transcluded pages)"

          action: |
            1. For Library chapters: noinclude often contains Book_Navigation
            2. Extract navigation metadata BEFORE removing
            3. Remove noinclude wrapper and content from clean_text
            4. Exception: keep content if it's Footnotes section

        onlyinclude:
          pattern: "<onlyinclude>...</onlyinclude>"
          action: |
            1. This IS the main content (for transcluded pages)
            2. Keep content, remove wrapper tags
            3. Common in Library chapters

    # -----------------------------------------------------------------
    # STRUCTURAL SECTIONS
    # -----------------------------------------------------------------
    structural_sections:
      see_also:
        pattern: "== See also =="
        frequency: "Common in Main/ articles"

        action: |
          1. Extract all [[links]] from this section
          2. Store as article.see_also (valuable related articles)
          3. Add to link_targets
          4. Optionally: exclude section from embedding (just links)

      external_links:
        pattern: "== External links =="
        action: |
          1. Extract URLs
          2. Remove section from clean_text (not valuable for embedding)
          3. Store as article.external_links

      references_section:
        patterns:
          - "== References =="
          - "== Footnotes =="
          - "== Notes =="
          - "<references />"
          - "<references group=\"lower-alpha\" />"

        action: |
          1. Remove entire section from clean_text
          2. References already extracted from <ref> tags
          3. <references /> just renders them (no new content)

    # -----------------------------------------------------------------
    # BLOCKQUOTES
    # -----------------------------------------------------------------
    blockquotes:
      pattern: "<blockquote>...</blockquote>"
      frequency: "Common in Essays and Library"

      action: |
        1. KEEP in clean_text (semantically valuable!)
        2. Extract attribution if present (often at end: "— Author")
        3. Format for embedding: preserve quote structure
        4. Tag chunk as has_blockquote = true

      attribution_patterns:
        - "— Author Name"
        - "-- [[Author Name]]"
        - "- Author, Title"

# =============================================================================
# STAGE 2: GRAPH COMPUTATION - Detailed
# =============================================================================

stage_2_graph:
  purpose: |
    Compute derived graph data from extracted links.
    Backlinks are the inverse of forward links.

  outputs:
    backlinks_jsonl:
      path: graph/backlinks.jsonl
      purpose: "Which articles link TO this article?"
      schema:
        target_article: str  # The article being linked to
        source_articles: list[BacklinkSource]

      backlink_source_schema:
        article: str
        section: str | null
        link_text: str
        count: int

    link_counts_json:
      path: graph/link_counts.json
      purpose: PageRank-style importance metrics
      schema:
        article: str
        in_degree: int   # How many articles link here
        out_degree: int  # How many articles this links to
        # Future: pagerank score

    category_graph_json:
      path: graph/category_graph.json
      purpose: Category co-occurrence and hierarchy
      schema:
        categories: list[CategoryInfo]

      category_info_schema:
        name: str
        article_count: int
        co_occurs_with: dict[str, int]  # Other categories and overlap count

# =============================================================================
# STAGE 3: TEXT CLEANING - Detailed
# =============================================================================

stage_3_cleaning:
  purpose: |
    Convert MediaWiki markup to clean plain text.
    Preserve semantic content, remove syntax noise.

  input:
    article_json: extracted/articles/{namespace}/{title}.json
    # Use clean_text field, or re-process from source

  output:
    path: cleaned/{namespace}/{title}.txt
    format: Plain text with preserved structure

  transformations:
    remove:
      - '"<ref>...</ref>" tags (content already extracted)'
      - '"<ref name="..." />" self-closing tags'
      - '"{{...}}" templates (except quotes already extracted)'
      - '"[[Category:...]]" (already extracted)'
      - '"<references />" and "==References==" section'
      - 'HTML comments "<!-- ... -->"'
      - '"<nowiki>...</nowiki>" tags (keep inner text)'
      - 'Multiple consecutive blank lines -> single blank line'

    transform:
      internal_links: |
        [[Target]] → Target
        [[Target|Display]] → Display
        [[Target#Section|Display]] → Display

      formatting: |
        '''bold''' → bold
        ''italic'' → italic

      lists: |
        * item → • item (or just "item")
        # item → 1. item (or just "item")

      headers: |
        == Title == → PRESERVED (for section detection in chunking)
        === Subtitle === → PRESERVED

    insert:
      quotes: |
        Where <ref> tag was, insert extracted quote:
        "Fear made the gods..." — Lenin, 1909

        Format TBD - could be:
        - Inline with attribution
        - Block quote on new line
        - Just the quote text (attribution in metadata)

# =============================================================================
# STAGE 4: SEMBR - Detailed
# =============================================================================

stage_4_sembr:
  status: IMPLEMENTED
  completed_date: 2025-12-15
  reference_doc: ai-docs/sembr.yaml
  tests: 33 passing

  purpose: |
    Apply semantic linebreaking so each line = one idea.
    Enables better chunking at semantic boundaries.

  input:
    primary: extracted/articles/{namespace}/{title}.json  # clean_text field
    alternative: cleaned/{namespace}/{title}.txt
    note: |
      Phase 2 extraction produces clean_text with all templates removed.
      This is the input for sembr processing.

  output: sembr/{namespace}/{title}.txt

  implementation:
    module: src/pw_mcp/ingest/linebreaker.py (487 lines)
    cli: src/pw_mcp/ingest/cli.py (362 lines)
    classes:
      SembrConfig:
        server_url: "http://localhost:8384"
        model_name: "admko/sembr2023-distilbert-base-multilingual-cased"
        timeout_seconds: 60.0
        max_retries: 3
        retry_delay_seconds: 1.0
        batch_size: 8
        predict_func: "argmax"

      SembrResult:
        text: str  # Processed output
        line_count: int
        processing_time_ms: float
        input_word_count: int
        output_word_count: int

    exceptions:
      - SembrError (base)
      - SembrServerError
      - SembrTimeoutError
      - SembrContentError

    functions:
      - check_server_health(config) -> bool
      - process_text(text, config) -> SembrResult  # async
      - process_file(input_path, output_path, config) -> SembrResult  # async
      - process_batch(input_dir, output_dir, config, callback, max_concurrent) -> list[SembrResult]  # async

  cli_usage: |
    # Check server health
    pw-ingest sembr --check-only

    # Process full corpus
    pw-ingest sembr -i extracted/ -o sembr/

    # Test with sample files
    pw-ingest sembr --sample 10

  mise_tasks:
    - sembr-server: Start sembr server
    - sembr-check: Check server health
    - sembr-process: Process full corpus
    - sembr-sample: Test with sample files

  server_protocol:
    description: |
      sembr exposes a Flask HTTP server for batch processing.
      Server mode keeps model loaded in memory.

    endpoints:
      health_check:
        path: /check
        method: GET
        response: '{"status": "success", "model": "...", "tokenizer": "..."}'

      rewrap:
        path: /rewrap
        method: POST
        content_type: application/x-www-form-urlencoded
        body: "text=<url-encoded-text>"
        response: '{"status": "success", "text": "processed\ntext"}'

  execution:
    mode: server  # REQUIRED - CLI mode too slow (~9s/file)
    server_url: "http://localhost:8384"
    model: "admko/sembr2023-distilbert-base-multilingual-cased"
    model_params: 135M
    languages: [English, Russian, Chinese, Spanish, German, French]

    startup_command: |
      mise run sembr-server
      # or: uv run sembr --listen -p 8384 -m admko/sembr2023-distilbert-base-multilingual-cased

  performance:
    cli_mode: "~9 seconds per file (model load + inference)"
    server_mode: "~0.2-0.5 seconds per file (inference only)"
    speedup: "10-50x faster for batch processing"
    full_corpus_estimate:
      cli: "13-16 hours for 5,222 files"
      server: "<2 hours for 5,222 files"

  content_invariants:
    - "All input words present in output (no content loss)"
    - "Word count: input == output"
    - "Section headers passed through unchanged"
    - "Unicode preserved (Cyrillic, Chinese, etc.)"
    - "Blank lines preserved as paragraph boundaries"

  notes: |
    - Headers passed through unchanged
    - Blank lines preserved as paragraph boundaries
    - Server mode REQUIRED for practical batch processing
    - See ai-docs/sembr.yaml for complete specification

# =============================================================================
# STAGE 5: CHUNKING - Detailed
# =============================================================================

stage_5_chunking:
  purpose: |
    Group semantic lines into coherent chunks.
    Attach all metadata from earlier stages.

  input:
    sembr_text: sembr/{namespace}/{title}.txt
    article_json: extracted/articles/{namespace}/{title}.json
    references: extracted/references.jsonl
    backlinks: graph/backlinks.jsonl

  output:
    path: chunks/{namespace}/{title}.jsonl
    schema:
      chunk_id: str  # "{namespace}/{title}#{index}"
      text: str

      # Article-level metadata
      article_title: str
      namespace: str
      categories: str  # JSON array
      internal_links: str  # JSON array (links FROM this article)
      backlinks: str  # JSON array (links TO this article)

      # Chunk-level metadata
      section: str | null
      chunk_index: int
      line_range: str  # "start-end"
      word_count: int

      # Reference-derived metadata
      cited_ref_ids: str  # JSON array of ref_ids cited in this chunk
      cited_authors: str  # JSON array (aggregated from refs)
      cited_years: str  # JSON array
      has_refs: bool
      ref_count: int
      has_quotes: bool

  chunking_algorithm:
    target_tokens: 350
    min_tokens: 100
    max_tokens: 500

    boundaries:
      hard:
        - Section headers (== ... ==)
        - Never split mid-section unless section > max_tokens
      soft:
        - Paragraph breaks (blank lines)
        - Prefer breaking at paragraph boundaries

    grouping: |
      1. Parse sembr output into semantic lines
      2. Group lines into paragraphs (separated by blank lines)
      3. Group paragraphs into chunks respecting token limits
      4. If section header encountered, start new chunk
      5. Track line numbers for line_range metadata

# =============================================================================
# STAGE 6: EMBEDDING - Detailed
# =============================================================================

stage_6_embedding:
  purpose: Generate vector embeddings for each chunk

  input: chunks/{namespace}/{title}.jsonl
  output: embeddings/{namespace}/{title}.npy

  model:
    provider: ollama
    model_name: embeddinggemma
    dimensions: 768
    base_url: "http://localhost:11434"

  batching:
    batch_size: 32  # Chunks per API call
    # Balance memory vs API overhead

  output_format: |
    NumPy array shape: (num_chunks, 768)
    Chunk order matches JSONL line order

    Alternative: embed directly into chunks JSONL
    (adds ~6KB per chunk for base64-encoded embedding)

# =============================================================================
# STAGE 7: CHROMADB LOADING - Detailed
# =============================================================================

stage_7_loading:
  purpose: Load everything into ChromaDB

  input:
    chunks: chunks/**/*.jsonl
    embeddings: embeddings/**/*.npy
    references: extracted/references.jsonl  # Load as separate collection?

  output: chroma_data/

  collections:
    prolewiki_chunks:
      purpose: Main searchable collection
      hnsw_config:
        space: cosine
        ef_construction: 200
        ef_search: 100
        # All chunk metadata from Stage 5 schema

    prolewiki_references:
      purpose: Reference lookup table (optional separate collection)
      note: |
        Alternative: store refs as JSON in chunks metadata
        Separate collection allows ref-specific queries:
        - "All works by Lenin"
        - "All citations from 1917"

      schema:
        ref_id: str
        ref_type: str
        authors: str  # JSON
        year: int
        title: str
        source_publication: str
        citing_articles: str  # JSON
        # No embedding needed - exact match queries only

  loading_strategy:
    batch_size: 5000  # Below max_batch_size
    upsert: true  # Allow re-running without duplicates

    id_format: |
      Chunks: "{namespace}/{title}#{chunk_index}"
      Refs: "ref:{ref_id}"

      Deterministic IDs enable idempotent loading

# =============================================================================
# INTERMEDIATE FILE FORMATS
# =============================================================================

file_formats:
  jsonl: |
    One JSON object per line
    Enables streaming processing
    Easy to inspect with head/tail/grep

  json: |
    Single JSON object
    For small aggregate data (link_counts, category_graph)

  txt: |
    Plain text
    For cleaned and sembr'd content

  npy: |
    NumPy binary format
    Efficient for large embedding arrays

# =============================================================================
# DIRECTORY STRUCTURE
# =============================================================================

directory_structure: |
  pw-mcp/
  ├── prolewiki-exports/     # Source (gitignored)
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  ├── extracted/             # Stage 1 output
  │   ├── articles/
  │   │   ├── Main/
  │   │   ├── Library/
  │   │   ├── Essays/
  │   │   └── ProleWiki/
  │   ├── references.jsonl
  │   ├── links.jsonl
  │   └── categories.jsonl
  │
  ├── graph/                 # Stage 2 output
  │   ├── backlinks.jsonl
  │   ├── link_counts.json
  │   └── category_graph.json
  │
  ├── cleaned/               # Stage 3 output
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  ├── sembr/                 # Stage 4 output
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  ├── chunks/                # Stage 5 output
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  ├── embeddings/            # Stage 6 output
  │   ├── Main/
  │   ├── Library/
  │   ├── Essays/
  │   └── ProleWiki/
  │
  └── chroma_data/           # Stage 7 output (gitignored)

# =============================================================================
# CLI INTERFACE (proposed)
# =============================================================================

cli:
  commands:
    extract:
      usage: "pw-ingest extract --source prolewiki-exports/ --output extracted/"
      description: "Run Stage 1: MediaWiki extraction"

    graph:
      usage: "pw-ingest graph --input extracted/ --output graph/"
      description: "Run Stage 2: Compute backlinks and graph metrics"

    clean:
      usage: "pw-ingest clean --input extracted/ --output cleaned/"
      description: "Run Stage 3: Strip markup to plain text"

    sembr:
      usage: "pw-ingest sembr --input cleaned/ --output sembr/ [--server URL]"
      description: "Run Stage 4: Semantic linebreaking"

    chunk:
      usage: "pw-ingest chunk --sembr sembr/ --extracted extracted/ --graph graph/ --output chunks/"
      description: "Run Stage 5: Create chunks with metadata"

    embed:
      usage: "pw-ingest embed --input chunks/ --output embeddings/ [--model embeddinggemma]"
      description: "Run Stage 6: Generate embeddings"

    load:
      usage: "pw-ingest load --chunks chunks/ --embeddings embeddings/ --output chroma_data/"
      description: "Run Stage 7: Load into ChromaDB"

    all:
      usage: "pw-ingest all --source prolewiki-exports/ --output chroma_data/"
      description: "Run full pipeline"

    # Utility commands
    validate:
      usage: "pw-ingest validate --stage N --path PATH"
      description: "Validate output of a specific stage"

    stats:
      usage: "pw-ingest stats --path extracted/"
      description: "Print statistics about extracted data"

# =============================================================================
# OPEN QUESTIONS / TODO
# =============================================================================

open_questions:
  - question: "How to handle articles that don't exist yet (red links)?"
    options:
      - Track as separate "missing articles" list
      - Create placeholder entries in link graph
      - Ignore

  - question: "Should Library books be chunked differently?"
    context: "Library contains full books (Capital, etc.) - much longer"
    options:
      - Same chunking, just more chunks
      - Larger chunk size for books
      - Chapter-level metadata

  - question: "Quote formatting in clean text?"
    options:
      - '"Inline quote" - Author, Year'
      - Block quote on separate lines
      - Just quote text, attribution in metadata only

resolved_questions:
  - question: "How to handle templates beyond Citation?"
    resolution: |
      RESOLVED (2025-12-14): Use mwparserfromhell generic template removal.
      - Specific parsers extract metadata from known templates
      - _strip_all_templates() removes ALL 948 template types from clean_text
      - 100% removal rate verified across 5,222 files (20,069 templates)
    implementation: src/pw_mcp/ingest/extraction.py

todo:
  - Define deterministic ref_id generation (hash of normalized fields)
  - Decide on quote formatting for clean_text
  - Performance optimization for Library namespace
  - Incremental update strategy (delta ingestion)

completed:
  - Handle edge cases (malformed refs, nested templates) - mwparserfromhell handles these
  - Template-specific handlers implemented for: Citation (7 types), Infobox (19 types), Library work, Quote
