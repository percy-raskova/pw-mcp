# Phase 3: SEmBr (Semantic Linebreaking) Integration
# Status: IMPLEMENTED - All tasks complete + GPU lifecycle management
# Purpose: Complete specification for sembr integration in pw-mcp pipeline
# Last updated: 2025-12-16 (GPU manager, server manager, graceful degradation)

# =============================================================================
# OVERVIEW
# =============================================================================

overview:
  purpose: |
    Apply semantic linebreaking to clean_text output from Phase 2 extraction.
    Each line in output represents roughly one semantic idea, enabling
    intelligent chunking at idea boundaries rather than arbitrary word counts.

  data_flow: |
    extracted/articles/{ns}/{title}.json → clean_text field
                         ↓
    linebreaker.py (HTTP client to sembr server)
                         ↓
    sembr/{namespace}/{title}.txt

  why_sembr: |
    Without sembr: Long paragraphs chunked arbitrarily at word boundaries
    With sembr: Text broken at semantic boundaries for coherent chunks

    Example input:
      "Stalin implemented the Five-Year Plans. These transformed the
       Soviet Union from an agrarian society into an industrial power."

    Example output:
      "Stalin implemented the Five-Year Plans.
       These transformed the Soviet Union
       from an agrarian society
       into an industrial power."

  model: admko/sembr2023-distilbert-base-multilingual-cased
  model_params: 135M parameters
  multilingual_support: [English, Russian, Chinese, Spanish, German, French]

# =============================================================================
# SEMBR HTTP SERVER PROTOCOL
# =============================================================================

server_protocol:
  description: |
    sembr exposes a Flask HTTP server for batch processing.
    Server mode keeps model loaded in memory, avoiding 9s startup per file.

  startup_command: |
    sembr --listen -p 8384 -m admko/sembr2023-distilbert-base-multilingual-cased

  endpoints:
    health_check:
      path: /check
      method: GET
      response_example: |
        {
          "status": "success",
          "model": "BertForTokenClassification",
          "tokenizer": "DistilBertTokenizerFast",
          "processor": "SemBrProcessor"
        }
      use_for: Verify server is running before batch processing

    rewrap:
      path: /rewrap
      method: POST
      content_type: application/x-www-form-urlencoded
      request_fields:
        text: "(required) Plain text to process"
        batch_size: "(optional) Inference batch size, default 8"
        predict_func: "(optional) argmax|logit_adjustment|greedy_linebreaks"
        tokens_per_line: "(optional) Max tokens per line for greedy mode"
        overlap_divisor: "(optional) Overlap for tiled inference, default 8"

      success_response: |
        {
          "status": "success",
          "model": "BertForTokenClassification",
          "tokenizer": "DistilBertTokenizerFast",
          "processor": "SemBrProcessor",
          "batch_size": 8,
          "predict_func": "argmax",
          "text": "Processed text\nwith semantic\nline breaks."
        }

      error_response: |
        {
          "status": "error",
          "model": "BertForTokenClassification",
          "tokenizer": "DistilBertTokenizerFast",
          "processor": "SemBrProcessor",
          "error": "Error message",
          "traceback": "Full Python traceback"
        }

  performance:
    cli_mode: "~9 seconds per file (model load + inference)"
    server_mode: "~0.2-0.5 seconds per file (inference only)"
    speedup: "10-50x faster for batch processing"
    full_corpus_estimate:
      cli: "13-16 hours for 5,222 files"
      server: "<2 hours for 5,222 files"

# =============================================================================
# MODULE DESIGN: linebreaker.py
# =============================================================================

module_design:
  location: src/pw_mcp/ingest/linebreaker.py

  dataclasses:
    SembrConfig:
      fields:
        server_url: "str = 'http://localhost:8384'"
        model_name: "str = 'admko/sembr2023-distilbert-base-multilingual-cased'"
        timeout_seconds: "float = 60.0"
        max_retries: "int = 3"
        retry_delay_seconds: "float = 1.0"
        batch_size: "int = 8"
        predict_func: "str = 'argmax'"

    SembrResult:
      fields:
        text: "str - Processed text with semantic line breaks"
        line_count: "int - Number of output lines"
        processing_time_ms: "float - Processing duration in milliseconds"
        input_word_count: "int - Word count of input (for validation)"
        output_word_count: "int - Word count of output (should match input)"

  exceptions:
    SembrError: "Base exception for all sembr errors"
    SembrServerError: "Server unavailable or connection failed"
    SembrTimeoutError: "Request timed out"
    SembrContentError: "Content validation failed (word count mismatch)"

  public_api:
    check_server_health:
      signature: "def check_server_health(config: SembrConfig | None = None) -> bool"
      description: "Check if sembr server is running and responsive"
      implementation: |
        1. GET request to {config.server_url}/check
        2. Return True if status code 200 and status="success"
        3. Return False on connection error or timeout
        4. Timeout: 0.3 seconds (fast fail)

    process_text:
      signature: |
        async def process_text(
            text: str,
            config: SembrConfig | None = None
        ) -> SembrResult
      description: "Process text through sembr server"
      implementation: |
        1. Validate input (non-empty string)
        2. If text is empty/whitespace only, return empty SembrResult
        3. Count input words (for validation)
        4. POST to {config.server_url}/rewrap with form data
        5. Retry on failure (up to max_retries with exponential backoff)
        6. Parse JSON response
        7. Validate output word count matches input
        8. Return SembrResult

    process_file:
      signature: |
        async def process_file(
            input_path: Path,
            output_path: Path,
            config: SembrConfig | None = None
        ) -> SembrResult
      description: "Process a single file through sembr"
      implementation: |
        1. Read input file (UTF-8)
        2. Call process_text()
        3. Create output directory if needed
        4. Write output file (UTF-8)
        5. Return result

    process_batch:
      signature: |
        async def process_batch(
            input_dir: Path,
            output_dir: Path,
            config: SembrConfig | None = None,
            progress_callback: Callable[[int, int, str], None] | None = None,
            max_concurrent: int = 10
        ) -> list[SembrResult | None]
      description: "Process all .txt and .json files in directory through sembr"
      implementation: |
        1. Glob for all .txt and .json files in input_dir (recursive)
        2. Check server health, raise if unavailable
        3. For each file:
           a. Compute output path (preserve namespace subdirs, always .txt output)
           b. Call process_file() wrapped in try/except
           c. Call progress_callback(current, total, filename)
           d. On error: return None for failed file (graceful degradation)
        4. Use asyncio.Semaphore for concurrency control
        5. Return list of results (None values indicate failures)
      enhancements_added:
        - "JSON file support: extracts clean_text from .json files"
        - "Graceful errors: failed files return None, batch continues"
        - "Output always .txt regardless of input format"

  helper_functions:
    _extract_clean_text_from_json:
      signature: "def _extract_clean_text_from_json(json_path: Path) -> str"
      description: |
        Extract clean text from JSON file.
        Supports both:
        - "clean_text" field (Phase 2 extraction output)
        - "text" field (sembr response output)
        Raises ValueError if neither field found.

    _validate_content_preservation:
      signature: "def _validate_content_preservation(input_text: str, output_text: str) -> bool"
      description: |
        Validate that no content was lost during processing.
        Compare word sets (order-independent) between input and output.

# =============================================================================
# CLI INTEGRATION
# =============================================================================

cli_integration:
  location: src/pw_mcp/ingest/cli.py

  subcommand: sembr
  usage: "pw-ingest sembr [OPTIONS]"

  options:
    --input:
      short: -i
      description: "Input directory (extracted/ or cleaned/)"
      default: extracted/

    --output:
      short: -o
      description: "Output directory for sembr'd files"
      default: sembr/

    --server:
      short: -s
      description: "Sembr server URL"
      default: "http://localhost:8384"

    --check-only:
      description: "Only check server health, don't process"
      flag: true

    --sample:
      description: "Process only N files (for testing)"
      type: int

    --progress:
      short: -p
      description: "Show progress bar"
      flag: true
      default: true

    --restart-server:
      description: "Restart sembr server before processing (default: true)"
      flag: true
      default: true
      note: "Ensures clean CUDA state by killing existing server, clearing cache, starting fresh"

    --no-restart:
      description: "Don't restart server (use existing instance)"
      flag: true
      note: "Use when server is already running and healthy"

    --gpu:
      description: "Specific GPU device ID to use"
      type: int
      default: "auto-detect"
      note: "If not specified, uses first available GPU with <90% memory usage"

    --no-gpu-failover:
      description: "Disable automatic GPU failover on errors"
      flag: true
      default: false
      note: "When disabled, server errors are fatal instead of triggering failover"

    --health-interval:
      description: "Seconds between health checks during batch processing"
      type: float
      default: 60.0
      note: "More frequent checks catch errors faster but add overhead"

    --verbose:
      short: -v
      description: "Enable verbose logging output"
      flag: true
      note: "Shows DEBUG level logs on console (always logged to file)"

  server_management:
    description: |
      The sembr server is single-threaded and can enter corrupted CUDA state.
      Auto-restart feature ensures clean state before batch processing.
      As of Phase 5.5, this is handled by dedicated modules:
      - gpu_manager.py: GPU detection, health checking, CUDA state management
      - server_manager.py: Server lifecycle management with graceful degradation

    modules:
      gpu_manager:
        location: src/pw_mcp/ingest/gpu_manager.py
        description: "GPU detection, health monitoring, and CUDA state management"
        public_api:
          - detect_gpus(): "Detect all NVIDIA GPUs via nvidia-smi"
          - check_gpu_health(gpu_id): "Check health of specific GPU"
          - get_available_gpu(exclude): "Get first healthy GPU, optionally excluding some"
          - reset_cuda_cache(gpu_id): "Clear CUDA cache via PyTorch subprocess"
          - is_cuda_corrupted(gpu_id): "Heuristic check for CUDA corruption"
          - get_gpu_processes(gpu_id): "List processes using GPU"
          - kill_gpu_processes(gpu_id): "Kill processes on GPU"
          - get_cuda_env(gpu_id): "Get env vars for running on specific GPU"

      server_manager:
        location: src/pw_mcp/ingest/server_manager.py
        description: "Server lifecycle management with graceful degradation"
        configs:
          - SEMBR_CONFIG: "Pre-configured for sembr server (port 8384, /check endpoint)"
          - OLLAMA_CONFIG: "Pre-configured for Ollama server (port 11434, /api/tags endpoint)"
        public_api:
          - start_server(config, gpu_id): "Start server on specified GPU"
          - stop_server(server_type, graceful): "Stop server (SIGTERM, then SIGKILL)"
          - restart_server(server_type, new_gpu): "Restart with optional GPU failover"
          - check_server_health(server_type): "HTTP health check"
          - setup_signal_handlers(): "Register SIGINT/SIGTERM handlers"
          - register_cleanup(): "Register atexit handler"
          - HealthMonitor: "Class for periodic health checks during batch"

    functions:
      _restart_sembr_server:
        description: "Kill existing server, clear CUDA cache, start fresh instance"
        implementation: "Now delegates to server_manager.stop_server() and start_server()"
        steps:
          - "1. stop_server('sembr', graceful=True) (SIGTERM with CUDA cleanup)"
          - "2. _reset_cuda_device() (additional CUDA cache clear)"
          - "3. start_server(SEMBR_CONFIG, gpu_id) (with GPU environment)"
          - "4. Poll /check until healthy or timeout"

      _reset_cuda_device:
        description: "Attempt to clear CUDA cache via PyTorch"
        code: |
          python3 -c "import torch; torch.cuda.empty_cache(); torch.cuda.reset_peak_memory_stats()"
        note: "For persistent errors, suggest: sudo nvidia-smi --gpu-reset -i 0"

    health_monitoring:
      description: |
        During batch processing, HealthMonitor periodically checks server health.
        On failure, it attempts recovery via server restart with optional GPU failover.
      parameters:
        interval: "Seconds between health checks (--health-interval)"
        max_recovery_attempts: "3 with failover enabled, 1 without"
      recovery_flow:
        - "1. Health check fails"
        - "2. Call restart_server() with new GPU if failover enabled"
        - "3. If recovery succeeds, continue batch"
        - "4. If max attempts exceeded, raise RuntimeError"

    errors_handled:
      already_borrowed:
        cause: "Server single-threaded, concurrent requests blocked"
        fix: "Added retry jitter to spread out retries"
        code: "jitter = random.uniform(0, base_delay) if is_busy else 0"

      cuda_device_side_assert:
        cause: "GPU state corruption from concurrent access or OOM"
        fix: "Restart server + clear CUDA cache + failover to alternate GPU"
        note: "May require nvidia-smi --gpu-reset for persistent errors"

      orphaned_gpu_memory:
        cause: "High GPU memory usage but no running processes"
        detection: "is_cuda_corrupted() checks nvidia-smi for this pattern"
        fix: "kill_gpu_processes() + reset_cuda_cache() + server restart"

      empty_response_body:
        cause: "Server returns HTTP 200 but empty body (common with large Library files)"
        symptoms:
          - "JSONDecodeError: Expecting value: line 1 column 1 (char 0)"
          - "Occurs primarily with files >400KB"
        fix: "Check for empty response before JSON parsing, retry with exponential backoff"
        implementation: |
          1. Check response.text is non-empty before calling response.json()
          2. If empty, set last_error and retry (up to max_retries)
          3. Log diagnostic info at debug level for troubleshooting
        constants_added:
          - "LARGE_FILE_THRESHOLD_BYTES = 400_000"
          - "HEALTH_CHECK_TIMEOUT_SECONDS = 5.0"
          - "MIN_CONTENT_TOLERANCE_CHARS = 10"
          - "CONTENT_TOLERANCE_RATIO = 0.001"
        tests_added:
          - "test_process_text_empty_response_body"
          - "test_process_text_invalid_json_response"
          - "test_process_text_empty_response_max_retries_exceeded"
          - "test_process_text_large_file_warning"
          - "test_check_server_health_empty_response"
          - "test_check_server_health_invalid_json"

  example_usage: |
    # Check server health
    pw-ingest sembr --check-only

    # Process full corpus (auto-restarts server, auto-selects GPU)
    pw-ingest sembr -i extracted/ -o sembr/

    # Process on specific GPU (e.g., use secondary GPU)
    pw-ingest sembr -i extracted/ -o sembr/ --gpu 1

    # Verbose logging for troubleshooting
    pw-ingest sembr -i extracted/ -o sembr/ --verbose

    # Disable GPU failover (fail fast on errors)
    pw-ingest sembr -i extracted/ -o sembr/ --no-gpu-failover

    # Frequent health checks (every 30 seconds)
    pw-ingest sembr -i extracted/ -o sembr/ --health-interval 30

    # Test with 10 files (preserves progress on interrupt)
    pw-ingest sembr -i extracted/ -o sembr/ --sample 10

    # Use existing server instance (no restart)
    pw-ingest sembr -i extracted/ -o sembr/ --no-restart

# =============================================================================
# MISE TASKS
# =============================================================================

mise_tasks:
  sembr-server:
    description: "Start sembr server with best multilingual model"
    command: |
      uv run sembr --listen -p 8384 -m admko/sembr2023-distilbert-base-multilingual-cased
    notes: |
      - Run in separate terminal (stays running)
      - Model loads once (~30s), then fast inference
      - Must be running before sembr-process

  sembr-check:
    description: "Check if sembr server is responding"
    command: |
      curl -s http://localhost:8384/check | python -m json.tool
    notes: "Returns JSON with model info if healthy"

  sembr-process:
    description: "Process corpus through sembr (requires sembr-server running)"
    command: |
      uv run pw-ingest sembr -i extracted/ -o sembr/ --progress
    depends: "[sembr-server must be running]"

  sembr-sample:
    description: "Test sembr with 10 sample files"
    command: |
      uv run pw-ingest sembr -i extracted/ -o sembr/ --sample 10 --progress

  sembr-test-text:
    description: "Test sembr with inline text"
    command: |
      TEXT="Stalin implemented the Five-Year Plans. These transformed the USSR."
      echo "$TEXT" | curl -s -X POST http://localhost:8384/rewrap \
        -d "text=$(cat -)" | python -m json.tool

# =============================================================================
# TESTING STRATEGY
# =============================================================================

testing_strategy:
  test_files:
    unit_tests:
      location: tests/unit/sembr/test_linebreaker.py
      markers: [unit]
      tests:
        # Config tests
        - test_sembr_config_defaults
        - test_sembr_config_custom_values
        - test_sembr_config_from_pyproject

        # Result dataclass tests
        - test_sembr_result_creation
        - test_sembr_result_word_count_validation

        # Health check tests (mocked)
        - test_check_server_health_success
        - test_check_server_health_connection_error
        - test_check_server_health_timeout
        - test_check_server_health_bad_response

        # Process text tests (mocked HTTP)
        - test_process_text_basic
        - test_process_text_empty_input
        - test_process_text_whitespace_only
        - test_process_text_unicode_russian
        - test_process_text_unicode_chinese
        - test_process_text_preserves_all_words
        - test_process_text_server_error_retry
        - test_process_text_timeout_retry
        - test_process_text_max_retries_exceeded
        - test_process_text_content_validation_fails

        # File processing tests
        - test_process_file_creates_output_dir
        - test_process_file_preserves_encoding
        - test_process_file_handles_missing_input

    integration_tests:
      location: tests/integration/test_sembr_integration.py
      markers: [integration]
      requires: "sembr server running (mock or real)"
      tests:
        - test_process_batch_small_sample
        - test_progress_callback_invoked
        - test_output_directory_structure_preserved
        - test_json_clean_text_extraction
        - test_concurrent_processing

    slow_tests:
      location: tests/slow/test_sembr_slow.py
      markers: [slow]
      requires: "Real sembr server with model loaded"
      tests:
        - test_real_server_processing
        - test_multilingual_content_russian
        - test_multilingual_content_chinese
        - test_large_library_document
        - test_full_corpus_sample_100

    gpu_integration_tests:
      location: tests/slow/test_gpu_integration.py
      markers: [slow]
      requires: "NVIDIA GPU with nvidia-smi, sembr dependencies"
      tests:
        # GPU detection
        - test_detect_gpus_returns_valid_status
        - test_check_gpu_health_returns_status
        - test_check_gpu_health_invalid_index
        - test_get_available_gpu_returns_valid
        - test_get_available_gpu_excludes_specified
        - test_get_gpu_summary_is_readable
        - test_is_cuda_corrupted_healthy_gpu
        # GPU processes
        - test_get_gpu_processes_returns_list
        # CUDA cache
        - test_reset_cuda_cache_succeeds_or_reports
        # Server lifecycle
        - test_server_start_stop_cycle
        - test_server_restart_same_gpu
        - test_server_restart_different_gpu
        # Signal handling
        - test_setup_signal_handlers_runs
        - test_shutdown_flag_persistence
        # Health monitoring
        - test_health_monitor_detects_server_down

    gpu_manager_unit_tests:
      location: tests/unit/ingest/test_gpu_manager.py
      markers: [unit]
      tests_count: 42
      coverage:
        - GPUStatus dataclass creation and properties
        - GPU detection with mocked nvidia-smi
        - Health checking and available GPU selection
        - CUDA cache reset via subprocess
        - CUDA corruption detection heuristics
        - GPU process listing and killing
        - Environment variable management

    server_manager_unit_tests:
      location: tests/unit/ingest/test_server_manager.py
      markers: [unit]
      tests_count: 47
      coverage:
        - ServerConfig and ServerProcess dataclasses
        - Health checking via HTTP
        - Server startup with GPU selection
        - Graceful vs forceful shutdown
        - Server restart with GPU failover
        - Signal handling (SIGINT/SIGTERM)
        - atexit cleanup registration
        - HealthMonitor periodic checks and recovery

  fixtures:
    input:
      location: tests/fixtures/sembr/input/
      files:
        - simple_english.txt: "Basic English paragraph"
        - russian_text.txt: "Russian content from Lenin"
        - chinese_text.txt: "Chinese content sample"
        - long_paragraph.txt: "500+ word paragraph"
        - headers_preserved.txt: "Text with section headers"
        - empty.txt: "Empty file"
        - whitespace_only.txt: "Only whitespace"

    expected:
      location: tests/fixtures/sembr/expected/
      files:
        - simple_english.txt: "Expected sembr output"
        - russian_text.txt: "Expected Russian output"
        - chinese_text.txt: "Expected Chinese output"
        - long_paragraph.txt: "Expected long output"
        - headers_preserved.txt: "Headers unchanged"

    mock_responses:
      location: tests/fixtures/sembr/mock_responses/
      files:
        - success.json: "Successful /rewrap response"
        - error.json: "Error response"
        - health_check.json: "Successful /check response"

  test_invariants:
    content_preservation: |
      # Critical invariant: no words lost
      input_words = set(input_text.split())
      output_words = set(output_text.split())
      assert input_words == output_words

    word_count_match: |
      # Word count should be identical
      assert len(input_text.split()) == len(output_text.split())

    no_empty_lines_at_boundaries: |
      # Output shouldn't start/end with blank lines
      lines = output.strip().split('\n')
      assert lines[0].strip()  # First line not empty
      assert lines[-1].strip()  # Last line not empty

# =============================================================================
# EDGE CASES
# =============================================================================

edge_cases:
  empty_input:
    input: '""'
    behavior: "Return empty string, don't call server"
    test: test_process_text_empty_input

  whitespace_only:
    input: '"   \n\t\n   "'
    behavior: "Return empty string, don't call server"
    test: test_process_text_whitespace_only

  very_long_document:
    scenario: "Library namespace documents can be 46k+ lines"
    behavior: |
      - sembr handles internally via batch_size
      - May need to increase timeout for very long docs
      - Consider splitting if >100KB
    test: test_large_library_document

  multilingual_content:
    scenario: "Russian, Chinese text in ProleWiki"
    behavior: |
      - Multilingual model handles correctly
      - Unicode preserved in output
      - Line breaks at semantic boundaries in native language
    tests:
      - test_multilingual_content_russian
      - test_multilingual_content_chinese

  section_headers:
    scenario: "Lines that are section headers (from Phase 2 clean_text)"
    behavior: |
      - Headers passed through unchanged
      - sembr doesn't add breaks within header text
      - Headers remain on single line
    test: test_headers_preserved

  server_unavailable:
    scenario: "sembr server not running when process_batch called"
    behavior: |
      - check_server_health() returns False
      - Raise SembrServerError with clear message
      - Suggest: "Start server with: mise run sembr-server"
    test: test_check_server_health_connection_error

  server_timeout:
    scenario: "Server takes too long to respond"
    behavior: |
      - Retry up to max_retries times
      - Exponential backoff between retries
      - Eventually raise SembrTimeoutError
    test: test_process_text_timeout_retry

  content_validation_failure:
    scenario: "Output has different word count than input"
    behavior: |
      - Raise SembrContentError
      - Log both input and output word counts
      - Include sample of missing/extra words
    test: test_process_text_content_validation_fails

# =============================================================================
# IMPLEMENTATION TASKS (TDD)
# =============================================================================

implementation_tasks:
  phase_3_1_test_infrastructure:
    status: COMPLETE
    completed_date: 2025-12-15
    description: "Create test structure (Red Phase)"
    results:
      tests_written: 33
      fixtures_created: 15
      files_created:
        - tests/unit/sembr/__init__.py
        - tests/unit/sembr/test_linebreaker.py (33 tests)
        - tests/slow/__init__.py
        - tests/fixtures/sembr/input/ (7 files)
        - tests/fixtures/sembr/expected/ (5 files)
        - tests/fixtures/sembr/mock_responses/ (3 files)
        - conftest.py (8 new fixtures)

  phase_3_2_core_module:
    status: COMPLETE
    completed_date: 2025-12-15
    description: "Implement linebreaker.py (Green Phase)"
    results:
      tests_passing: 33
      lines_of_code: 487
      file: src/pw_mcp/ingest/linebreaker.py
      components:
        - SembrConfig (frozen dataclass, 7 fields)
        - SembrResult (dataclass, 5 fields)
        - SembrError, SembrServerError, SembrTimeoutError, SembrContentError
        - check_server_health() - sync HTTP GET
        - process_text() - async with retry logic
        - process_file() - async file I/O wrapper
        - process_batch() - async with Semaphore concurrency
        - _extract_clean_text_from_json() helper

  phase_3_3_cli_integration:
    status: COMPLETE
    completed_date: 2025-12-15
    description: "Add sembr CLI subcommand"
    results:
      lines_of_code: 362
      file: src/pw_mcp/ingest/cli.py
      cli_options: 8
      mise_tasks_added:
        - sembr-check
        - sembr-process
        - sembr-sample

  phase_3_4_documentation:
    status: COMPLETE
    completed_date: 2025-12-15
    description: "Update documentation"
    files_updated:
      - ai-docs/project-status.yaml
      - ai-docs/sembr.yaml
      - ai-docs/index.yaml
      - ai-docs/pipeline.yaml
      - ai-docs/testing-plan.yaml

# =============================================================================
# SUCCESS CRITERIA
# =============================================================================

success_criteria:
  functionality:
    - "All unit tests pass with mocked HTTP responses"
    - "All integration tests pass with real sembr server"
    - "CLI subcommand works for full corpus"
    - "Progress bar displays during batch processing"

  performance:
    - "Full corpus (5,222 files) processed in <2 hours"
    - "Single file processing <1 second average"
    - "Server health check <500ms"

  quality:
    - "No content lost: input words == output words for all files"
    - "Unicode preserved correctly (Russian, Chinese)"
    - "Section headers passed through unchanged"
    - "Proper error messages for server unavailable"

  code_quality:
    - "mypy strict mode passes"
    - "ruff lint passes"
    - "Test coverage >80% for linebreaker.py"
    - "All functions have docstrings"

# =============================================================================
# DEPENDENCIES
# =============================================================================

dependencies:
  existing:
    - httpx: "Async HTTP client (already in requirements)"
    - sembr: "Semantic linebreaker CLI/server (already in requirements)"

  new_required: []

  pyproject_config:
    section: "[tool.pw-mcp]"
    keys:
      sembr_model: "admko/sembr2023-distilbert-base-multilingual-cased"
      sembr_server_url: "http://localhost:8384"
      sembr_timeout_seconds: 60.0

# =============================================================================
# SOURCES
# =============================================================================

sources:
  sembr_github: https://github.com/admk/sembr
  sembr_spec: https://sembr.org/
