# Project Status - pw-mcp
# Purpose: Track implementation progress and next steps
# Updated: 2025-12-15

overview:
  project: ProleWiki MCP Server
  goal: Semantic vector search over ProleWiki corpus via MCP
  artifact: ChromaDB database distributable independently of source corpus
  corpus_size: 5,222 files in prolewiki-exports/
  corpus_stats:
    total_templates: 20,069
    unique_template_types: 948
    total_characters: 197.1M

# =============================================================================
# IMPLEMENTATION PHASES
# =============================================================================

phases:
  phase_1_foundation:
    status: COMPLETE
    description: Test infrastructure and TDD Red Phase
    completed:
      - pyproject.toml updated (hypothesis, new markers)
      - .mise.toml updated (test-integration, test-property, test-benchmark, test-retrieval)
      - Test directory structure created
      - tests/conftest.py with shared fixtures
      - Fixture files extracted from prolewiki-exports/

  phase_2_parsers:
    status: COMPLETE
    description: TDD Green Phase - implement parsers to pass tests
    completed_date: 2025-12-14
    test_counts:
      link_parser: 28
      citation_parser: 48
      infobox_parser: 25
      library_work_parser: 25
      quote_parser: 12
      extraction_pipeline: 22
      total: 160

    tasks:
      - id: 2.1
        name: Create parser module structure
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          src/pw_mcp/ingest/parsers/
          ├── __init__.py        # Public API exports
          ├── types.py           # Shared dataclasses
          ├── link.py            # parse_links()
          ├── citation.py        # parse_citations()
          ├── infobox.py         # parse_infobox()
          ├── library_work.py    # parse_library_work()
          └── quote.py           # parse_quotes()

      - id: 2.2
        name: Implement link parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 28 passing
        details: |
          Regex-based parser for MediaWiki links:
          - [[Target]] simple internal links
          - [[Target|Display]] piped links
          - [[Category:Name]] category links
          - [https://url text] external links
          - [[Article#Section]] section anchors
        implementation: src/pw_mcp/ingest/parsers/link.py

      - id: 2.3
        name: Implement citation parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 48 passing
        details: |
          mwparserfromhell-based parser for 7 citation types:
          - {{Citation|...}} book citations
          - {{Web citation|...}} web articles
          - {{News citation|...}} news articles
          - {{YouTube citation|...}} video citations
          - {{Video citation|...}} video citations (YouTube alternative)
          - {{Library citation|...}} internal Library links
          - {{Textcite|...}} book/web citations with translation fields
          Handle <ref name="..."> named refs and ref reuse/deduplication.
        implementation: src/pw_mcp/ingest/parsers/citation.py
        corpus_coverage:
          web_citation: 5,499
          citation: 4,227
          news_citation: 1,574
          library_citation: 253
          youtube_citation: 85
          video_citation: 83
          textcite: 30

      - id: 2.4
        name: Implement infobox parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          mwparserfromhell-based parser for 19 infobox types:
          - politician, country, political_party, person, revolutionary
          - essay, philosopher, company, settlement, military_person
          - organization, guerilla_organization, youtuber, military_conflict
          - book, religion, website, transcript, flag
          Features: nested templates, <br> multi-values, [[link]] extraction
        implementation: src/pw_mcp/ingest/parsers/infobox.py
        corpus_coverage:
          infobox_politician: 644
          infobox_country: 443
          infobox_political_party: 419
          infobox_person: 249
          infobox_revolutionary: 114
          # ... 14 more types

      - id: 2.5
        name: Implement extraction pipeline
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 22 passing
        details: |
          Orchestrates all parsers into full extraction:
          - extract_article(text, source_path) -> ArticleData
          - Combines infobox, citations, links, quotes, categories
          - Generates clean_text with ALL templates removed
          - Detects namespace from source_path
          - Extracts section headers
          - Counts references, detects stubs, blockquotes
        implementation: src/pw_mcp/ingest/extraction.py

      - id: 2.6
        name: Implement library work parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          Parser for {{Library work}} template in Library namespace.
          Extracts bibliographic metadata: title, author, year, type,
          publisher, source URL, translator, audiobook URL, etc.
          Handles 70 work types found in corpus.
        implementation: src/pw_mcp/ingest/parsers/library_work.py
        corpus_coverage: 1,027 instances

      - id: 2.7
        name: Implement quote parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 12 passing
        details: |
          Parser for {{Quote|text|attribution}} template.
          Extracts block quotes with optional attribution.
        implementation: src/pw_mcp/ingest/parsers/quote.py
        corpus_coverage: 223 instances

      - id: 2.8
        name: Generic template removal
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          Added _strip_all_templates() using mwparserfromhell.
          Removes ALL 948 template types from clean_text.
          100% removal rate: 20,069 templates across 5,222 files.
          Essential for clean text generation before semantic linebreaking.
        implementation: src/pw_mcp/ingest/extraction.py

  phase_3_sembr:
    status: COMPLETE
    completed_date: 2025-12-15
    description: Semantic linebreaking integration
    reference_doc: ai-docs/sembr.yaml
    test_counts:
      linebreaker_unit: 33
      total_with_sembr: 171

    tasks:
      - id: 3.1
        name: Test infrastructure (Red Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Created test infrastructure:
          - tests/unit/sembr/__init__.py
          - tests/unit/sembr/test_linebreaker.py (33 tests)
          - tests/slow/__init__.py
          - tests/fixtures/sembr/input/ (7 files)
          - tests/fixtures/sembr/expected/ (5 files)
          - tests/fixtures/sembr/mock_responses/ (3 files)
          - conftest.py updated with 8 sembr fixtures

      - id: 3.2
        name: Core module implementation (Green Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 33 passing
        details: |
          Created src/pw_mcp/ingest/linebreaker.py (487 lines):
          - SembrConfig (frozen dataclass, 7 config fields)
          - SembrResult (dataclass, 5 result fields)
          - SembrError, SembrServerError, SembrTimeoutError, SembrContentError
          - check_server_health() - sync HTTP GET
          - process_text() - async with retry logic
          - process_file() - async file I/O wrapper
          - process_batch() - async with Semaphore concurrency
          - _extract_clean_text_from_json() helper
          All 33 unit tests pass. Pre-commit hooks pass.

      - id: 3.3
        name: CLI integration
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Updated src/pw_mcp/ingest/cli.py (362 lines):
          - Added "sembr" subcommand with 8 options
          - --input/-i, --output/-o, --server/-s
          - --check-only, --sample, --max-concurrent
          - --no-progress, --timeout
          - Progress display with file counts
          - Proper error handling and exit codes

          Added 3 mise tasks in .mise.toml:
          - sembr-check: Check server health
          - sembr-process: Process full corpus
          - sembr-sample: Test with sample files

      - id: 3.4
        name: Documentation
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Updated documentation files:
          - ai-docs/project-status.yaml (this file)
          - ai-docs/sembr.yaml (implementation tasks)
          - ai-docs/index.yaml (status update)
          - ai-docs/pipeline.yaml (Stage 4 status)
          - ai-docs/testing-plan.yaml (test counts)

    server_protocol:
      endpoints:
        health: "GET /check"
        process: "POST /rewrap (JSON: {text, predict_func})"
      response_format: JSON with status, text fields

    model:
      name: admko/sembr2023-distilbert-base-multilingual-cased
      params: 135M
      languages: [English, Russian, Chinese, Spanish, German, French]

    implementation_notes: |
      Phase 3 COMPLETE - sembr integration fully implemented:
      - linebreaker.py handles HTTP client, retries, content validation
      - CLI provides full batch processing with progress
      - All 33 tests pass, pre-commit hooks pass
      - Ready for Phase 4 (Chunking)

  phase_4_chunking:
    status: future
    description: Text chunking for embeddings
    tasks:
      - Implement chunker.py
      - Respect section boundaries
      - Target 200-500 tokens per chunk
      - Store line offsets for citations
      - Add chunking tests

  phase_5_embedding:
    status: future
    description: Vector embedding generation
    tasks:
      - Integrate Ollama embeddinggemma
      - Batch embedding with progress
      - Add embedding quality tests
      - Investigate parallel embeddinggemmas by directory (3 instances if GPU permits)

  phase_6_loading:
    status: future
    description: ChromaDB ingestion
    tasks:
      - Implement chroma.py loader
      - Batch upsert with metadata
      - Add loading tests

  phase_7_mcp:
    status: future
    description: MCP server implementation
    tasks:
      - Implement search tool
      - Implement get_article tool
      - Implement list_categories tool
      - Add MCP protocol tests

  phase_8_finetune:
    status: PLANNED
    description: Marxist-Leninist LLM fine-tuning on ProleWiki corpus
    reference_doc: ai-docs/finetune.yaml
    dependencies: Phases 1-7 (RAG pipeline provides Q&A source data)

    overview:
      goal: Fine-tune DeepSeek 7B to produce Marxist-Leninist domain expert
      method: QLoRA via Unsloth framework
      dataset: 3,000-5,000 Q&A pairs from ProleWiki
      output: GGUF for Ollama deployment
      hardware: 16-24GB VRAM (RTX 3090/4090/A4000)

    tasks:
      - id: 8.1
        name: Data Generation Pipeline
        status: PLANNED
        details: |
          Q&A generation from ProleWiki corpus:
          - infobox_to_qa() - factual pairs from infobox fields
          - section_to_qa() - conceptual pairs from article sections
          - library_to_qa() - quotes from primary sources
          - crossref_to_qa() - relationship pairs from internal links
          Target: 3,000-5,000 Q&A pairs

      - id: 8.2
        name: Dataset Preparation
        status: PLANNED
        details: |
          - Deduplication (exact + semantic)
          - Length filtering (50-500 tokens)
          - Train/validation/test splits (80/10/10)
          - Format: Alpaca JSONL

      - id: 8.3
        name: Training Infrastructure
        status: PLANNED
        details: |
          - Install unsloth framework
          - Create training script
          - Configure QLoRA: r=16, lora_alpha=16
          - Set up checkpoint saving and monitoring

      - id: 8.4
        name: Fine-tuning Execution
        status: PLANNED
        details: |
          - Train for 3-5 epochs
          - Monitor loss curves
          - Evaluate on validation set
          - Select best checkpoint

      - id: 8.5
        name: Evaluation and Export
        status: PLANNED
        details: |
          - Perplexity measurement on test set
          - Manual evaluation (50 hand-picked questions)
          - Export to GGUF format
          - Create Ollama Modelfile
          - Integration test with Ollama

    model_info:
      base_model: DeepSeek-R1-7B
      huggingface: unsloth/DeepSeek-R1-7B
      params: 7B
      context_length: 4096

# =============================================================================
# CURRENT FOCUS
# =============================================================================

current_focus:
  phase: 4 (Chunking)
  completed: [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 3.1, 3.2, 3.3, 3.4]
  next_task: 4.1 Implement chunker.py
  reference: ai-docs/pipeline.yaml

  next_action: |
    Phase 3 COMPLETE - sembr integration fully implemented.
    171 tests passing across 7 test files.

    Phase 4 NEXT - Text chunking for embeddings:
    1. Implement chunker.py with section-aware splitting
    2. Target 200-500 tokens per chunk
    3. Store line offsets for citations
    4. Add chunking unit tests

    BEFORE processing corpus:
    - Start sembr server: mise run sembr-server
    - Process corpus: mise run sembr-process

  test_status:
    link_parser: 28 passing
    citation_parser: 48 passing
    infobox_parser: 25 passing
    library_work_parser: 25 passing
    quote_parser: 12 passing
    extraction_pipeline: 22 passing
    linebreaker: 33 passing
    total: 171 passing

# =============================================================================
# KEY DECISIONS
# =============================================================================

decisions:
  tdd_approach:
    decision: Use pytest.skip() for Red Phase tests
    rationale: Tests pass as "skipped" rather than failing with ImportError
    date: 2024-12-14

  fixture_source:
    decision: Use prolewiki-exports/ for fixture extraction
    rationale: Real corpus data ensures realistic test coverage
    date: 2024-12-14

  parser_order:
    decision: Implement link -> citation -> infobox -> library_work -> quote
    rationale: Increasing complexity, foundational parsers first
    date: 2024-12-14

  type_stubs:
    decision: Create proper stub files for mcp library instead of type-ignore comments
    rationale: Proper stubs provide real type safety without suppressing errors
    date: 2025-12-14
    files:
      - stubs/mcp/__init__.pyi
      - stubs/mcp/server/__init__.pyi
      - stubs/mcp/server/fastmcp.pyi

  mwparserfromhell:
    decision: Use mwparserfromhell for template parsing, not regex
    rationale: |
      Handles nested templates, multiline, edge cases that regex cannot.
      Already a project dependency.
    date: 2025-12-14

  generic_template_removal:
    decision: Use mwparserfromhell to remove ALL templates from clean_text
    rationale: |
      Corpus has 948 unique template types (20,069 instances).
      Specific parsers extract metadata, then all templates removed.
      Ensures clean text for sembr and embeddings.
    date: 2025-12-14

  lock_file_policy:
    decision: Exclude uv.lock from pre-commit large file check
    rationale: Lock files are auto-generated, necessary for reproducibility
    date: 2025-12-14

# =============================================================================
# FILE LOCATIONS
# =============================================================================

key_files:
  source_code:
    parsers:
      - src/pw_mcp/ingest/parsers/__init__.py   # Public API exports
      - src/pw_mcp/ingest/parsers/types.py      # Link, Citation, InfoboxData, LibraryWorkData, QuoteData, ArticleData
      - src/pw_mcp/ingest/parsers/link.py       # parse_links() - 28 tests
      - src/pw_mcp/ingest/parsers/citation.py   # parse_citations() - 48 tests
      - src/pw_mcp/ingest/parsers/infobox.py    # parse_infobox() - 25 tests
      - src/pw_mcp/ingest/parsers/library_work.py  # parse_library_work() - 25 tests
      - src/pw_mcp/ingest/parsers/quote.py      # parse_quotes() - 12 tests
    extraction:
      - src/pw_mcp/ingest/extraction.py         # extract_article() - 22 tests
    sembr:
      - src/pw_mcp/ingest/linebreaker.py        # sembr HTTP client - 33 tests
    server:
      - src/pw_mcp/server.py
      - src/pw_mcp/config.py
      - src/pw_mcp/ingest/cli.py                # pw-ingest CLI with sembr subcommand

  type_stubs:
    - stubs/mcp/__init__.pyi
    - stubs/mcp/server/__init__.pyi
    - stubs/mcp/server/fastmcp.pyi

  tests:
    unit:
      - tests/unit/extraction/test_link_parser.py          # 28 tests
      - tests/unit/extraction/test_citation_parser.py      # 48 tests
      - tests/unit/extraction/test_infobox_parser.py       # 25 tests
      - tests/unit/extraction/test_library_work_parser.py  # 25 tests
      - tests/unit/extraction/test_quote_parser.py         # 12 tests
      - tests/unit/sembr/test_linebreaker.py               # 33 tests
    integration:
      - tests/integration/test_extraction_pipeline.py      # 22 tests
    config:
      - tests/conftest.py

  fixtures:
    - tests/fixtures/mediawiki/infoboxes/     # 7 infobox type samples
    - tests/fixtures/mediawiki/citations/     # 7 citation type samples
    - tests/fixtures/mediawiki/links/         # 3 link type samples
    - tests/fixtures/mediawiki/library_work/  # 7 library work samples
    - tests/fixtures/mediawiki/quotes/        # 3 quote samples
    - tests/fixtures/mediawiki/edge_cases/    # unicode, nested, multiline
    - tests/fixtures/sembr/input/             # 7 sembr input samples
    - tests/fixtures/sembr/expected/          # 5 sembr expected outputs
    - tests/fixtures/sembr/mock_responses/    # 3 HTTP mock responses

  corpus:
    - prolewiki-exports/Main/       # ~4000+ encyclopedia articles
    - prolewiki-exports/Library/    # Full books (Marx, Lenin, etc.)
    - prolewiki-exports/Essays/     # User-contributed essays
    - prolewiki-exports/ProleWiki/  # Meta/admin pages

  documentation:
    - ai-docs/index.yaml
    - ai-docs/chromadb.yaml
    - ai-docs/chromadb-schema.yaml
    - ai-docs/pipeline.yaml
    - ai-docs/testing-plan.yaml
    - ai-docs/project-status.yaml  # This file

# =============================================================================
# DATACLASS REFERENCE
# =============================================================================

dataclasses:
  Link:
    fields: [target, display, link_type, section, start, end]
    link_types: [internal, category, external, interwiki]

  Citation:
    fields: |
      type, title, author, year, url, lg_url, pdf_url, publisher,
      newspaper, chapter, page, isbn, date, retrieved, channel,
      quote, ref_name, link, published_location, translation_title,
      translation_language
    types: [book, web, news, youtube, video, library]

  InfoboxData:
    fields: [type, fields, remaining_text, extracted_links]
    types: |
      politician, country, political_party, person, revolutionary,
      essay, philosopher, company, settlement, military_person,
      organization, guerilla_organization, youtuber, military_conflict,
      book, religion, website, transcript, flag

  LibraryWorkData:
    fields: |
      title, author, authors, year, work_type, source_url,
      published_location, translator, original_language,
      publisher, audiobook_url, remaining_text

  QuoteData:
    fields: [text, attribution]

  ArticleData:
    fields: |
      source_path, namespace, title, clean_text, sections,
      categories, links, citations, infobox, library_work,
      quotes, is_stub, citation_needed_count, has_blockquote,
      reference_count

  SembrConfig:
    fields: |
      server_url, model_name, timeout_seconds, max_retries,
      retry_delay_seconds, batch_size, predict_func
    frozen: true

  SembrResult:
    fields: |
      text, line_count, processing_time_ms,
      input_word_count, output_word_count

# =============================================================================
# COMMANDS
# =============================================================================

useful_commands:
  testing:
    all_tests: mise run test
    unit_only: mise run test-fast
    integration: mise run test-integration
    with_coverage: mise run test-cov
    specific_file: uv run pytest tests/unit/extraction/test_citation_parser.py -v

  code_quality:
    lint: uv run ruff check src/ tests/
    format: uv run ruff format src/ tests/
    typecheck: uv run mypy src/pw_mcp/
    all_checks: mise run check
    pre_commit: mise run pre-commit

  development:
    install: mise run install
    sembr_server: mise run sembr-server
    sembr_check: mise run sembr-check
    sembr_process: mise run sembr-process
    sembr_sample: mise run sembr-sample
