# Project Status - pw-mcp
# Purpose: Track implementation progress and next steps
# Updated: 2025-12-14

overview:
  project: ProleWiki MCP Server
  goal: Semantic vector search over ProleWiki corpus via MCP
  artifact: ChromaDB database distributable independently of source corpus
  corpus_size: 5,222 files in prolewiki-exports/
  corpus_stats:
    total_templates: 20,069
    unique_template_types: 948
    total_characters: 197.1M

# =============================================================================
# IMPLEMENTATION PHASES
# =============================================================================

phases:
  phase_1_foundation:
    status: COMPLETE
    description: Test infrastructure and TDD Red Phase
    completed:
      - pyproject.toml updated (hypothesis, new markers)
      - .mise.toml updated (test-integration, test-property, test-benchmark, test-retrieval)
      - Test directory structure created
      - tests/conftest.py with shared fixtures
      - Fixture files extracted from prolewiki-exports/

  phase_2_parsers:
    status: COMPLETE
    description: TDD Green Phase - implement parsers to pass tests
    completed_date: 2025-12-14
    test_counts:
      link_parser: 28
      citation_parser: 48
      infobox_parser: 25
      library_work_parser: 25
      quote_parser: 12
      extraction_pipeline: 22
      total: 160

    tasks:
      - id: 2.1
        name: Create parser module structure
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          src/pw_mcp/ingest/parsers/
          ├── __init__.py        # Public API exports
          ├── types.py           # Shared dataclasses
          ├── link.py            # parse_links()
          ├── citation.py        # parse_citations()
          ├── infobox.py         # parse_infobox()
          ├── library_work.py    # parse_library_work()
          └── quote.py           # parse_quotes()

      - id: 2.2
        name: Implement link parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 28 passing
        details: |
          Regex-based parser for MediaWiki links:
          - [[Target]] simple internal links
          - [[Target|Display]] piped links
          - [[Category:Name]] category links
          - [https://url text] external links
          - [[Article#Section]] section anchors
        implementation: src/pw_mcp/ingest/parsers/link.py

      - id: 2.3
        name: Implement citation parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 48 passing
        details: |
          mwparserfromhell-based parser for 7 citation types:
          - {{Citation|...}} book citations
          - {{Web citation|...}} web articles
          - {{News citation|...}} news articles
          - {{YouTube citation|...}} video citations
          - {{Video citation|...}} video citations (YouTube alternative)
          - {{Library citation|...}} internal Library links
          - {{Textcite|...}} book/web citations with translation fields
          Handle <ref name="..."> named refs and ref reuse/deduplication.
        implementation: src/pw_mcp/ingest/parsers/citation.py
        corpus_coverage:
          web_citation: 5,499
          citation: 4,227
          news_citation: 1,574
          library_citation: 253
          youtube_citation: 85
          video_citation: 83
          textcite: 30

      - id: 2.4
        name: Implement infobox parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          mwparserfromhell-based parser for 19 infobox types:
          - politician, country, political_party, person, revolutionary
          - essay, philosopher, company, settlement, military_person
          - organization, guerilla_organization, youtuber, military_conflict
          - book, religion, website, transcript, flag
          Features: nested templates, <br> multi-values, [[link]] extraction
        implementation: src/pw_mcp/ingest/parsers/infobox.py
        corpus_coverage:
          infobox_politician: 644
          infobox_country: 443
          infobox_political_party: 419
          infobox_person: 249
          infobox_revolutionary: 114
          # ... 14 more types

      - id: 2.5
        name: Implement extraction pipeline
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 22 passing
        details: |
          Orchestrates all parsers into full extraction:
          - extract_article(text, source_path) -> ArticleData
          - Combines infobox, citations, links, quotes, categories
          - Generates clean_text with ALL templates removed
          - Detects namespace from source_path
          - Extracts section headers
          - Counts references, detects stubs, blockquotes
        implementation: src/pw_mcp/ingest/extraction.py

      - id: 2.6
        name: Implement library work parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          Parser for {{Library work}} template in Library namespace.
          Extracts bibliographic metadata: title, author, year, type,
          publisher, source URL, translator, audiobook URL, etc.
          Handles 70 work types found in corpus.
        implementation: src/pw_mcp/ingest/parsers/library_work.py
        corpus_coverage: 1,027 instances

      - id: 2.7
        name: Implement quote parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 12 passing
        details: |
          Parser for {{Quote|text|attribution}} template.
          Extracts block quotes with optional attribution.
        implementation: src/pw_mcp/ingest/parsers/quote.py
        corpus_coverage: 223 instances

      - id: 2.8
        name: Generic template removal
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          Added _strip_all_templates() using mwparserfromhell.
          Removes ALL 948 template types from clean_text.
          100% removal rate: 20,069 templates across 5,222 files.
          Essential for clean text generation before semantic linebreaking.
        implementation: src/pw_mcp/ingest/extraction.py

  phase_3_sembr:
    status: NEXT
    description: Semantic linebreaking integration
    tasks:
      - Integrate sembr server mode for batch processing
      - Create linebreaker.py wrapper
      - Add sembr tests
    notes: |
      clean_text is now ready - all 948 template types removed.
      sembr can process without MediaWiki markup interference.

  phase_4_chunking:
    status: future
    description: Text chunking for embeddings
    tasks:
      - Implement chunker.py
      - Respect section boundaries
      - Target 200-500 tokens per chunk
      - Store line offsets for citations
      - Add chunking tests

  phase_5_embedding:
    status: future
    description: Vector embedding generation
    tasks:
      - Integrate Ollama embeddinggemma
      - Batch embedding with progress
      - Add embedding quality tests

  phase_6_loading:
    status: future
    description: ChromaDB ingestion
    tasks:
      - Implement chroma.py loader
      - Batch upsert with metadata
      - Add loading tests

  phase_7_mcp:
    status: future
    description: MCP server implementation
    tasks:
      - Implement search tool
      - Implement get_article tool
      - Implement list_categories tool
      - Add MCP protocol tests

# =============================================================================
# CURRENT FOCUS
# =============================================================================

current_focus:
  phase: 3 (Sembr Integration)
  completed: [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8]
  next_task: sembr server integration
  next_action: |
    Phase 2 COMPLETE - all parsers implemented and tested.
    160 tests passing across 6 test files.

    Ready for Phase 3:
    1. Set up sembr server mode for batch processing
    2. Create linebreaker.py wrapper module
    3. Test on sample articles
    4. Process full corpus

  test_status:
    link_parser: 28 passing
    citation_parser: 48 passing
    infobox_parser: 25 passing
    library_work_parser: 25 passing
    quote_parser: 12 passing
    extraction_pipeline: 22 passing
    total: 160 passing

# =============================================================================
# KEY DECISIONS
# =============================================================================

decisions:
  tdd_approach:
    decision: Use pytest.skip() for Red Phase tests
    rationale: Tests pass as "skipped" rather than failing with ImportError
    date: 2024-12-14

  fixture_source:
    decision: Use prolewiki-exports/ for fixture extraction
    rationale: Real corpus data ensures realistic test coverage
    date: 2024-12-14

  parser_order:
    decision: Implement link -> citation -> infobox -> library_work -> quote
    rationale: Increasing complexity, foundational parsers first
    date: 2024-12-14

  type_stubs:
    decision: Create proper stub files for mcp library instead of type-ignore comments
    rationale: Proper stubs provide real type safety without suppressing errors
    date: 2025-12-14
    files:
      - stubs/mcp/__init__.pyi
      - stubs/mcp/server/__init__.pyi
      - stubs/mcp/server/fastmcp.pyi

  mwparserfromhell:
    decision: Use mwparserfromhell for template parsing, not regex
    rationale: |
      Handles nested templates, multiline, edge cases that regex cannot.
      Already a project dependency.
    date: 2025-12-14

  generic_template_removal:
    decision: Use mwparserfromhell to remove ALL templates from clean_text
    rationale: |
      Corpus has 948 unique template types (20,069 instances).
      Specific parsers extract metadata, then all templates removed.
      Ensures clean text for sembr and embeddings.
    date: 2025-12-14

  lock_file_policy:
    decision: Exclude uv.lock from pre-commit large file check
    rationale: Lock files are auto-generated, necessary for reproducibility
    date: 2025-12-14

# =============================================================================
# FILE LOCATIONS
# =============================================================================

key_files:
  source_code:
    parsers:
      - src/pw_mcp/ingest/parsers/__init__.py   # Public API exports
      - src/pw_mcp/ingest/parsers/types.py      # Link, Citation, InfoboxData, LibraryWorkData, QuoteData, ArticleData
      - src/pw_mcp/ingest/parsers/link.py       # parse_links() - 28 tests
      - src/pw_mcp/ingest/parsers/citation.py   # parse_citations() - 48 tests
      - src/pw_mcp/ingest/parsers/infobox.py    # parse_infobox() - 25 tests
      - src/pw_mcp/ingest/parsers/library_work.py  # parse_library_work() - 25 tests
      - src/pw_mcp/ingest/parsers/quote.py      # parse_quotes() - 12 tests
    extraction:
      - src/pw_mcp/ingest/extraction.py         # extract_article() - 22 tests
    server:
      - src/pw_mcp/server.py
      - src/pw_mcp/config.py
      - src/pw_mcp/ingest/cli.py

  type_stubs:
    - stubs/mcp/__init__.pyi
    - stubs/mcp/server/__init__.pyi
    - stubs/mcp/server/fastmcp.pyi

  tests:
    unit:
      - tests/unit/extraction/test_link_parser.py          # 28 tests
      - tests/unit/extraction/test_citation_parser.py      # 48 tests
      - tests/unit/extraction/test_infobox_parser.py       # 25 tests
      - tests/unit/extraction/test_library_work_parser.py  # 25 tests
      - tests/unit/extraction/test_quote_parser.py         # 12 tests
    integration:
      - tests/integration/test_extraction_pipeline.py      # 22 tests
    config:
      - tests/conftest.py

  fixtures:
    - tests/fixtures/mediawiki/infoboxes/     # 7 infobox type samples
    - tests/fixtures/mediawiki/citations/     # 7 citation type samples
    - tests/fixtures/mediawiki/links/         # 3 link type samples
    - tests/fixtures/mediawiki/library_work/  # 7 library work samples
    - tests/fixtures/mediawiki/quotes/        # 3 quote samples
    - tests/fixtures/mediawiki/edge_cases/    # unicode, nested, multiline

  corpus:
    - prolewiki-exports/Main/       # ~4000+ encyclopedia articles
    - prolewiki-exports/Library/    # Full books (Marx, Lenin, etc.)
    - prolewiki-exports/Essays/     # User-contributed essays
    - prolewiki-exports/ProleWiki/  # Meta/admin pages

  documentation:
    - ai-docs/index.yaml
    - ai-docs/chromadb.yaml
    - ai-docs/chromadb-schema.yaml
    - ai-docs/pipeline.yaml
    - ai-docs/testing-plan.yaml
    - ai-docs/project-status.yaml  # This file

# =============================================================================
# DATACLASS REFERENCE
# =============================================================================

dataclasses:
  Link:
    fields: [target, display, link_type, section, start, end]
    link_types: [internal, category, external, interwiki]

  Citation:
    fields: |
      type, title, author, year, url, lg_url, pdf_url, publisher,
      newspaper, chapter, page, isbn, date, retrieved, channel,
      quote, ref_name, link, published_location, translation_title,
      translation_language
    types: [book, web, news, youtube, video, library]

  InfoboxData:
    fields: [type, fields, remaining_text, extracted_links]
    types: |
      politician, country, political_party, person, revolutionary,
      essay, philosopher, company, settlement, military_person,
      organization, guerilla_organization, youtuber, military_conflict,
      book, religion, website, transcript, flag

  LibraryWorkData:
    fields: |
      title, author, authors, year, work_type, source_url,
      published_location, translator, original_language,
      publisher, audiobook_url, remaining_text

  QuoteData:
    fields: [text, attribution]

  ArticleData:
    fields: |
      source_path, namespace, title, clean_text, sections,
      categories, links, citations, infobox, library_work,
      quotes, is_stub, citation_needed_count, has_blockquote,
      reference_count

# =============================================================================
# COMMANDS
# =============================================================================

useful_commands:
  testing:
    all_tests: mise run test
    unit_only: mise run test-fast
    integration: mise run test-integration
    with_coverage: mise run test-cov
    specific_file: uv run pytest tests/unit/extraction/test_citation_parser.py -v

  code_quality:
    lint: uv run ruff check src/ tests/
    format: uv run ruff format src/ tests/
    typecheck: uv run mypy src/pw_mcp/
    all_checks: mise run check
    pre_commit: mise run pre-commit

  development:
    install: mise run install
    sembr_server: mise run sembr-server
