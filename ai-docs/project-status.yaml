# Project Status - pw-mcp
# Purpose: Track implementation progress and next steps
# Updated: 2025-12-15 (Phase 5.5 - Test fixes + CLI server management)

overview:
  project: ProleWiki MCP Server
  goal: Semantic vector search over ProleWiki corpus via MCP
  artifact: ChromaDB database distributable independently of source corpus
  corpus_size: 5,222 files in prolewiki-exports/
  corpus_stats:
    total_templates: 20,069
    unique_template_types: 948
    total_characters: 197.1M

# =============================================================================
# IMPLEMENTATION PHASES
# =============================================================================

phases:
  phase_1_foundation:
    status: COMPLETE
    description: Test infrastructure and TDD Red Phase
    completed:
      - pyproject.toml updated (hypothesis, new markers)
      - .mise.toml updated (test-integration, test-property, test-benchmark, test-retrieval)
      - Test directory structure created
      - tests/conftest.py with shared fixtures
      - Fixture files extracted from prolewiki-exports/

  phase_2_parsers:
    status: COMPLETE
    description: TDD Green Phase - implement parsers to pass tests
    completed_date: 2025-12-14
    test_counts:
      link_parser: 28
      citation_parser: 48
      infobox_parser: 25
      library_work_parser: 25
      quote_parser: 12
      extraction_pipeline: 22
      total: 160

    tasks:
      - id: 2.1
        name: Create parser module structure
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          src/pw_mcp/ingest/parsers/
          ├── __init__.py        # Public API exports
          ├── types.py           # Shared dataclasses
          ├── link.py            # parse_links()
          ├── citation.py        # parse_citations()
          ├── infobox.py         # parse_infobox()
          ├── library_work.py    # parse_library_work()
          └── quote.py           # parse_quotes()

      - id: 2.2
        name: Implement link parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 28 passing
        details: |
          Regex-based parser for MediaWiki links:
          - [[Target]] simple internal links
          - [[Target|Display]] piped links
          - [[Category:Name]] category links
          - [https://url text] external links
          - [[Article#Section]] section anchors
        implementation: src/pw_mcp/ingest/parsers/link.py

      - id: 2.3
        name: Implement citation parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 48 passing
        details: |
          mwparserfromhell-based parser for 7 citation types:
          - {{Citation|...}} book citations
          - {{Web citation|...}} web articles
          - {{News citation|...}} news articles
          - {{YouTube citation|...}} video citations
          - {{Video citation|...}} video citations (YouTube alternative)
          - {{Library citation|...}} internal Library links
          - {{Textcite|...}} book/web citations with translation fields
          Handle <ref name="..."> named refs and ref reuse/deduplication.
        implementation: src/pw_mcp/ingest/parsers/citation.py
        corpus_coverage:
          web_citation: 5,499
          citation: 4,227
          news_citation: 1,574
          library_citation: 253
          youtube_citation: 85
          video_citation: 83
          textcite: 30

      - id: 2.4
        name: Implement infobox parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          mwparserfromhell-based parser for 19 infobox types:
          - politician, country, political_party, person, revolutionary
          - essay, philosopher, company, settlement, military_person
          - organization, guerilla_organization, youtuber, military_conflict
          - book, religion, website, transcript, flag
          Features: nested templates, <br> multi-values, [[link]] extraction
        implementation: src/pw_mcp/ingest/parsers/infobox.py
        corpus_coverage:
          infobox_politician: 644
          infobox_country: 443
          infobox_political_party: 419
          infobox_person: 249
          infobox_revolutionary: 114
          # ... 14 more types

      - id: 2.5
        name: Implement extraction pipeline
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 22 passing
        details: |
          Orchestrates all parsers into full extraction:
          - extract_article(text, source_path) -> ArticleData
          - Combines infobox, citations, links, quotes, categories
          - Generates clean_text with ALL templates removed
          - Detects namespace from source_path
          - Extracts section headers
          - Counts references, detects stubs, blockquotes
        implementation: src/pw_mcp/ingest/extraction.py

      - id: 2.6
        name: Implement library work parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          Parser for {{Library work}} template in Library namespace.
          Extracts bibliographic metadata: title, author, year, type,
          publisher, source URL, translator, audiobook URL, etc.
          Handles 70 work types found in corpus.
        implementation: src/pw_mcp/ingest/parsers/library_work.py
        corpus_coverage: 1,027 instances

      - id: 2.7
        name: Implement quote parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 12 passing
        details: |
          Parser for {{Quote|text|attribution}} template.
          Extracts block quotes with optional attribution.
        implementation: src/pw_mcp/ingest/parsers/quote.py
        corpus_coverage: 223 instances

      - id: 2.8
        name: Generic template removal
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          Added _strip_all_templates() using mwparserfromhell.
          Removes ALL 948 template types from clean_text.
          100% removal rate: 20,069 templates across 5,222 files.
          Essential for clean text generation before semantic linebreaking.
        implementation: src/pw_mcp/ingest/extraction.py

  phase_3_sembr:
    status: COMPLETE
    completed_date: 2025-12-15
    description: Semantic linebreaking integration
    reference_doc: ai-docs/sembr.yaml
    test_counts:
      linebreaker_unit: 33
      sembr_integration: 8
      sembr_slow: 9
      total_with_sembr: 171
    enhancements_added:
      date: 2025-12-15
      items:
        - "CLI server management (--restart-server, --no-restart)"
        - "CUDA cache reset on server restart"
        - "JSON file support in process_batch (both .txt and .json)"
        - "Graceful error handling (failed files return None)"
        - "Retry jitter for 'Already borrowed' errors"
        - "_extract_clean_text_from_json() supports both clean_text and text fields"

    tasks:
      - id: 3.1
        name: Test infrastructure (Red Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Created test infrastructure:
          - tests/unit/sembr/__init__.py
          - tests/unit/sembr/test_linebreaker.py (33 tests)
          - tests/slow/__init__.py
          - tests/fixtures/sembr/input/ (7 files)
          - tests/fixtures/sembr/expected/ (5 files)
          - tests/fixtures/sembr/mock_responses/ (3 files)
          - conftest.py updated with 8 sembr fixtures

      - id: 3.2
        name: Core module implementation (Green Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 33 passing
        details: |
          Created src/pw_mcp/ingest/linebreaker.py (487 lines):
          - SembrConfig (frozen dataclass, 7 config fields)
          - SembrResult (dataclass, 5 result fields)
          - SembrError, SembrServerError, SembrTimeoutError, SembrContentError
          - check_server_health() - sync HTTP GET
          - process_text() - async with retry logic
          - process_file() - async file I/O wrapper
          - process_batch() - async with Semaphore concurrency
          - _extract_clean_text_from_json() helper
          All 33 unit tests pass. Pre-commit hooks pass.

      - id: 3.3
        name: CLI integration
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Updated src/pw_mcp/ingest/cli.py (362 lines):
          - Added "sembr" subcommand with 8 options
          - --input/-i, --output/-o, --server/-s
          - --check-only, --sample, --max-concurrent
          - --no-progress, --timeout
          - Progress display with file counts
          - Proper error handling and exit codes

          Added 3 mise tasks in .mise.toml:
          - sembr-check: Check server health
          - sembr-process: Process full corpus
          - sembr-sample: Test with sample files

      - id: 3.4
        name: Documentation
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Updated documentation files:
          - ai-docs/project-status.yaml (this file)
          - ai-docs/sembr.yaml (implementation tasks)
          - ai-docs/index.yaml (status update)
          - ai-docs/pipeline.yaml (Stage 4 status)
          - ai-docs/testing-plan.yaml (test counts)

    server_protocol:
      endpoints:
        health: "GET /check"
        process: "POST /rewrap (JSON: {text, predict_func})"
      response_format: JSON with status, text fields

    model:
      name: admko/sembr2023-distilbert-base-multilingual-cased
      params: 135M
      languages: [English, Russian, Chinese, Spanish, German, French]

    implementation_notes: |
      Phase 3 COMPLETE - sembr integration fully implemented:
      - linebreaker.py handles HTTP client, retries, content validation
      - CLI provides full batch processing with progress
      - All 33 tests pass, pre-commit hooks pass
      - Ready for Phase 4 (Chunking)

  phase_4_chunking:
    status: COMPLETE
    completed_date: 2025-12-15
    description: Text chunking for embeddings
    reference_doc: ai-docs/chunking.yaml
    dependencies: Phases 2-3 (ArticleData + sembr'd text)
    test_counts:
      chunker_unit: 25
      chunk_metadata: 18
      cli_chunk: 18
      total: 61

    overview:
      input: sembr/{namespace}/{title}.txt + extracted article JSON
      output: chunks/{namespace}/{title}.jsonl
      token_targets:
        target: 600  # Revised from 350 to leverage EmbeddingGemma's 2048 limit
        min: 200
        max: 1000

    tasks:
      - id: 4.1
        name: Test Infrastructure (Red Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 52 (25 chunker + 18 metadata + 9 fixtures)
        details: |
          Created test infrastructure:
          - tests/unit/chunking/__init__.py
          - tests/unit/chunking/test_chunker.py (25 tests)
          - tests/unit/chunking/test_chunk_metadata.py (18 tests)
          - tests/fixtures/chunking/input/ (8 files)
          - tests/fixtures/chunking/metadata/ (3 JSON files)
          - conftest.py updated with chunking fixtures

      - id: 4.2
        name: Core Chunker Module (Green Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 52 passing
        details: |
          Created src/pw_mcp/ingest/chunker.py (443 lines):
          - ChunkConfig (frozen dataclass, 4 config fields)
          - Chunk (dataclass, 7 fields for chunk metadata)
          - ChunkedArticle (dataclass, 10 fields with metadata propagation)
          - is_section_header() - regex-based section detection
          - extract_section_title() - title extraction from headers
          - estimate_tokens() - word_count × factor estimation
          - generate_chunk_id() - URL-safe chunk ID generation
          - chunk_text() - core algorithm with section/paragraph awareness
          - chunk_article() - file wrapper with metadata propagation
          - write_chunks_jsonl() - JSONL serialization
          All 52 tests pass. Pre-commit hooks pass.

      - id: 4.3
        name: CLI Integration
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 18 passing
        details: |
          Updated src/pw_mcp/ingest/cli.py:
          - Added "chunk" subcommand with 7 options
          - -i/--input, -o/--output, -e/--extracted
          - --sample, --no-progress
          - --target-tokens, --max-tokens
          - Progress display with file counts
          - Random sampling support
          - Proper error handling and exit codes

          Created tests/unit/cli/__init__.py
          Created tests/unit/cli/test_chunk_cli.py (18 tests):
          - 8 parser tests (argument parsing, defaults)
          - 6 processing tests (file discovery, output)
          - 2 sample mode tests (limiting, random selection)
          - 2 progress tests (display, suppression)

      - id: 4.4
        name: Documentation
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Updated documentation files:
          - ai-docs/chunking.yaml (implementation status)
          - ai-docs/pipeline.yaml (Stage 5 status)
          - ai-docs/project-status.yaml (this file)
          - ai-docs/index.yaml (status update)

    key_algorithm: |
      1. Parse sembr output line by line
      2. Detect section headers (== ... ==) as hard breaks
      3. Track paragraph boundaries (blank lines) as soft breaks
      4. Accumulate lines until approaching max_tokens
      5. Prefer breaking at paragraph boundaries
      6. Track line numbers for citation back to source
      7. Propagate article-level metadata to all chunks

    implementation_notes: |
      Phase 4 COMPLETE - chunking module fully implemented:
      - chunker.py handles text chunking with section/paragraph awareness
      - CLI provides full batch processing with progress
      - All 70 tests pass (52 chunker + 18 CLI), pre-commit hooks pass
      - Ready for Phase 5 (Embedding)

  phase_5_embedding:
    status: COMPLETE
    description: Vector embedding generation
    reference_doc: ai-docs/embedding.yaml
    model: embeddinggemma (768-dim, Ollama local)
    output: embeddings/{namespace}/{title}.npy
    completed_date: 2025-12-15
    test_counts:
      embedder_unit: 21
      embed_cli: 10
      embed_slow: 4
      total: 35

    tasks:
      - id: 5.1
        name: Test Infrastructure (Red Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 31 (21 embedder + 10 CLI, all skipped initially)
        details: |
          Created test infrastructure:
          - tests/unit/embedding/__init__.py
          - tests/unit/embedding/test_embedder.py (21 tests)
          - tests/unit/embedding/test_embed_cli.py (10 tests)
          - tests/fixtures/embedding/sample_chunks.jsonl (5 chunks)
          - conftest.py updated with 6 embedding fixtures

      - id: 5.2
        name: Core Embedder Module (Green Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 21 passing
        details: |
          Created src/pw_mcp/ingest/embedder.py (complete):
          - EmbedConfig (frozen dataclass, 6 config fields, validated)
          - EmbeddedArticle (dataclass, 4 fields with numpy array)
          - OllamaConnectionError, OllamaModelError exceptions
          - embed_texts() with batching and exponential backoff retry
          - embed_article_chunks() with JSONL parsing, order preservation
          - write_embeddings_npy() with directory creation
          - check_ollama_ready() pre-flight health check
          Added ollama>=0.4.0 dependency to pyproject.toml
          All 21 unit tests pass. Pre-commit hooks pass.

      - id: 5.3
        name: CLI Integration
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 10 passing
        details: |
          Updated src/pw_mcp/ingest/cli.py:
          - Added "embed" subcommand with 7 options
          - -i/--input, -o/--output, --model, --batch-size
          - --sample, --no-progress, --host
          - Pre-flight Ollama health check
          - Progress display with [n/total] format
          - Resume support (skip existing .npy files)
          - Namespace directory structure preservation
          All 10 CLI tests pass. Pre-commit hooks pass.

      - id: 5.4
        name: Integration Tests + Documentation
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 4 passing (slow)
        details: |
          - Created tests/slow/test_embedding_slow.py (4 tests)
          - Added require_ollama_server fixture to conftest.py
          - Tests: embed_real_chunk, dimensions, normalized, deterministic
          - Updated ai-docs/ with final status

    nice_to_have:
      - Investigate parallel embeddinggemmas by directory (3 instances if GPU permits)
      - Async embedding for higher throughput

    implementation_notes: |
      Phase 5 COMPLETE - embedding module fully implemented:
      - embedder.py handles Ollama API, batching, retries, JSONL parsing
      - CLI provides full batch processing with progress and resume
      - Slow tests validate real Ollama integration (L2 norm, determinism)
      - All 35 tests pass (21 unit + 10 CLI + 4 slow), pre-commit hooks pass
      - Ready for Phase 6 (ChromaDB loading)

  phase_6_loading:
    status: future
    description: ChromaDB ingestion
    schema_reference: chromadb-schema.yaml#schema_phases
    note: |
      Schema is PHASED - MVP uses Phase A (13 fields from chunker).
      No chunker changes needed for MVP. See chromadb-schema.yaml for phases:
        Phase A (MVP): Core chunk + article metadata (13 fields) - ready now
        Phase B: Rich infobox/library_work metadata (14 fields) - needs chunker extension
        Phase C: Reference metadata (7 fields) - needs extraction enhancement
        Phase D: Graph metadata (2 fields) - DEFERRED
    tasks:
      - Finalize Phase A schema validation
      - Implement chroma.py loader
      - Batch upsert with metadata
      - Add loading tests
      - (Later) Extend chunker for Phase B fields

  phase_7_mcp:
    status: future
    description: MCP server implementation
    tasks:
      - Implement search tool
      - Implement get_article tool
      - Implement list_categories tool
      - Add MCP protocol tests

  phase_8_finetune:
    status: PLANNED
    description: Marxist-Leninist LLM fine-tuning on ProleWiki corpus
    reference_doc: ai-docs/finetune.yaml
    dependencies: Phases 1-7 (RAG pipeline provides Q&A source data)

    overview:
      goal: Fine-tune DeepSeek 7B to produce Marxist-Leninist domain expert
      method: QLoRA via Unsloth framework
      dataset: 3,000-5,000 Q&A pairs from ProleWiki
      output: GGUF for Ollama deployment
      hardware: 16-24GB VRAM (RTX 3090/4090/A4000)

    tasks:
      - id: 8.1
        name: Data Generation Pipeline
        status: PLANNED
        details: |
          Q&A generation from ProleWiki corpus:
          - infobox_to_qa() - factual pairs from infobox fields
          - section_to_qa() - conceptual pairs from article sections
          - library_to_qa() - quotes from primary sources
          - crossref_to_qa() - relationship pairs from internal links
          Target: 3,000-5,000 Q&A pairs

      - id: 8.2
        name: Dataset Preparation
        status: PLANNED
        details: |
          - Deduplication (exact + semantic)
          - Length filtering (50-500 tokens)
          - Train/validation/test splits (80/10/10)
          - Format: Alpaca JSONL

      - id: 8.3
        name: Training Infrastructure
        status: PLANNED
        details: |
          - Install unsloth framework
          - Create training script
          - Configure QLoRA: r=16, lora_alpha=16
          - Set up checkpoint saving and monitoring

      - id: 8.4
        name: Fine-tuning Execution
        status: PLANNED
        details: |
          - Train for 3-5 epochs
          - Monitor loss curves
          - Evaluate on validation set
          - Select best checkpoint

      - id: 8.5
        name: Evaluation and Export
        status: PLANNED
        details: |
          - Perplexity measurement on test set
          - Manual evaluation (50 hand-picked questions)
          - Export to GGUF format
          - Create Ollama Modelfile
          - Integration test with Ollama

    model_info:
      base_model: DeepSeek-R1-7B
      huggingface: unsloth/DeepSeek-R1-7B
      params: 7B
      context_length: 4096

# =============================================================================
# CURRENT FOCUS
# =============================================================================

current_focus:
  phase: 6 (ChromaDB Loading) - NEXT
  completed: [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 3.1, 3.2, 3.3, 3.4, 4.1, 4.2, 4.3, 4.4, 5.1, 5.2, 5.3, 5.4]
  next_task: 6.1 - Phase A Schema Implementation
  reference: ai-docs/chromadb-schema.yaml

  next_action: |
    Phase 5 COMPLETE - embedding module fully implemented with integration tests.

    Phase 5 Summary:
    5.1 Red Phase - COMPLETE (31 tests created)
    5.2 Green Phase - COMPLETE (embedder.py implemented, 21 tests pass)
    5.3 CLI Integration - COMPLETE (embed subcommand, 10 tests pass)
    5.4 Integration Tests - COMPLETE (4 slow tests with live Ollama)

    Phase 6 Next Steps:
    - Implement Phase A schema (13 fields from chunker output)
    - Create ChromaDB loader module
    - Batch upsert with metadata
    - Add loading tests

    Key Implementations from Phase 5:
    - src/pw_mcp/ingest/embedder.py: EmbedConfig, EmbeddedArticle, embed_texts(), etc.
    - CLI: pw-ingest embed -i chunks/ -o embeddings/ (7 options)
    - Resume support, pre-flight check, batching, retry logic

  # How Implementation Phases map to Pipeline Stages
  phase_to_stage_mapping: |
    Implementation Phases (work order)     Pipeline Stages (data flow)
    ─────────────────────────────────      ─────────────────────────────
    Phase 1: Foundation (infrastructure)   (no stage - test setup)
    Phase 2: Extraction (parsers)    ────► Stage 1: Extraction
    Phase 3: Sembr                   ────► Stage 2: Sembr
    Phase 4: Chunking                ────► Stage 3: Chunking
    Phase 5: Embedding               ────► Stage 4: Embedding
    Phase 6: Loading                 ────► Stage 5: ChromaDB Loading
    Phase 7: MCP Server                    (consumer of database)
    Phase 8: Fine-tuning                   (separate goal)
    (Optional) Graph                       (deferred enhancement)

  test_status:
    link_parser: 28 passing
    citation_parser: 48 passing
    infobox_parser: 25 passing
    library_work_parser: 25 passing
    quote_parser: 12 passing
    extraction_pipeline: 22 passing
    linebreaker: 33 passing
    sembr_integration: 8 passing
    sembr_slow: 9 passing
    chunker: 25 passing
    chunk_metadata: 18 passing
    cli_chunk: 18 passing
    embedder: 21 passing
    embed_cli: 10 passing
    embed_slow: 4 passing
    total: 312 passing

# =============================================================================
# KEY DECISIONS
# =============================================================================

decisions:
  tdd_approach:
    decision: Use pytest.skip() for Red Phase tests
    rationale: Tests pass as "skipped" rather than failing with ImportError
    date: 2024-12-14

  fixture_source:
    decision: Use prolewiki-exports/ for fixture extraction
    rationale: Real corpus data ensures realistic test coverage
    date: 2024-12-14

  parser_order:
    decision: Implement link -> citation -> infobox -> library_work -> quote
    rationale: Increasing complexity, foundational parsers first
    date: 2024-12-14

  type_stubs:
    decision: Create proper stub files for mcp library instead of type-ignore comments
    rationale: Proper stubs provide real type safety without suppressing errors
    date: 2025-12-14
    files:
      - stubs/mcp/__init__.pyi
      - stubs/mcp/server/__init__.pyi
      - stubs/mcp/server/fastmcp.pyi

  mwparserfromhell:
    decision: Use mwparserfromhell for template parsing, not regex
    rationale: |
      Handles nested templates, multiline, edge cases that regex cannot.
      Already a project dependency.
    date: 2025-12-14

  generic_template_removal:
    decision: Use mwparserfromhell to remove ALL templates from clean_text
    rationale: |
      Corpus has 948 unique template types (20,069 instances).
      Specific parsers extract metadata, then all templates removed.
      Ensures clean text for sembr and embeddings.
    date: 2025-12-14

  lock_file_policy:
    decision: Exclude uv.lock from pre-commit large file check
    rationale: Lock files are auto-generated, necessary for reproducibility
    date: 2025-12-14

  schema_phasing:
    decision: Phase ChromaDB schema into A (MVP) / B / C / D increments
    rationale: |
      Schema has 60+ fields but chunker only outputs 13.
      Phase A (13 fields): Already output by chunker - ship MVP now.
      Phase B (14 fields): Infobox/library_work - data extracted, needs serialization.
      Phase C (7 fields): Reference metadata - needs extraction enhancement.
      Phase D (2 fields): Graph metadata - requires optional graph computation.
      This enables MVP delivery without blocking on full schema implementation.
    date: 2025-12-15
    reference: chromadb-schema.yaml#schema_phases

  sembr_server_management:
    decision: Auto-restart sembr server by default with CUDA cleanup
    rationale: |
      The sembr server is single-threaded and can enter corrupted CUDA state.
      Problems encountered:
      - "Already borrowed" errors on concurrent requests (server mutex)
      - CUDA device-side assert errors persisting after server restart
      Fix: --restart-server flag (default True) kills existing server,
      clears CUDA cache, and starts fresh instance before batch processing.
      --no-restart available for using existing server instance.
    date: 2025-12-15
    implementation: src/pw_mcp/ingest/cli.py

  graceful_batch_error_handling:
    decision: Return None for failed files instead of failing entire batch
    rationale: |
      process_batch() was using asyncio.gather() which propagates exceptions.
      When one file fails, the entire batch would crash.
      Fix: Wrap individual file processing in try/except, return None for
      failures. Callers can filter [r for r in results if r is not None].
      This enables processing large batches with partial failures.
    date: 2025-12-15
    implementation: src/pw_mcp/ingest/linebreaker.py

# =============================================================================
# FILE LOCATIONS
# =============================================================================

key_files:
  source_code:
    parsers:
      - src/pw_mcp/ingest/parsers/__init__.py   # Public API exports
      - src/pw_mcp/ingest/parsers/types.py      # Link, Citation, InfoboxData, LibraryWorkData, QuoteData, ArticleData
      - src/pw_mcp/ingest/parsers/link.py       # parse_links() - 28 tests
      - src/pw_mcp/ingest/parsers/citation.py   # parse_citations() - 48 tests
      - src/pw_mcp/ingest/parsers/infobox.py    # parse_infobox() - 25 tests
      - src/pw_mcp/ingest/parsers/library_work.py  # parse_library_work() - 25 tests
      - src/pw_mcp/ingest/parsers/quote.py      # parse_quotes() - 12 tests
    extraction:
      - src/pw_mcp/ingest/extraction.py         # extract_article() - 22 tests
    sembr:
      - src/pw_mcp/ingest/linebreaker.py        # sembr HTTP client - 33 tests
    chunking:
      - src/pw_mcp/ingest/chunker.py            # chunk_text(), chunk_article() - 43 tests
    embedding:
      - src/pw_mcp/ingest/embedder.py           # embed_texts(), embed_article_chunks() - 31 tests
    server:
      - src/pw_mcp/server.py
      - src/pw_mcp/config.py
      - src/pw_mcp/ingest/cli.py                # pw-ingest CLI with sembr, chunk, embed subcommands

  type_stubs:
    - stubs/mcp/__init__.pyi
    - stubs/mcp/server/__init__.pyi
    - stubs/mcp/server/fastmcp.pyi

  tests:
    unit:
      - tests/unit/extraction/test_link_parser.py          # 28 tests
      - tests/unit/extraction/test_citation_parser.py      # 48 tests
      - tests/unit/extraction/test_infobox_parser.py       # 25 tests
      - tests/unit/extraction/test_library_work_parser.py  # 25 tests
      - tests/unit/extraction/test_quote_parser.py         # 12 tests
      - tests/unit/sembr/test_linebreaker.py               # 33 tests
      - tests/unit/chunking/test_chunker.py                # 25 tests
      - tests/unit/chunking/test_chunk_metadata.py         # 18 tests
      - tests/unit/cli/test_chunk_cli.py                   # 18 tests
      - tests/unit/embedding/test_embedder.py              # 21 tests
      - tests/unit/embedding/test_embed_cli.py             # 10 tests
    slow:
      - tests/slow/test_sembr_slow.py                      # 9 tests (sembr server)
      - tests/slow/test_embedding_slow.py                  # 4 tests (Ollama)
    integration:
      - tests/integration/test_extraction_pipeline.py      # 22 tests
    config:
      - tests/conftest.py

  fixtures:
    - tests/fixtures/mediawiki/infoboxes/     # 7 infobox type samples
    - tests/fixtures/mediawiki/citations/     # 7 citation type samples
    - tests/fixtures/mediawiki/links/         # 3 link type samples
    - tests/fixtures/mediawiki/library_work/  # 7 library work samples
    - tests/fixtures/mediawiki/quotes/        # 3 quote samples
    - tests/fixtures/mediawiki/edge_cases/    # unicode, nested, multiline
    - tests/fixtures/sembr/input/             # 7 sembr input samples
    - tests/fixtures/sembr/expected/          # 5 sembr expected outputs
    - tests/fixtures/sembr/mock_responses/    # 3 HTTP mock responses
    - tests/fixtures/chunking/input/          # 8 chunking input samples
    - tests/fixtures/chunking/metadata/       # 3 article metadata JSONs
    - tests/fixtures/embedding/               # 5-chunk sample JSONL fixture

  corpus:
    - prolewiki-exports/Main/       # ~4000+ encyclopedia articles
    - prolewiki-exports/Library/    # Full books (Marx, Lenin, etc.)
    - prolewiki-exports/Essays/     # User-contributed essays
    - prolewiki-exports/ProleWiki/  # Meta/admin pages

  documentation:
    - ai-docs/index.yaml
    - ai-docs/chromadb.yaml
    - ai-docs/chromadb-schema.yaml
    - ai-docs/pipeline.yaml
    - ai-docs/testing-plan.yaml
    - ai-docs/project-status.yaml  # This file

# =============================================================================
# DATACLASS REFERENCE
# =============================================================================

dataclasses:
  Link:
    fields: [target, display, link_type, section, start, end]
    link_types: [internal, category, external, interwiki]

  Citation:
    fields: |
      type, title, author, year, url, lg_url, pdf_url, publisher,
      newspaper, chapter, page, isbn, date, retrieved, channel,
      quote, ref_name, link, published_location, translation_title,
      translation_language
    types: [book, web, news, youtube, video, library]

  InfoboxData:
    fields: [type, fields, remaining_text, extracted_links]
    types: |
      politician, country, political_party, person, revolutionary,
      essay, philosopher, company, settlement, military_person,
      organization, guerilla_organization, youtuber, military_conflict,
      book, religion, website, transcript, flag

  LibraryWorkData:
    fields: |
      title, author, authors, year, work_type, source_url,
      published_location, translator, original_language,
      publisher, audiobook_url, remaining_text

  QuoteData:
    fields: [text, attribution]

  ArticleData:
    fields: |
      source_path, namespace, title, clean_text, sections,
      categories, links, citations, infobox, library_work,
      quotes, is_stub, citation_needed_count, has_blockquote,
      reference_count

  SembrConfig:
    fields: |
      server_url, model_name, timeout_seconds, max_retries,
      retry_delay_seconds, batch_size, predict_func
    frozen: true

  SembrResult:
    fields: |
      text, line_count, processing_time_ms,
      input_word_count, output_word_count

  ChunkConfig:
    fields: |
      target_tokens, min_tokens, max_tokens,
      token_estimation_factor
    frozen: true

  Chunk:
    fields: |
      text, chunk_index, section, line_start,
      line_end, word_count, estimated_tokens

  ChunkedArticle:
    fields: |
      article_title, namespace, chunks, categories,
      internal_links, infobox, library_work, is_stub,
      citation_needed_count, has_blockquote

  EmbedConfig:
    fields: |
      model, dimensions, batch_size, ollama_host,
      max_retries, retry_delay
    frozen: true
    validated: true

  EmbeddedArticle:
    fields: |
      article_title, namespace, num_chunks, embeddings
    embeddings_shape: (num_chunks, 768)
    embeddings_dtype: float32

# =============================================================================
# COMMANDS
# =============================================================================

useful_commands:
  testing:
    all_tests: mise run test
    unit_only: mise run test-fast
    integration: mise run test-integration
    with_coverage: mise run test-cov
    specific_file: uv run pytest tests/unit/extraction/test_citation_parser.py -v

  code_quality:
    lint: uv run ruff check src/ tests/
    format: uv run ruff format src/ tests/
    typecheck: uv run mypy src/pw_mcp/
    all_checks: mise run check
    pre_commit: mise run pre-commit

  development:
    install: mise run install
    sembr_server: mise run sembr-server
    sembr_check: mise run sembr-check
    sembr_process: mise run sembr-process
    sembr_sample: mise run sembr-sample
    chunk_help: uv run pw-ingest chunk --help
    chunk_sample: uv run pw-ingest chunk --sample 10
    chunk_process: uv run pw-ingest chunk -i sembr/ -o chunks/
    embed_help: uv run pw-ingest embed --help
    embed_sample: uv run pw-ingest embed --sample 10
    embed_process: uv run pw-ingest embed -i chunks/ -o embeddings/
