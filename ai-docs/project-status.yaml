# Project Status - pw-mcp
# Purpose: Track implementation progress and next steps
# Updated: 2025-12-14

overview:
  project: ProleWiki MCP Server
  goal: Semantic vector search over ProleWiki corpus via MCP
  artifact: ChromaDB database distributable independently of source corpus
  corpus_size: ~5,222 files in prolewiki-exports/

# =============================================================================
# IMPLEMENTATION PHASES
# =============================================================================

phases:
  phase_1_foundation:
    status: COMPLETE
    description: Test infrastructure and TDD Red Phase
    completed:
      - pyproject.toml updated (hypothesis, new markers)
      - .mise.toml updated (test-integration, test-property, test-benchmark, test-retrieval)
      - Test directory structure created
      - tests/conftest.py with shared fixtures
      - Fixture files extracted from prolewiki-exports/
      - test_infobox_parser.py (24 tests, all skip)
      - test_citation_parser.py (28 tests, all skip)
      - test_link_parser.py (30 tests, all skip)
      - test_extraction_pipeline.py (22 tests, all skip)
    test_counts:
      total: 104
      unit: 82
      integration: 22
      status: all skip (TDD Red Phase complete)

  phase_2_parsers:
    status: IN_PROGRESS
    description: TDD Green Phase - implement parsers to pass tests
    tasks:
      - id: 2.1
        name: Create parser module structure
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          src/pw_mcp/ingest/parsers/
          ├── __init__.py   # Public API exports
          ├── types.py      # Shared dataclasses (Link, Citation, InfoboxData)
          ├── infobox.py    # parse_infobox() -> InfoboxData | None
          ├── citation.py   # parse_citations() -> list[Citation]
          └── link.py       # parse_links() -> list[Link]
        implementation_notes: |
          - types.py contains LinkType, CitationType, InfoboxType literals
          - Link dataclass: target, display, link_type, section, start, end
          - Citation dataclass: ref_type, title, authors, year, url, quote, etc.
          - InfoboxData dataclass: type, fields, remaining_text, extracted_links
          - Stub functions raise NotImplementedError for TDD workflow

      - id: 2.2
        name: Implement link parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: tests/unit/extraction/test_link_parser.py (28 tests passing)
        details: |
          Regex-based parser for MediaWiki links:
          - [[Target]] simple internal links
          - [[Target|Display]] piped links
          - [[Category:Name]] category links (strips "Category:" prefix)
          - [https://url text] external links
          - [[Article#Section]] section anchors
        implementation_notes: |
          Functions implemented:
          - parse_links(text, link_type=None, include_positions=False) -> list[Link]
          - count_internal_links(text) -> int
          - count_categories(text) -> int
          - get_unique_targets(text) -> set[str]

          Edge cases handled:
          - Empty [[]] targets (skipped)
          - File:/Image:/Media: links (skipped)
          - Interwiki links ru:, de:, etc. (skipped)
          - Multiple pipes [[A|B|C]] (target=A, display=B|C)
          - Unicode in targets (fully supported)
          - Consecutive links without spaces
          - Position tracking for text replacement

      - id: 2.3
        name: Implement citation parser
        details: |
          Parse MediaWiki citation templates:
          - {{Citation|...}} book citations
          - {{Web citation|...}} web articles
          - {{News citation|...}} news articles
          - {{YouTube citation|...}} video citations
          - {{Library citation|...}} internal Library links
          Handle <ref name="..."> named refs and ref reuse.
        tests: tests/unit/extraction/test_citation_parser.py (28 tests)
        status: pending

      - id: 2.4
        name: Implement infobox parser
        details: |
          Most complex parser. Uses mwparserfromhell.
          - Detect infobox type (politician, country, party, etc.)
          - Extract all fields as dict
          - Handle nested templates in values
          - Handle <br> separated multi-values
          - Strip [[links]] from field values
          - Return remaining text after infobox removal
        tests: tests/unit/extraction/test_infobox_parser.py (24 tests)
        status: pending

      - id: 2.5
        name: Implement extraction pipeline
        details: |
          Orchestrate parsers into full extraction:
          - extract_article(text, source_path) -> ArticleData
          - Combine infobox, citations, links, categories
          - Generate clean_text with markup removed
          - Detect namespace from source_path
          - Extract section headers
          - Count references, detect stubs, etc.
        tests: tests/integration/test_extraction_pipeline.py (22 tests)
        status: pending

  phase_3_sembr:
    status: future
    description: Semantic linebreaking integration
    tasks:
      - Integrate sembr server mode for batch processing
      - Create linebreaker.py wrapper
      - Add sembr tests

  phase_4_chunking:
    status: future
    description: Text chunking for embeddings
    tasks:
      - Implement chunker.py
      - Respect section boundaries
      - Target 200-500 tokens per chunk
      - Store line offsets for citations
      - Add chunking tests

  phase_5_embedding:
    status: future
    description: Vector embedding generation
    tasks:
      - Integrate Ollama embeddinggemma
      - Batch embedding with progress
      - Add embedding quality tests

  phase_6_loading:
    status: future
    description: ChromaDB ingestion
    tasks:
      - Implement chroma.py loader
      - Batch upsert with metadata
      - Add loading tests

  phase_7_mcp:
    status: future
    description: MCP server implementation
    tasks:
      - Implement search tool
      - Implement get_article tool
      - Implement list_categories tool
      - Add MCP protocol tests

# =============================================================================
# CURRENT FOCUS
# =============================================================================

current_focus:
  phase: 2 (Green Phase - Parsers)
  completed: [2.1, 2.2]
  next_task: 2.3 (citation parser)
  next_action: |
    Continue implementing parsers in order of complexity:
    ✅ 1. link.py (COMPLETE - 28/28 tests passing)
    ⏳ 2. citation.py (template parsing) - NEXT
    ⏳ 3. infobox.py (most complex)
    ⏳ 4. extraction pipeline (orchestration)

    For each parser:
    1. Remove pytest.skip() from tests
    2. Uncomment assertions
    3. Implement code to pass tests
    4. Run tests to verify
    5. Refactor if needed

  test_status:
    link_parser: 28 passing
    citation_parser: 28 skipped
    infobox_parser: 24 skipped
    extraction_pipeline: 22 skipped
    total: 102 tests (28 passing, 74 skipped)

# =============================================================================
# KEY DECISIONS
# =============================================================================

decisions:
  tdd_approach:
    decision: Use pytest.skip() for Red Phase tests
    rationale: Tests pass as "skipped" rather than failing with ImportError
    date: 2024-12-14

  fixture_source:
    decision: Use prolewiki-exports/ for fixture extraction
    rationale: Real corpus data, prolewiki-sembr-sample/ was only for exploration
    date: 2024-12-14

  parser_order:
    decision: Implement link -> citation -> infobox
    rationale: Increasing complexity, link parser is foundation for others
    date: 2024-12-14

  type_stubs:
    decision: Create proper stub files for mcp library instead of # type: ignore
    rationale: |
      Avoid "reward hacking" by suppressing type errors.
      Proper stubs in stubs/mcp/ provide real type safety.
    date: 2025-12-14
    files:
      - stubs/mcp/__init__.pyi
      - stubs/mcp/server/__init__.pyi
      - stubs/mcp/server/fastmcp.pyi

  lock_file_policy:
    decision: Exclude uv.lock from pre-commit large file check
    rationale: Lock files are auto-generated, necessary for reproducibility, often exceed 500KB
    date: 2025-12-14

# =============================================================================
# FILE LOCATIONS
# =============================================================================

key_files:
  source_code:
    - src/pw_mcp/ingest/parsers/types.py    # Dataclasses: Link, Citation, InfoboxData
    - src/pw_mcp/ingest/parsers/link.py     # IMPLEMENTED - parse_links()
    - src/pw_mcp/ingest/parsers/citation.py # Stub - parse_citations()
    - src/pw_mcp/ingest/parsers/infobox.py  # Stub - parse_infobox()
    - src/pw_mcp/ingest/cli.py
    - src/pw_mcp/server.py
    - src/pw_mcp/config.py

  type_stubs:
    - stubs/mcp/__init__.pyi
    - stubs/mcp/server/__init__.pyi
    - stubs/mcp/server/fastmcp.pyi  # FastMCP type definitions

  tests:
    - tests/conftest.py
    - tests/unit/extraction/test_infobox_parser.py
    - tests/unit/extraction/test_citation_parser.py
    - tests/unit/extraction/test_link_parser.py
    - tests/integration/test_extraction_pipeline.py

  fixtures:
    - tests/fixtures/mediawiki/infoboxes/  # 7 types
    - tests/fixtures/mediawiki/citations/  # 5 types
    - tests/fixtures/mediawiki/links/      # 3 types
    - tests/fixtures/mediawiki/edge_cases/ # unicode, nested, multiline

  corpus:
    - prolewiki-exports/Main/      # ~4000+ encyclopedia articles
    - prolewiki-exports/Library/   # Full books (Marx, Lenin, etc.)
    - prolewiki-exports/Essays/    # User-contributed essays
    - prolewiki-exports/ProleWiki/ # Meta/admin pages

  documentation:
    - ai-docs/index.yaml           # This index
    - ai-docs/chromadb.yaml        # ChromaDB reference
    - ai-docs/chromadb-schema.yaml # Schema definition
    - ai-docs/pipeline.yaml        # Pipeline architecture
    - ai-docs/testing-plan.yaml    # Testing strategy
    - ai-docs/project-status.yaml  # This file

# =============================================================================
# COMMANDS
# =============================================================================

useful_commands:
  testing:
    all_tests: mise run test
    unit_only: mise run test-fast
    integration: mise run test-integration
    with_coverage: mise run test-cov

  code_quality:
    lint: uv run ruff check src/ tests/
    format: uv run ruff format src/ tests/
    typecheck: uv run mypy src/pw_mcp/
    all_checks: mise run check
    pre_commit: mise run pre-commit

  development:
    install: mise run install
    sembr_server: mise run sembr-server
