# Project Status - pw-mcp
# Purpose: Track implementation progress and next steps
# Updated: 2025-12-16 (MAJOR REFACTOR - Replaced sembr with tiktoken-based chunking)
#
# KEY CHANGE: Pipeline is now 4 stages (extract â†’ chunk â†’ embed â†’ load)
# The sembr ML pipeline was completely removed (~6000 lines of code)

overview:
  project: ProleWiki MCP Server
  goal: Semantic vector search over ProleWiki corpus via MCP
  artifact: ChromaDB database distributable independently of source corpus
  corpus_size: 5,222 files in prolewiki-exports/
  corpus_stats:
    total_templates: 20,069
    unique_template_types: 948
    total_characters: 197.1M

# =============================================================================
# IMPLEMENTATION PHASES
# =============================================================================

phases:
  phase_1_foundation:
    status: COMPLETE
    description: Test infrastructure and TDD Red Phase
    completed:
      - pyproject.toml updated (hypothesis, new markers)
      - .mise.toml updated (test-integration, test-property, test-benchmark, test-retrieval)
      - Test directory structure created
      - tests/conftest.py with shared fixtures
      - Fixture files extracted from prolewiki-exports/

  phase_2_parsers:
    status: COMPLETE
    description: TDD Green Phase - implement parsers to pass tests
    completed_date: 2025-12-14
    test_counts:
      link_parser: 28
      citation_parser: 48
      infobox_parser: 25
      library_work_parser: 25
      quote_parser: 12
      extraction_pipeline: 22
      total: 160

    tasks:
      - id: 2.1
        name: Create parser module structure
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          src/pw_mcp/ingest/parsers/
          â”œâ”€â”€ __init__.py        # Public API exports
          â”œâ”€â”€ types.py           # Shared dataclasses
          â”œâ”€â”€ link.py            # parse_links()
          â”œâ”€â”€ citation.py        # parse_citations()
          â”œâ”€â”€ infobox.py         # parse_infobox()
          â”œâ”€â”€ library_work.py    # parse_library_work()
          â””â”€â”€ quote.py           # parse_quotes()

      - id: 2.2
        name: Implement link parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 28 passing
        details: |
          Regex-based parser for MediaWiki links:
          - [[Target]] simple internal links
          - [[Target|Display]] piped links
          - [[Category:Name]] category links
          - [https://url text] external links
          - [[Article#Section]] section anchors
        implementation: src/pw_mcp/ingest/parsers/link.py

      - id: 2.3
        name: Implement citation parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 48 passing
        details: |
          mwparserfromhell-based parser for 7 citation types:
          - {{Citation|...}} book citations
          - {{Web citation|...}} web articles
          - {{News citation|...}} news articles
          - {{YouTube citation|...}} video citations
          - {{Video citation|...}} video citations (YouTube alternative)
          - {{Library citation|...}} internal Library links
          - {{Textcite|...}} book/web citations with translation fields
          Handle <ref name="..."> named refs and ref reuse/deduplication.
        implementation: src/pw_mcp/ingest/parsers/citation.py
        corpus_coverage:
          web_citation: 5,499
          citation: 4,227
          news_citation: 1,574
          library_citation: 253
          youtube_citation: 85
          video_citation: 83
          textcite: 30

      - id: 2.4
        name: Implement infobox parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          mwparserfromhell-based parser for 19 infobox types:
          - politician, country, political_party, person, revolutionary
          - essay, philosopher, company, settlement, military_person
          - organization, guerilla_organization, youtuber, military_conflict
          - book, religion, website, transcript, flag
          Features: nested templates, <br> multi-values, [[link]] extraction
        implementation: src/pw_mcp/ingest/parsers/infobox.py
        corpus_coverage:
          infobox_politician: 644
          infobox_country: 443
          infobox_political_party: 419
          infobox_person: 249
          infobox_revolutionary: 114
          # ... 14 more types

      - id: 2.5
        name: Implement extraction pipeline
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 22 passing
        details: |
          Orchestrates all parsers into full extraction:
          - extract_article(text, source_path) -> ArticleData
          - Combines infobox, citations, links, quotes, categories
          - Generates clean_text with ALL templates removed
          - Detects namespace from source_path
          - Extracts section headers
          - Counts references, detects stubs, blockquotes
        implementation: src/pw_mcp/ingest/extraction.py

      - id: 2.6
        name: Implement library work parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 25 passing
        details: |
          Parser for {{Library work}} template in Library namespace.
          Extracts bibliographic metadata: title, author, year, type,
          publisher, source URL, translator, audiobook URL, etc.
          Handles 70 work types found in corpus.
        implementation: src/pw_mcp/ingest/parsers/library_work.py
        corpus_coverage: 1,027 instances

      - id: 2.7
        name: Implement quote parser
        status: COMPLETE
        completed_date: 2025-12-14
        tests: 12 passing
        details: |
          Parser for {{Quote|text|attribution}} template.
          Extracts block quotes with optional attribution.
        implementation: src/pw_mcp/ingest/parsers/quote.py
        corpus_coverage: 223 instances

      - id: 2.8
        name: Generic template removal
        status: COMPLETE
        completed_date: 2025-12-14
        details: |
          Added _strip_all_templates() using mwparserfromhell.
          Removes ALL 948 template types from clean_text.
          100% removal rate: 20,069 templates across 5,222 files.
          Essential for clean text generation before semantic linebreaking.
        implementation: src/pw_mcp/ingest/extraction.py

  phase_3_sembr:
    status: REMOVED (2025-12-16)
    description: |
      Semantic linebreaking was COMPLETELY REMOVED and replaced with
      tiktoken-based chunking. This simplified the pipeline from 5 stages
      to 4 stages and removed ~6000 lines of complex ML infrastructure.

    removal_reason: |
      Sembr required:
      - GPU server management with CUDA crash recovery
      - HTTP client with retry logic, semaphores, health checks
      - 135M parameter DistilBERT model (6+ second startup)
      - Complex error handling for device-side assertions
      - ~9 seconds per file processing time
      - ~6000 lines of infrastructure code

      Tiktoken replacement provides:
      - No server needed (runs in-process)
      - No GPU needed (pure CPU)
      - Deterministic output
      - <1ms per file processing
      - Zero crash risk
      - ~600 lines of simple code

    files_deleted:
      - src/pw_mcp/ingest/linebreaker.py (~950 lines)
      - src/pw_mcp/ingest/server_manager.py (~740 lines)
      - src/pw_mcp/ingest/gpu_manager.py (~520 lines)
      - tests/unit/sembr/test_linebreaker.py (~1400 lines)
      - tests/unit/ingest/test_server_manager.py (~600 lines)
      - tests/unit/ingest/test_gpu_manager.py (~500 lines)
      - tests/fixtures/sembr/ (entire directory)
      - ai-docs/sembr.yaml

    replacement: |
      See phase_4_chunking - now uses tiktoken-based chunking that
      reads directly from extracted/ JSON files (no sembr preprocessing).
      Pipeline: extract â†’ chunk â†’ embed â†’ load (4 stages)

  phase_4_chunking:
    status: COMPLETE (PERFORMANCE FIX 2025-12-24)
    completed_date: 2025-12-16
    enhanced_date: 2025-12-24
    description: Tiktoken-based text chunking (replaced sembr dependency)
    reference_doc: ai-docs/chunking.yaml
    refactoring_plan: ai-docs/chunker-refactoring.yaml
    dependencies: Phase 2 only (ArticleData) - no sembr needed
    test_counts:
      chunker_unit: 107  # Updated with fast path tests
      cli_chunk: 18
      integration: 26
      total: 133

    performance_fix_2025_12_24:
      problem: "O(nÂ²) string operations caused timeout on large headerless documents"
      example: "Political Economy (1.9MB, 8,009 lines) was timing out"
      solution: |
        Added fast path for headerless documents using semantic-text-splitter:
        - Detects documents without section headers (== ... ==)
        - Uses Rust/SIMD-optimized semantic-text-splitter directly
        - Avoids O(nÂ²) line-by-line string reconstruction
        - Character-to-line mapping for accurate citations
      result: "Political Economy now processes in 3.11 seconds with zero duplicates"
      new_dependency: "semantic-text-splitter>=0.28.0"
      files_modified:
        - src/pw_mcp/ingest/chunker.py  # Added _chunk_without_headers()
        - tests/unit/chunking/test_chunker.py  # Updated with realistic test data
        - pyproject.toml  # Added semantic-text-splitter dependency

    key_changes_2025_12_16:
      - "Replaced word-based estimation with tiktoken (cl100k_base)"
      - "Added oversized line handling (splits at sentence/word boundaries)"
      - "Added token-aware batching for OpenAI embeddings API"
      - "Reads directly from extracted/ (no sembr preprocessing)"
      - "No chunk exceeds max_tokens (enforced by splitting)"

    overview:
      input: extracted/articles/{namespace}/{title}.json (clean_text field)
      output: chunks/{namespace}/{title}.jsonl
      token_targets:
        target: 350
        min: 100
        max: 1000
        overlap: 50

    tasks:
      - id: 4.1
        name: Test Infrastructure (Red Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 52 (25 chunker + 18 metadata + 9 fixtures)
        details: |
          Created test infrastructure:
          - tests/unit/chunking/__init__.py
          - tests/unit/chunking/test_chunker.py (25 tests)
          - tests/unit/chunking/test_chunk_metadata.py (18 tests)
          - tests/fixtures/chunking/input/ (8 files)
          - tests/fixtures/chunking/metadata/ (3 JSON files)
          - conftest.py updated with chunking fixtures

      - id: 4.2
        name: Core Chunker Module (Green Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 52 passing
        details: |
          Created src/pw_mcp/ingest/chunker.py (443 lines):
          - ChunkConfig (frozen dataclass, 4 config fields)
          - Chunk (dataclass, 7 fields for chunk metadata)
          - ChunkedArticle (dataclass, 10 fields with metadata propagation)
          - is_section_header() - regex-based section detection
          - extract_section_title() - title extraction from headers
          - estimate_tokens() - word_count Ã— factor estimation
          - generate_chunk_id() - URL-safe chunk ID generation
          - chunk_text() - core algorithm with section/paragraph awareness
          - chunk_article() - file wrapper with metadata propagation
          - write_chunks_jsonl() - JSONL serialization
          All 52 tests pass. Pre-commit hooks pass.

      - id: 4.3
        name: CLI Integration
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 18 passing
        details: |
          Updated src/pw_mcp/ingest/cli.py:
          - Added "chunk" subcommand with 7 options
          - -i/--input, -o/--output, -e/--extracted
          - --sample, --no-progress
          - --target-tokens, --max-tokens
          - Progress display with file counts
          - Random sampling support
          - Proper error handling and exit codes

          Created tests/unit/cli/__init__.py
          Created tests/unit/cli/test_chunk_cli.py (18 tests):
          - 8 parser tests (argument parsing, defaults)
          - 6 processing tests (file discovery, output)
          - 2 sample mode tests (limiting, random selection)
          - 2 progress tests (display, suppression)

      - id: 4.4
        name: Documentation
        status: COMPLETE
        completed_date: 2025-12-15
        details: |
          Updated documentation files:
          - ai-docs/chunking.yaml (implementation status)
          - ai-docs/pipeline.yaml (Stage 5 status)
          - ai-docs/project-status.yaml (this file)
          - ai-docs/index.yaml (status update)

    key_algorithm: |
      1. Parse extracted text line by line
      2. Detect section headers (== ... ==) as hard breaks
      3. Track paragraph boundaries (blank lines) as soft breaks
      4. Accumulate lines until approaching max_tokens
      5. Prefer breaking at paragraph boundaries
      6. Track line numbers for citation back to source
      7. Propagate article-level metadata to all chunks

    implementation_notes: |
      Phase 4 COMPLETE - chunking module fully implemented:
      - chunker.py handles text chunking with section/paragraph awareness
      - CLI provides full batch processing with progress
      - All 70 tests pass (52 chunker + 18 CLI), pre-commit hooks pass
      - Ready for Phase 5 (Embedding)

  phase_5_embedding:
    status: COMPLETE
    description: Vector embedding generation
    reference_doc: ai-docs/embedding.yaml
    architecture: "Dual-provider: OpenAI (primary) + Ollama (fallback)"
    models:
      primary: "OpenAI text-embedding-3-large (1536-dim)"
      fallback: "Ollama embeddinggemma (768-dim)"
    output: embeddings/{namespace}/{title}.npy
    completed_date: 2025-12-15
    enhanced_date: 2025-12-16
    test_counts:
      embedder_unit: 21
      embed_cli: 10
      embed_slow: 4
      total: 35

    tasks:
      - id: 5.1
        name: Test Infrastructure (Red Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 31 (21 embedder + 10 CLI, all skipped initially)
        details: |
          Created test infrastructure:
          - tests/unit/embedding/__init__.py
          - tests/unit/embedding/test_embedder.py (21 tests)
          - tests/unit/embedding/test_embed_cli.py (10 tests)
          - tests/fixtures/embedding/sample_chunks.jsonl (5 chunks)
          - conftest.py updated with 6 embedding fixtures

      - id: 5.2
        name: Core Embedder Module (Green Phase)
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 21 passing
        details: |
          Created src/pw_mcp/ingest/embedder.py (complete):
          - EmbedConfig (frozen dataclass, 6 config fields, validated)
          - EmbeddedArticle (dataclass, 4 fields with numpy array)
          - OllamaConnectionError, OllamaModelError exceptions
          - embed_texts() with batching and exponential backoff retry
          - embed_article_chunks() with JSONL parsing, order preservation
          - write_embeddings_npy() with directory creation
          - check_ollama_ready() pre-flight health check
          Added ollama>=0.4.0 dependency to pyproject.toml
          All 21 unit tests pass. Pre-commit hooks pass.

      - id: 5.3
        name: CLI Integration
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 10 passing
        details: |
          Updated src/pw_mcp/ingest/cli.py:
          - Added "embed" subcommand with 7 options
          - -i/--input, -o/--output, --model, --batch-size
          - --sample, --no-progress, --host
          - Pre-flight Ollama health check
          - Progress display with [n/total] format
          - Resume support (skip existing .npy files)
          - Namespace directory structure preservation
          All 10 CLI tests pass. Pre-commit hooks pass.

      - id: 5.4
        name: Integration Tests + Documentation
        status: COMPLETE
        completed_date: 2025-12-15
        tests: 4 passing (slow)
        details: |
          - Created tests/slow/test_embedding_slow.py (4 tests)
          - Added require_ollama_server fixture to conftest.py
          - Tests: embed_real_chunk, dimensions, normalized, deterministic
          - Updated ai-docs/ with final status

      - id: 5.5
        name: Dual-Provider Architecture + CLI Enhancements
        status: COMPLETE
        completed_date: 2025-12-16
        details: |
          Enhanced embedder with OpenAI as primary provider:
          - Added --provider flag (openai/ollama)
          - OpenAI text-embedding-3-large (1536-dim) as default
          - Ollama embeddinggemma (768-dim) as fallback
          - Updated mise tasks for both providers
          - Sample pipeline uses OpenAI for production quality

    nice_to_have:
      - Investigate parallel embedding by directory (3 instances if GPU permits)
      - Async embedding for higher throughput
      - OpenAI dimension reduction (1536 â†’ 768) for ChromaDB compatibility

    implementation_notes: |
      Phase 5 COMPLETE - dual-provider embedding module:
      - embedder.py supports OpenAI (primary) and Ollama (fallback)
      - CLI provides --provider flag for easy switching
      - OpenAI produces higher quality 1536-dim embeddings
      - Ollama provides local/free fallback option
      - All 35 tests pass, pre-commit hooks pass
      - Sample pipeline verified with 152 OpenAI embeddings

  phase_6_loading:
    status: future
    description: ChromaDB ingestion
    schema_reference: chromadb-schema.yaml#schema_phases
    note: |
      Schema is PHASED - MVP uses Phase A (13 fields from chunker).
      No chunker changes needed for MVP. See chromadb-schema.yaml for phases:
        Phase A (MVP): Core chunk + article metadata (13 fields) - ready now
        Phase B: Rich infobox/library_work metadata (14 fields) - needs chunker extension
        Phase C: Reference metadata (7 fields) - needs extraction enhancement
        Phase D: Graph metadata (2 fields) - DEFERRED
    tasks:
      - Finalize Phase A schema validation
      - Implement chroma.py loader
      - Batch upsert with metadata
      - Add loading tests
      - (Later) Extend chunker for Phase B fields

  phase_7_mcp:
    status: future
    description: MCP server implementation
    tasks:
      - Implement search tool
      - Implement get_article tool
      - Implement list_categories tool
      - Add MCP protocol tests

  phase_8_finetune:
    status: IN_PROGRESS
    description: GRPO fine-tuning with multi-layer anti-hacking reward system
    reference_docs:
      - ai-docs/reward-modeling.yaml  # Reward function design and anti-hacking measures
      - ai-docs/finetune.yaml  # Overall fine-tuning methodology
    dependencies: Curated Q&A dataset (training_data/curated_qa.jsonl)
    updated: 2025-12-17

    overview:
      goal: Fine-tune DeepSeek-R1-0528-Qwen3-8B for Marxist-Leninist theory responses
      method: GRPO (Group Relative Policy Optimization) via Unsloth + TRL
      dataset: 128 curated Q&A pairs (training_data/grpo_dataset.jsonl)
      output: LoRA adapter â†’ GGUF for Ollama deployment
      hardware: A40 48GB (RunPod) or similar

    methodology_rationale: |
      GRPO chosen over SFT because:
      - Political theory has no single "correct" answer (unlike math)
      - Open-ended prose requires semantic similarity, not exact match
      - Reward functions can encode domain expertise
      - Multi-layer rewards defeat adversarial "word soup" attacks

    tasks:
      - id: 8.1
        name: Dataset Preparation
        status: COMPLETE
        completed_date: 2025-12-17
        details: |
          - Curated 128 Q&A pairs (training_data/curated_qa.jsonl)
          - Transformed to GRPO format (training_data/grpo_dataset.jsonl)
          - Format: {"prompt": [...messages...], "answer": "..."}
          - Topics: revisionism, surplus value, imperialism, dialectics, etc.
        files:
          - training_data/curated_qa.jsonl
          - training_data/grpo_dataset.jsonl

      - id: 8.2
        name: Reward Function System
        status: COMPLETE
        completed_date: 2025-12-17
        tests: 60 passing
        details: |
          Multi-layer reward system defeating word soup attacks:
          1. Format rewards (exact/approximate </think> pattern)
          2. Semantic similarity (sentence-transformers embeddings)
          3. Terminology (Marxist lexicon bonus)
          4. NLI coherence (BART-large-MNLI entailment check)
          5. Self-consistency (no internal contradictions)
          6. Structural coherence (terms in syntactic roles)
          7. Topic relevance (questionâ†’answer concept coverage)
          8. Interconnection depth (anti-buzzword-salad)
          9. Completeness (key concept coverage)

          Anti-hacking measures:
          - HOLLOW_BUZZWORDS penalty (activist jargon without substance)
          - DEPTH_MARKERS bonus (historical specificity)
          - EXPLANATORY_PHRASES detection (causal reasoning)
          - Depth ratio scoring (words per concept)
        files:
          - src/pw_mcp/ai_training/grpo_rewards.py
          - tests/unit/training/test_grpo_rewards.py

      - id: 8.3
        name: W&B Logging Integration
        status: COMPLETE
        completed_date: 2025-12-17
        tests: 17 passing
        details: |
          Weights & Biases integration for training observability:
          - WandbSampleLogger for periodic sample tables
          - Per-reward metric logging (mean, min, max)
          - create_logging_reward() factory for GRPOTrainer
          - Graceful degradation without wandb installed
        files:
          - src/pw_mcp/ai_training/wandb_logging.py
          - tests/unit/training/test_wandb_logging.py

      - id: 8.4
        name: Training Notebook
        status: COMPLETE
        completed_date: 2025-12-17
        details: |
          Self-contained Jupyter notebook for RunPod execution:
          - Model loading (DeepSeek-R1-0528-Qwen3-8B via Unsloth)
          - All reward functions inline (no external imports)
          - W&B logging integration
          - A40-optimized configuration
          - LoRA training and saving
          - GGUF export instructions
        files:
          - training_data/Marxist_GRPO_Training.ipynb

      - id: 8.5
        name: Training Execution
        status: PLANNED
        details: |
          Remaining work:
          - Execute notebook on RunPod A40
          - Monitor W&B metrics during training
          - Evaluate model outputs
          - Export to GGUF and test with Ollama

      - id: 8.6
        name: Model Evaluation
        status: PLANNED
        details: |
          - Manual evaluation on test questions
          - Check for reward hacking (word soup detection)
          - Verify Marxist-Leninist accuracy
          - Iterate on reward weights if needed

    model_info:
      base_model: DeepSeek-R1-0528-Qwen3-8B
      huggingface: unsloth/DeepSeek-R1-0528-Qwen3-8B
      params: 8B
      context_length: 2048 (training max_completion_length)
      lora_rank: 64
      lora_alpha: 64

    training_config:
      max_steps: 250
      batch_size: 2
      gradient_accumulation: 2
      learning_rate: 5e-6
      num_generations: 4
      temperature: 1.0
      gpu_memory_utilization: 0.85

    module_location: src/pw_mcp/ai_training/
    module_files:
      grpo_rewards.py: "13+ reward functions with anti-hacking measures"
      wandb_logging.py: "W&B integration for training observability"
      __init__.py: "Public API exports"

# =============================================================================
# CURRENT FOCUS
# =============================================================================

current_focus:
  phase: "6 (ChromaDB Loading) + 8 (GRPO Fine-tuning)"
  completed: [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 4.1, 4.2, 4.3, 4.4, 5.1, 5.2, 5.3, 5.4, 5.5, 8.1, 8.2, 8.3, 8.4]
  removed: [3.1, 3.2, 3.3, 3.4]  # sembr phases removed 2025-12-16
  next_tasks:
    - 6.1 - Phase B Implementation - COMPLETE (2025-12-25)
    - 6.2 - Re-run pipeline with Phase B metadata (re-extract, re-chunk, re-load)
    - 6.3 - Phase A Schema Implementation (ChromaDB loader)
    - 8.5 - Training Execution (RunPod A40)
  references:
    - ai-docs/phase-b-implementation.yaml  # Phase B COMPLETE
    - ai-docs/chromadb-schema.yaml
    - ai-docs/reward-modeling.yaml

  today_summary: |
    2025-12-25 Summary (PHASE B METADATA IMPLEMENTATION COMPLETE):
    - PHASE B COMPLETE: Implemented 7 enriched metadata fields for ChromaDB
      * Fixed CLI namespace bug - passes relative paths to extract_article()
      * Added _extract_year() helper for parsing publication years
      * Extended jsonl_writer.py with 4 new fields:
        - library_work_type, library_work_published_year
        - primary_category, category_count
      * Extended chroma.py serialize/deserialize with all 7 Phase B fields
      * ChromaDB-safe value handling (Noneâ†’"", Noneâ†’-1)
    - Tests: 52 passing in affected modules (chroma + chunk_metadata)
    - New test classes: TestExtractYear (5 tests), Phase B serialization tests
    - All 422 tests passing, all lint/type checks passing
    - Files modified:
      * src/pw_mcp/ingest/cli.py (namespace bug fix)
      * src/pw_mcp/ingest/chunker/jsonl_writer.py (Phase B fields + year extraction)
      * src/pw_mcp/db/chroma.py (serialize/deserialize Phase B fields)
      * tests/unit/chunking/test_chunk_metadata.py (extended Phase B tests)
      * tests/unit/db/test_chroma.py (Phase B roundtrip tests)

    Previous (2025-12-24 - MIKADO REFACTORING COMPLETE):
    - Decomposed 1,278-line chunker.py into 9 focused modules
    - All 133 tests passing, full backward compatibility maintained

    2025-12-20 Summary (PHASE B PLANNING SESSION):
    - CRITICAL DISCOVERY: parse_library_work() exists but is NEVER CALLED
      in extraction pipeline. This is incomplete Phase A work.
    - Documented four-layer fix needed:
      1. types.py: Add library_work field to ArticleData
      2. extraction.py: Call parse_library_work()
      3. CLI: Serialize infobox + library_work to metadata JSON
      4. chunker.py: Add Phase B fields to JSONL output
    - Created ai-docs/phase-b-implementation.yaml with full gap analysis
    - Confirmed 539 tests across 17 files (baseline verified)
    - Key insight: No re-embedding needed! Only re-extract + re-chunk + re-load
    - Methodology: Refactoring with targeted tests (not TDD)
    - Phase B fields: library_work_author, infobox_type, political_orientation

    2025-12-19 Summary (PERSONA DRIFT PREVENTION + SYNTHETIC TRAINING DATA):
    - Identified critical failure mode: model switches to saccharine emoji-soup
      chatbot mode when receiving casual/benign inputs ("mama mia!" â†’ "Aww teehee ðŸ’™")
    - Created synthetic_benign_input_handling.jsonl (95 training pairs) covering:
      * Greetings (hi, hey, yo, sup, hello)
      * Emotional exclamations (wow, lol, mama mia)
      * Off-topic requests (weather, jokes, songs)
      * Personal questions, farewells, nonsense inputs
    - Designed 4 new reward functions (pending implementation via TDD):
      * emoji_penalty_reward - penalize emoji usage
      * saccharine_language_reward - detect chatbot patterns
      * register_consistency_reward - maintain professional register
      * scope_maintenance_reward - redirect off-topic inputs professionally
    - Updated MODEL_CARD.yaml with comprehensive synthetic_datasets section
    - Documented failure mode and mitigation strategy

    Previous (2025-12-18) - ANTI-HALLUCINATION + IDEOLOGICAL FIRMNESS:
    - Implemented anti-hallucination system with entity whitelist (24,040 entities)
    - Added ideological_firmness_reward (combat reactionary claims with ML analysis)
    - Added entity_verification_reward (NER + whitelist anti-hallucination)
    - Test audit: added 25 tests for 3 previously untested reward functions
    - 79 GRPO reward tests passing (was 54), 432 total tests

  next_action: |
    Phase 8 (GRPO Training) - Iteration 3 Preparation:
    - New synthetic data: synthetic_benign_input_handling.jsonl (95 pairs)
    - 4 new reward functions pending TDD implementation:
      * emoji_penalty_reward (tests ready)
      * saccharine_language_reward (tests ready)
      * register_consistency_reward (tests ready)
      * scope_maintenance_reward (tests ready)
    - After reward implementation: Execute iteration 3 training on RunPod A40

    Phase 6 (ChromaDB Loading) - Still Pending:
    - Implement Phase A schema (13 fields from chunker output)
    - Create ChromaDB loader module
    - Batch upsert with metadata

    Key Implementations (Phase 8):
    - src/pw_mcp/ai_training/grpo_rewards.py: 17+ reward functions
    - src/pw_mcp/ai_training/wandb_logging.py: W&B integration
    - training_data/synthetic_benign_input_handling.jsonl: Persona consistency training

  # How Implementation Phases map to Pipeline Stages
  # NOTE: Phase 3 (sembr) was REMOVED - pipeline is now 4 stages
  phase_to_stage_mapping: |
    Implementation Phases (work order)     Pipeline Stages (data flow)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Phase 1: Foundation (infrastructure)   (no stage - test setup)
    Phase 2: Extraction (parsers)    â”€â”€â”€â”€â–º Stage 1: Extraction
    Phase 3: Sembr                         REMOVED (2025-12-16)
    Phase 4: Chunking (tiktoken)     â”€â”€â”€â”€â–º Stage 2: Chunking
    Phase 5: Embedding               â”€â”€â”€â”€â–º Stage 3: Embedding
    Phase 6: Loading                 â”€â”€â”€â”€â–º Stage 4: ChromaDB Loading
    Phase 7: MCP Server                    (consumer of database)
    Phase 8: Fine-tuning             â”€â”€â”€â”€â–º IN_PROGRESS (GRPO rewards complete)
    (Optional) Graph                       (deferred enhancement)

  test_status:
    # Extraction tests
    link_parser: 28 passing
    citation_parser: 48 passing
    infobox_parser: 25 passing
    library_work_parser: 25 passing
    quote_parser: 12 passing
    extraction_pipeline: 22 passing
    # Chunking tests
    chunker_tiktoken: 67 passing
    oversized_line_handling: 7 passing
    cli_chunk: 18 passing
    # Embedding tests
    embedder: 21 passing
    openai_embedder: 10 passing
    embed_cli: 10 passing
    embed_slow: 4 passing
    # Training tests
    grpo_rewards: 79 passing  # Multi-layer + anti-hallucination + ideological firmness (25 new)
    wandb_logging: 17 passing  # W&B integration
    total: 432 passing  # Updated 2025-12-18

# =============================================================================
# KEY DECISIONS
# =============================================================================

decisions:
  tdd_approach:
    decision: Use pytest.skip() for Red Phase tests
    rationale: Tests pass as "skipped" rather than failing with ImportError
    date: 2024-12-14

  fixture_source:
    decision: Use prolewiki-exports/ for fixture extraction
    rationale: Real corpus data ensures realistic test coverage
    date: 2024-12-14

  parser_order:
    decision: Implement link -> citation -> infobox -> library_work -> quote
    rationale: Increasing complexity, foundational parsers first
    date: 2024-12-14

  type_stubs:
    decision: Create proper stub files for mcp library instead of type-ignore comments
    rationale: Proper stubs provide real type safety without suppressing errors
    date: 2025-12-14
    files:
      - stubs/mcp/__init__.pyi
      - stubs/mcp/server/__init__.pyi
      - stubs/mcp/server/fastmcp.pyi

  mwparserfromhell:
    decision: Use mwparserfromhell for template parsing, not regex
    rationale: |
      Handles nested templates, multiline, edge cases that regex cannot.
      Already a project dependency.
    date: 2025-12-14

  generic_template_removal:
    decision: Use mwparserfromhell to remove ALL templates from clean_text
    rationale: |
      Corpus has 948 unique template types (20,069 instances).
      Specific parsers extract metadata, then all templates removed.
      Ensures clean text for chunking and embeddings.
    date: 2025-12-14

  lock_file_policy:
    decision: Exclude uv.lock from pre-commit large file check
    rationale: Lock files are auto-generated, necessary for reproducibility
    date: 2025-12-14

  schema_phasing:
    decision: Phase ChromaDB schema into A (MVP) / B / C / D increments
    rationale: |
      Schema has 60+ fields but chunker only outputs 13.
      Phase A (13 fields): Already output by chunker - ship MVP now.
      Phase B (14 fields): Infobox/library_work - data extracted, needs serialization.
      Phase C (7 fields): Reference metadata - needs extraction enhancement.
      Phase D (2 fields): Graph metadata - requires optional graph computation.
      This enables MVP delivery without blocking on full schema implementation.
    date: 2025-12-15
    reference: chromadb-schema.yaml#schema_phases

  sembr_server_management:
    status: REMOVED (2025-12-16)
    decision: Auto-restart sembr server by default with CUDA cleanup
    rationale: |
      HISTORICAL NOTE: This decision was superseded by the tiktoken refactor.
      Sembr was completely removed from the pipeline on 2025-12-16.
      See sembr_replacement_with_tiktoken decision for details.
    date: 2025-12-15
    removed_date: 2025-12-16

  graceful_batch_error_handling:
    decision: Return None for failed files instead of failing entire batch
    rationale: |
      process_batch() was using asyncio.gather() which propagates exceptions.
      When one file fails, the entire batch would crash.
      Fix: Wrap individual file processing in try/except, return None for
      failures. Callers can filter [r for r in results if r is not None].
      This enables processing large batches with partial failures.
    date: 2025-12-15
    implementation: src/pw_mcp/ingest/linebreaker.py

  flat_directory_mode:
    decision: Add --flat flag to extract command with filename-based namespace inference
    rationale: |
      The extract command expected namespace subdirectories (Main/, Library/, Essays/).
      For sample-pipeline/ and similar flat workflows, needed alternative approach.
      Solution: _infer_namespace_from_filename() maps prefixes to namespaces:
        - Essay_* â†’ Essays
        - Library_* â†’ Library
        - ProleWiki_* â†’ ProleWiki
        - (no prefix) â†’ Main
      Only extract stage needed changes; other stages use rglob() and work with any structure.
    date: 2025-12-16
    implementation: src/pw_mcp/ingest/cli.py

  openai_primary_embedding:
    decision: Use OpenAI text-embedding-3-large as primary, Ollama as fallback
    rationale: |
      OpenAI embeddings (1536-dim) provide superior quality for production use.
      Ollama (768-dim) remains available for local/private/free development.
      CLI --provider flag enables easy switching.
    date: 2025-12-16
    implementation: src/pw_mcp/ingest/embedder.py

  sample_pipeline_committed:
    decision: Commit all sample pipeline outputs to repository
    rationale: |
      ProleWiki granted permission to include 10 public domain/original works.
      Full pipeline outputs committed (extracted, sembr, chunks, embeddings)
      to serve as reference examples and demonstrate the RAG workflow.
      Enables users to understand pipeline without running it.
    date: 2025-12-16
    files:
      - sample-pipeline/source/ (10 input files)
      - sample-pipeline/extracted/ (10 JSON files)
      - sample-pipeline/sembr/ (10 text files)
      - sample-pipeline/chunks/ (10 JSONL files, 152 total chunks)
      - sample-pipeline/embeddings/ (10 NPY files, 152 vectors @ 1536-dim)

  sembr_replacement_with_tiktoken:
    decision: Replace sembr ML pipeline with tiktoken-based chunking
    rationale: |
      The sembr approach required GPU server management, CUDA crash recovery,
      HTTP client infrastructure, and ~6000 lines of complex code. It was
      causing CUDA device-side assertion errors and was slow (~9s/file).

      The tiktoken chunker:
      - No server needed (runs in-process)
      - No GPU needed (pure CPU)
      - Deterministic output
      - <1ms per file processing
      - Zero crash risk
      - ~600 lines of simple code

      This simplified the pipeline from 5 stages to 4 stages and eliminated
      all GPU-related stability issues.
    date: 2025-12-16
    files_deleted:
      - src/pw_mcp/ingest/linebreaker.py
      - src/pw_mcp/ingest/server_manager.py
      - src/pw_mcp/ingest/gpu_manager.py
      - tests/unit/sembr/*
      - tests/unit/ingest/test_server_manager.py
      - tests/unit/ingest/test_gpu_manager.py
      - tests/fixtures/sembr/
      - ai-docs/sembr.yaml
    implementation: src/pw_mcp/ingest/chunker.py (tiktoken-based)

# =============================================================================
# FILE LOCATIONS
# =============================================================================

key_files:
  source_code:
    parsers:
      - src/pw_mcp/ingest/parsers/__init__.py   # Public API exports
      - src/pw_mcp/ingest/parsers/types.py      # Link, Citation, InfoboxData, LibraryWorkData, QuoteData, ArticleData
      - src/pw_mcp/ingest/parsers/link.py       # parse_links() - 28 tests
      - src/pw_mcp/ingest/parsers/citation.py   # parse_citations() - 48 tests
      - src/pw_mcp/ingest/parsers/infobox.py    # parse_infobox() - 25 tests
      - src/pw_mcp/ingest/parsers/library_work.py  # parse_library_work() - 25 tests
      - src/pw_mcp/ingest/parsers/quote.py      # parse_quotes() - 12 tests
    extraction:
      - src/pw_mcp/ingest/extraction.py         # extract_article() - 22 tests
    sembr:
      - src/pw_mcp/ingest/linebreaker.py        # sembr HTTP client - 33 tests
    chunking:
      - src/pw_mcp/ingest/chunker.py            # chunk_text(), chunk_article() - 43 tests
    embedding:
      - src/pw_mcp/ingest/embedder.py           # embed_texts(), embed_article_chunks() - 31 tests
    server:
      - src/pw_mcp/server.py
      - src/pw_mcp/config.py
      - src/pw_mcp/ingest/cli.py                # pw-ingest CLI with sembr, chunk, embed subcommands

  type_stubs:
    - stubs/mcp/__init__.pyi
    - stubs/mcp/server/__init__.pyi
    - stubs/mcp/server/fastmcp.pyi

  tests:
    unit:
      - tests/unit/extraction/test_link_parser.py          # 28 tests
      - tests/unit/extraction/test_citation_parser.py      # 48 tests
      - tests/unit/extraction/test_infobox_parser.py       # 25 tests
      - tests/unit/extraction/test_library_work_parser.py  # 25 tests
      - tests/unit/extraction/test_quote_parser.py         # 12 tests
      - tests/unit/sembr/test_linebreaker.py               # 33 tests
      - tests/unit/chunking/test_chunker.py                # 25 tests
      - tests/unit/chunking/test_chunk_metadata.py         # 18 tests
      - tests/unit/cli/test_chunk_cli.py                   # 18 tests
      - tests/unit/embedding/test_embedder.py              # 21 tests
      - tests/unit/embedding/test_embed_cli.py             # 10 tests
    slow:
      - tests/slow/test_sembr_slow.py                      # 9 tests (sembr server)
      - tests/slow/test_embedding_slow.py                  # 4 tests (Ollama)
    integration:
      - tests/integration/test_extraction_pipeline.py      # 22 tests
    config:
      - tests/conftest.py

  fixtures:
    - tests/fixtures/mediawiki/infoboxes/     # 7 infobox type samples
    - tests/fixtures/mediawiki/citations/     # 7 citation type samples
    - tests/fixtures/mediawiki/links/         # 3 link type samples
    - tests/fixtures/mediawiki/library_work/  # 7 library work samples
    - tests/fixtures/mediawiki/quotes/        # 3 quote samples
    - tests/fixtures/mediawiki/edge_cases/    # unicode, nested, multiline
    - tests/fixtures/sembr/input/             # 7 sembr input samples
    - tests/fixtures/sembr/expected/          # 5 sembr expected outputs
    - tests/fixtures/sembr/mock_responses/    # 3 HTTP mock responses
    - tests/fixtures/chunking/input/          # 8 chunking input samples
    - tests/fixtures/chunking/metadata/       # 3 article metadata JSONs
    - tests/fixtures/embedding/               # 5-chunk sample JSONL fixture

  corpus:
    - prolewiki-exports/Main/       # ~4000+ encyclopedia articles
    - prolewiki-exports/Library/    # Full books (Marx, Lenin, etc.)
    - prolewiki-exports/Essays/     # User-contributed essays
    - prolewiki-exports/ProleWiki/  # Meta/admin pages

  sample_pipeline:
    description: "10 public domain works demonstrating full RAG workflow"
    source: sample-pipeline/source/         # 10 input files
    extracted: sample-pipeline/extracted/   # 10 JSON metadata files
    sembr: sample-pipeline/sembr/           # 10 sembr'd text files
    chunks: sample-pipeline/chunks/         # 10 JSONL files (152 chunks)
    embeddings: sample-pipeline/embeddings/  # 10 NPY files (152 Ã— 1536 vectors)

  documentation:
    - ai-docs/index.yaml
    - ai-docs/chromadb.yaml
    - ai-docs/chromadb-schema.yaml
    - ai-docs/pipeline.yaml
    - ai-docs/testing-plan.yaml
    - ai-docs/project-status.yaml  # This file

# =============================================================================
# DATACLASS REFERENCE
# =============================================================================

dataclasses:
  Link:
    fields: [target, display, link_type, section, start, end]
    link_types: [internal, category, external, interwiki]

  Citation:
    fields: |
      type, title, author, year, url, lg_url, pdf_url, publisher,
      newspaper, chapter, page, isbn, date, retrieved, channel,
      quote, ref_name, link, published_location, translation_title,
      translation_language
    types: [book, web, news, youtube, video, library]

  InfoboxData:
    fields: [type, fields, remaining_text, extracted_links]
    types: |
      politician, country, political_party, person, revolutionary,
      essay, philosopher, company, settlement, military_person,
      organization, guerilla_organization, youtuber, military_conflict,
      book, religion, website, transcript, flag

  LibraryWorkData:
    fields: |
      title, author, authors, year, work_type, source_url,
      published_location, translator, original_language,
      publisher, audiobook_url, remaining_text

  QuoteData:
    fields: [text, attribution]

  ArticleData:
    fields: |
      source_path, namespace, title, clean_text, sections,
      categories, links, citations, infobox, library_work,
      quotes, is_stub, citation_needed_count, has_blockquote,
      reference_count

  SembrConfig:
    fields: |
      server_url, model_name, timeout_seconds, max_retries,
      retry_delay_seconds, batch_size, predict_func
    frozen: true

  SembrResult:
    fields: |
      text, line_count, processing_time_ms,
      input_word_count, output_word_count

  ChunkConfig:
    fields: |
      target_tokens, min_tokens, max_tokens,
      token_estimation_factor
    frozen: true

  Chunk:
    fields: |
      text, chunk_index, section, line_start,
      line_end, word_count, estimated_tokens

  ChunkedArticle:
    fields: |
      article_title, namespace, chunks, categories,
      internal_links, infobox, library_work, is_stub,
      citation_needed_count, has_blockquote

  EmbedConfig:
    fields: |
      model, dimensions, batch_size, ollama_host,
      max_retries, retry_delay
    frozen: true
    validated: true

  EmbeddedArticle:
    fields: |
      article_title, namespace, num_chunks, embeddings
    embeddings_shape: (num_chunks, 768)
    embeddings_dtype: float32

# =============================================================================
# COMMANDS
# =============================================================================

useful_commands:
  testing:
    all_tests: mise run test
    unit_only: mise run test-fast
    integration: mise run test-integration
    with_coverage: mise run test-cov
    specific_file: uv run pytest tests/unit/extraction/test_citation_parser.py -v

  code_quality:
    lint: uv run ruff check src/ tests/
    format: uv run ruff format src/ tests/
    typecheck: uv run mypy src/pw_mcp/
    all_checks: mise run check
    pre_commit: mise run pre-commit

  development:
    install: mise run install
    chunk_help: uv run pw-ingest chunk --help
    chunk_sample: uv run pw-ingest chunk --sample 10
    chunk_process: uv run pw-ingest chunk -i extracted/ -o chunks/
    embed_help: uv run pw-ingest embed --help
    embed_sample_openai: uv run pw-ingest embed --sample 10 --provider openai
    embed_sample_ollama: uv run pw-ingest embed --sample 10 --provider ollama
    embed_process: uv run pw-ingest embed -i chunks/ -o embeddings/ --provider openai

  sample_pipeline:
    status: mise run sample-status
    full_pipeline: mise run sample-pipeline
    extract: mise run sample-extract
    chunk: mise run sample-chunk
    embed_openai: mise run sample-embed
    embed_ollama: mise run sample-embed-ollama
    clean: mise run sample-clean
