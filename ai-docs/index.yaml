# AI Documentation Index
# Token-efficient reference files for AI assistants
# Format: YAML for structured lookup, minimal prose

files:
  chromadb.yaml:
    purpose: ChromaDB vector database schema, operations, metadata patterns
    topics:
      - persistence and client setup
      - collection configuration (HNSW, distance metrics)
      - metadata schema and filtering operators
      - batch operations and performance
      - interlinking via metadata (wiki-style relationships)
    use_when:
      - designing database schema
      - implementing ingestion pipeline
      - writing query/search logic
      - optimizing performance

  chromadb-schema.yaml:
    status: Phase A (MVP) IMPLEMENTED - using OpenAI embeddings
    purpose: Concrete ChromaDB schema definition for ProleWiki (phased rollout)
    topics:
      - SCHEMA PHASES (A/B/C/D incremental implementation)
      - MVP SCHEMA (Phase A - 13 fields, ready now)
      - collection definitions (prolewiki_chunks, prolewiki_references)
      - field schemas with types and examples
      - HNSW and embedding configuration
      - ID format patterns
      - metadata constraints (JSON arrays, indexing)
      - query patterns with code examples
      - INFOBOX METADATA (Phase B - biographical, political, organizational fields)
      - LIBRARY WORK METADATA (Phase B - bibliographic fields for Library namespace)
      - REFERENCE METADATA (Phase C - citation-derived fields)
      - GRAPH METADATA (Phase D - backlinks, importance metrics)
      - ARTICLE STATUS FLAGS (Phase A - is_stub, citation_needed_count)
      - CONTENT FLAGS (Phase A/B - has_blockquote, has_foreword)
    sections:
      schema_phases: Phase A/B/C/D definitions with field lists
      mvp_schema: Quick reference for Phase A (13 fields)
      collections: Full field reference tagged by phase
      constraints: ChromaDB metadata limitations
      query_patterns: Common query code examples
      migration: Schema change strategies
      open_questions: Design decisions to resolve
    schema_phases:
      phase_a: MVP - 13 fields already output by chunker
      phase_b: Rich metadata - infobox + library_work (14 fields)
      phase_c: Reference metadata - citations (7 fields)
      phase_d: Graph metadata - backlinks (2 fields) - DEFERRED
    use_when:
      - implementing ChromaDB loading (start with Phase A)
      - writing search/query code
      - reviewing what metadata is available by phase
      - planning schema extensions beyond MVP
      - debugging query results
      - understanding infobox field mappings
      - understanding Library namespace metadata
      - filtering by article quality

  pipeline.yaml:
    status: Stages 1-4 IMPLEMENTED, Stage 5 NEXT
    purpose: Full ingestion pipeline architecture from MediaWiki to ChromaDB
    topics:
      - 4-stage core pipeline (extract → chunk → embed → load)
      - tiktoken-based chunking (replaced sembr ML pipeline)
      - optional graph computation (backlinks, link counts)
      - intermediate file formats for debuggability
      - CLI interface design
      - INFOBOX EXTRACTION (21 types, field mapping, date parsing, link extraction)
      - ADDITIONAL TEMPLATES (Library work, Book_Navigation, citations, status markers)
    sections:
      stage_1_extraction: MediaWiki parsing + clean_text generation ✓
      stage_1_extraction.infoboxes: |
        Infobox type detection and field extraction
        - 21 infobox types (politician, country, party, etc.)
        - Field mapping (extract vs discard)
        - Date template parsing
        - Link extraction from values
        - Removal from clean_text before chunking
      stage_1_extraction.additional_templates: |
        All other MediaWiki templates beyond infoboxes:
        - {{Library work}} - bibliographic metadata for Library namespace
        - {{Foreword}} - editorial forewords (keep in text)
        - {{Book_Navigation}}, {{Book_TOC}} - chapter navigation
        - {{Main article}} - links to main articles (valuable for graph)
        - {{Stub}}, {{Citation needed}} - article status markers
        - Citation variants (Web, News, Library, Video, YouTube)
        - Display control (DEFAULTSORT, DISPLAYTITLE, TOC limit)
        - Sidebars and navboxes (remove, no extraction)
        - Transclusion markers (noinclude, onlyinclude)
      stage_2_chunking: Tiktoken-based chunking with metadata ✓
      stage_3_embedding: OpenAI text-embedding-3-large (1536-dim) ✓
      stage_4_loading: ChromaDB ingestion
      optional_graph: Backlinks, link counts, category graph (deferred)
      open_questions: Unresolved design decisions
    use_when:
      - planning pipeline implementation
      - understanding data flow
      - debugging intermediate outputs
      - understanding infobox extraction rules
      - understanding graph computation (optional)

  testing-plan.yaml:
    status: DRAFT - ready for implementation
    purpose: Comprehensive testing strategy for RAG database
    topics:
      - test pyramid (unit, integration, E2E)
      - property-based testing with Hypothesis
      - retrieval quality metrics (P@K, MRR, NDCG)
      - performance benchmarks
      - MCP server tests
      - CI/CD configuration
    sections:
      infrastructure: pytest setup, markers, directory structure
      stage_1_extraction_tests: 21 infobox types, citations, links, templates
      stage_2_chunking_tests: token boundaries, metadata attachment
      stage_3_embedding_tests: OpenAI/Ollama provider tests, quality checks
      stage_4_loading_tests: ChromaDB operations, integrity
      integration_tests: cross-stage data flow validation
      retrieval_tests: semantic search quality, filter tests, golden test set
      property_tests: Hypothesis-based parser robustness
      benchmarks: extraction, embedding, query performance
      mcp_tests: tool tests, protocol compliance, concurrency
      implementation_phases: 4-phase rollout plan
    use_when:
      - setting up test infrastructure
      - writing new tests
      - understanding test coverage requirements
      - configuring CI/CD

  project-status.yaml:
    status: ACTIVE - primary tracking document
    purpose: Track implementation progress and next steps
    topics:
      - implementation phases (8 phases from foundation to fine-tuning)
      - current focus and next actions
      - key decisions made
      - file locations reference
      - useful commands
      - dataclass reference (Link, Citation, InfoboxData, etc.)
    sections:
      phases: All 8 implementation phases with status
      phase_1_foundation: Test infrastructure (COMPLETE)
      phase_2_parsers: TDD Green Phase - parser implementation (COMPLETE)
      phase_3_sembr: REMOVED (replaced by tiktoken chunking)
      phase_4_chunking: Tiktoken-based chunking (COMPLETE)
      phase_5_embedding: OpenAI/Ollama embeddings (COMPLETE)
      phase_6_loading: ChromaDB ingestion (PENDING)
      phase_7_mcp: MCP server implementation (PENDING)
      phase_8_finetune: GRPO training with multi-layer rewards (IN_PROGRESS)
      current_focus: What to work on next
      decisions: Key architectural/process decisions
      key_files: Where everything lives (source_code, type_stubs, tests, fixtures)
      dataclasses: Quick reference for all data types
      useful_commands: Common development commands
    use_when:
      - starting a new session
      - understanding project state
      - deciding what to work on next
      - finding file locations
      - running common commands
    recent_updates:
      - "2025-12-18: Test audit + 25 new tests (79 total GRPO reward tests)"
      - "2025-12-18: Anti-hallucination system (entity whitelist + 3 new reward functions)"
      - "2025-12-18: ideological_firmness_reward - combat reactionary claims with ML analysis"
      - "2025-12-18: entity_verification_reward - NER + 24,040 entity whitelist"
      - "2025-12-18: epistemic_calibration_reward - pattern-based uncertainty detection"
      - "2025-12-18: Synthetic training data (epistemic humility, creator/contributor context)"
      - "2025-12-17: PHASE 8 IN_PROGRESS - GRPO reward system (13+ functions)"
      - "2025-12-17: Added interconnection_depth_reward (anti-buzzword-salad)"
      - "2025-12-17: Added W&B logging integration (wandb_logging.py)"
      - "2025-12-16: MAJOR REFACTOR - Replaced sembr with tiktoken-based chunking"
      - "2025-12-15: PHASE 5 COMPLETE - embedding module fully implemented"
      - "2025-12-14: PHASE 2 COMPLETE - all parsers implemented"

  embedding.yaml:
    status: IMPLEMENTED - Phase 5 COMPLETE (OpenAI provider added)
    purpose: Vector embedding generation for semantic search
    topics:
      - OpenAI text-embedding-3-large (1536-dim, primary)
      - Ollama embeddinggemma (768-dim, local fallback)
      - EmbedConfig with provider selection
      - NPY output format (aligned with JSONL chunk order)
      - CLI integration (pw-ingest embed --provider openai)
      - Testing strategy (~35 tests: unit, CLI, integration)
    sections:
      overview: Goal, input/output, model selection rationale
      model: OpenAI (primary) and Ollama (fallback) specs
      config: EmbedConfig dataclass with provider field
      dataclasses: EmbeddedArticle structure
      public_api: embed_texts, embed_article_chunks, write_embeddings_npy
      openai_integration: API key from .env, retry logic
      ollama_integration: Python library patterns (local fallback)
      output_format: NPY file structure, alignment with chunks
      cli: pw-ingest embed --provider [ollama|openai]
      testing: Test categories, mock strategy, fixtures
    use_when:
      - running embeddings with OpenAI API
      - understanding provider selection
      - writing embedding tests
      - debugging embedding issues
    model_info:
      primary:
        name: text-embedding-3-large
        provider: OpenAI (API)
        dimensions: 1536 (configurable 256-3072)
        cost: ~$0.13 per million tokens
      fallback:
        name: embeddinggemma
        provider: Ollama (local)
        dimensions: 768

  chatbot-ideology.yaml:
    status: READY FOR IMPLEMENTATION
    purpose: Training set design for general Marxist-Leninist chatbot
    topics:
      - General ML chatbot (not game-specific)
      - ProleWiki Library chunks as data source
      - Question generation from chunk metadata
      - Qwen chat template formatting
      - Transformation code (chunks → Q&A pairs)
      - System prompt options
      - Evaluation criteria
    sections:
      overview: Goal, approach, estimated training size
      data_source: ProleWiki chunks, available works, schema
      system_prompt: Recommended persona and alternatives
      question_generation: Templates, author extraction, priority
      training_format: Qwen template structure
      transformation_code: Complete Python implementation
      quality_considerations: Chunk quality, diversity, improvements
      implementation_steps: 6-step workflow
      evaluation: Test questions, quality criteria, red flags
    use_when:
      - preparing training data for ML chatbot
      - understanding question generation strategy
      - writing the transformation script
      - evaluating fine-tuned model quality

  finetune.yaml:
    status: NEEDS_UPDATE - now using GRPO (not SFT)
    purpose: Marxist-Leninist LLM fine-tuning on ProleWiki corpus
    note: |
      THIS FILE NEEDS MAJOR UPDATE - methodology changed from SFT to GRPO.
      See reward-modeling.yaml and ai_training_module for current approach.
      The training notebook (training_data/Marxist_GRPO_Training.ipynb) is the
      authoritative reference for current implementation.
    current_approach:
      method: GRPO (Group Relative Policy Optimization)
      model: DeepSeek-R1-0528-Qwen3-8B (via Unsloth)
      reward_functions: 13+ multi-layer rewards (see ai_training_module)
      dataset: 128 curated Q&A pairs (training_data/grpo_dataset.jsonl)
      notebook: training_data/Marxist_GRPO_Training.ipynb
    topics:
      - GRPO vs SFT methodology rationale
      - DeepSeek-R1-0528-Qwen3-8B model via Unsloth
      - Multi-layer reward functions (anti-word-soup)
      - Weights & Biases logging integration
      - LoRA configuration (r=64, alpha=64)
      - GGUF export for Ollama deployment
    sections:
      overview: Goal, GRPO method, hardware requirements
      grpo_rationale: Why GRPO over SFT for political theory
      model: DeepSeek-R1-0528-Qwen3-8B, Unsloth integration
      reward_functions: Reference to reward-modeling.yaml
      data_source: Curated Q&A pairs (128 examples)
      training_config: GRPOTrainer configuration
      export: GGUF conversion, Ollama Modelfile
      evaluation: Manual review, reward hacking detection
    use_when:
      - understanding GRPO vs SFT trade-offs
      - planning fine-tuning infrastructure
      - configuring Unsloth for GRPO
      - evaluating fine-tuned model
      - deploying to Ollama

  runpod.yaml:
    status: READY - verified configuration (research complete 2025-12-16)
    purpose: RunPod.io GPU pod setup for Unsloth fine-tuning
    topics:
      - GPU selection (RTX 4090 recommended, A40 alternative)
      - Spot vs On-Demand pricing comparison
      - PyTorch 2.4 template with CUDA 12.4
      - Storage configuration (Volume vs Container disk)
      - Unsloth installation commands
      - Data upload methods (JupyterLab, wget, SCP)
      - GGUF export and download workflow
      - Cost optimization (~$0.30-0.60 for full training run)
    sections:
      overview: Purpose, estimated cost, workflow summary
      gpu_selection: VRAM requirements, pricing, recommendations
      pod_configuration: Template, storage, ports, environment variables
      step_by_step_setup: 8 steps from pod creation to Ollama deployment
      ollama_deployment: Modelfile with Qwen template
      cost_summary: Breakdown by task, optimization tips
      troubleshooting: OOM, slow training, checkpoint recovery
      references: RunPod docs, Unsloth GitHub, related ai-docs
    use_when:
      - setting up RunPod for fine-tuning
      - choosing GPU type and pricing model
      - installing Unsloth on cloud GPU
      - uploading training data to pod
      - exporting and downloading GGUF
      - troubleshooting pod issues
      - estimating training costs

  chunking.yaml:
    status: IMPLEMENTED - tiktoken-based (sembr removed)
    purpose: Text chunking for embedding and retrieval
    topics:
      - Tiktoken-based accurate token counting (not word estimation)
      - Oversized line handling (splits at sentence/word boundaries)
      - Section/paragraph awareness with overlap support
      - Metadata propagation from ArticleData
      - JSONL output format for ChromaDB loading
      - CLI integration (pw-ingest chunk subcommand)
    sections:
      design_rationale: Why chunk, tiktoken vs word estimation
      config: ChunkConfig dataclass fields (includes overlap_tokens)
      dataclasses: Chunk, ChunkedArticle structures
      algorithm: Tiktoken-based with oversized line splitting
      output_format: JSONL schema matching chromadb-schema.yaml
      cli: Subcommand options (reads from extracted/ directly)
      testing: Test structure and categories (74 tests)
      implementation: TDD phases - all complete
      success_criteria: Functionality, quality, performance (all met)
    use_when:
      - understanding chunk algorithm
      - writing chunking tests
      - configuring chunk sizes
      - using pw-ingest chunk CLI

  reward-modeling.yaml:
    status: IMPLEMENTED - multi-layer coherence rewards with W&B logging
    purpose: Robust reward functions for Marxist-Leninist GRPO training
    topics:
      - NLI-based coherence rewards (bart-large-mnli)
      - Self-consistency checking (internal contradictions)
      - Structural coherence via spaCy dependency parsing
      - Interconnection depth (anti-buzzword-salad)
      - Topic relevance (question→answer concept coverage)
      - Defeating reward hacking / word soup attacks
      - Ideological bias considerations in NLI models
      - Weights & Biases logging integration
    sections:
      overview: Goal, problem, solution, research basis
      ideological_bias: Concern, empirical findings, mitigation strategy
      reward_functions: All 13+ reward functions with scoring details
      marxist_terms: Terminology categories for structural checking
      anti_hacking: HOLLOW_BUZZWORDS, DEPTH_MARKERS, EXPLANATORY_PHRASES
      wandb_logging: Sample tables, per-reward metrics, create_logging_reward
      training_usage: Recommended reward sets, GRPOTrainer example
      dependencies: Required packages, model downloads, GPU memory
      testing: Example test cases for validation
      references: Research papers, models, related docs
    use_when:
      - implementing GRPO reward functions
      - understanding NLI-based coherence checking
      - defeating word soup / reward hacking
      - configuring robust training rewards
      - understanding ideological bias in ML models
      - setting up W&B logging for training observability
    research_basis:
      - "arxiv.org/abs/2508.18212 - NLI as reward paradigm"
      - "arxiv.org/abs/2509.22047 - MO-GRPO reward hacking"
      - "arxiv.org/abs/2508.05170 - Posterior-GRPO"

  # ==========================================================================
  # MODULE REFERENCE: src/pw_mcp/ai_training/
  # ==========================================================================

  ai_training_module:
    status: IMPLEMENTED - GRPO rewards + anti-hallucination + W&B logging
    purpose: Python module for GRPO fine-tuning reward functions and logging
    location: src/pw_mcp/ai_training/
    files:
      grpo_rewards.py: |
        17+ reward functions for GRPO training:
        - match_format_exactly/approximately (</think> pattern)
        - semantic_similarity_reward (sentence-transformers)
        - terminology_reward (Marxist lexicon bonus)
        - nli_coherence_reward (BART-large-MNLI entailment)
        - self_consistency_reward (no internal contradictions)
        - structural_coherence_reward (terms in syntactic roles)
        - topic_relevance_reward (question→answer coverage)
        - interconnection_depth_reward (anti-buzzword-salad)
        - completeness_reward (key concept coverage)
        - full_coherence_reward (combined 5-layer)
        - robust_coherence_reward (NLI + self-consistency + structural)
        - entity_verification_reward (NER + whitelist anti-hallucination)
        - epistemic_calibration_reward (pattern-based uncertainty)
        - ideological_firmness_reward (combat reactionary claims)
        - debug_print_reward (development logging)
      wandb_logging.py: |
        Weights & Biases integration:
        - WandbSampleLogger (periodic sample tables)
        - create_logging_reward() (zero-cost logging reward)
        - init_wandb_logging() / finish_wandb_logging()
        - log_reward_metrics() / log_model_checkpoint()
        - Graceful degradation without wandb installed
      __init__.py: "Public API exports for all functions and constants"
    constants:
      - MARXIST_TERMS (100+ terms by category)
      - DISCOURSE_CONNECTIVES (analytical reasoning markers)
      - EXPLANATORY_PHRASES (causal reasoning indicators)
      - HOLLOW_BUZZWORDS (activist jargon penalty)
      - DEPTH_MARKERS (historical specificity bonus)
      - CONCEPT_EQUIVALENCES (term synonym mapping)
      - IDEOLOGICAL_CHALLENGE_PATTERNS (18 reactionary claim patterns)
      - CAPITULATION_PATTERNS (19 hedging/both-sidesing patterns)
      - FIRMNESS_PATTERNS (21 firm Marxist response patterns)
      - CLASS_ANALYSIS_MARKERS (13 substantive class analysis terms)
    test_files:
      - tests/unit/training/test_grpo_rewards.py (79 tests)
      - tests/unit/training/test_wandb_logging.py (17 tests)
    training_data:
      location: training_data/
      files:
        - curated_qa.jsonl (1,058 Q&A pairs from ProleWiki)
        - grpo_dataset.jsonl (GRPO-formatted version)
        - synthetic_epistemic_humility.jsonl (20 anti-hallucination examples)
        - synthetic_creator_context.jsonl (12 examples about Persephone Raskova)
        - synthetic_av_dremel_context.jsonl (8 examples about AV Dremel)
        - entity_whitelist_clean.json (24,040 verified entities from corpus)
        - MANIFEST.yaml (dataset tracking and statistics)
    use_when:
      - implementing or modifying GRPO reward functions
      - adding new anti-hacking measures
      - integrating W&B logging
      - understanding reward function signatures
      - testing reward function behavior
      - adding synthetic training data
    related_docs:
      - ai-docs/reward-modeling.yaml (detailed design)
      - training_data/Marxist_GRPO_Training.ipynb (notebook with inline versions)
      - training_data/MANIFEST.yaml (dataset provenance and statistics)

# Future entries (add as needed):
# mediawiki.yaml: MediaWiki parsing patterns, template extraction
# mcp.yaml: FastMCP server patterns, tool definitions
