# Fine-tuning Documentation - Phase 8
# Purpose: Marxist-Leninist LLM fine-tuning on ProleWiki corpus
# Status: IN_PROGRESS (implementation complete, training execution pending)
# Method: GRPO (Group Relative Policy Optimization) - NOT SFT
# Last Updated: 2025-12-17

# =============================================================================
# METHODOLOGY PIVOT: SFT → GRPO
# =============================================================================
# IMPORTANT: This project pivoted from SFT to GRPO methodology.
# This document has been updated to reflect the current approach.
#
# WHY GRPO OVER SFT:
# - Political theory has no single "correct" answer (unlike math)
# - Open-ended prose requires semantic similarity, not exact string matching
# - Reward functions can encode domain expertise
# - Multi-layer rewards defeat adversarial "word soup" attacks
# - GRPO naturally handles the inherent subjectivity of political analysis
#
# AUTHORITATIVE SOURCES:
# - Training notebook: training_data/Marxist_GRPO_Training.ipynb
# - Reward functions: src/pw_mcp/ai_training/grpo_rewards.py
# - W&B logging: src/pw_mcp/ai_training/wandb_logging.py
# - Reward design: ai-docs/reward-modeling.yaml
# =============================================================================

overview:
  goal: Fine-tune DeepSeek-R1-0528-Qwen3-8B for Marxist-Leninist theory responses
  method: GRPO (Group Relative Policy Optimization) via Unsloth + TRL
  dataset: 128 curated Q&A pairs (training_data/grpo_dataset.jsonl)
  output_format: LoRA adapter → GGUF for Ollama deployment
  hardware_requirement: A40 48GB (RunPod) or similar high-VRAM GPU
  status: Implementation complete, execution pending

# =============================================================================
# GRPO VS SFT RATIONALE
# =============================================================================

grpo_rationale:
  description: |
    GRPO (Group Relative Policy Optimization) is preferred over SFT for
    Marxist-Leninist training because political theory responses have no
    single "correct" answer. Unlike math problems (where GRPO was originally
    popularized with DeepSeek-R1), political theory is inherently open-ended.

  sft_limitations:
    - "Exact answer matching fails for prose responses"
    - "No way to express 'this answer is better than that answer'"
    - "Can only train on (input, output) pairs, not preferences"
    - "Easily reward-hacked by models memorizing training data"

  grpo_advantages:
    - "Uses reward functions to score response quality"
    - "Multiple generations compared (Group Relative)"
    - "Semantic similarity captures meaning, not verbatim wording"
    - "Multi-layer rewards prevent adversarial gaming"
    - "Can penalize 'word soup' via coherence checks"

  key_insight: |
    The breakthrough insight is that GRPO's reward function paradigm
    perfectly maps to political theory training: we can encode what
    makes a "good Marxist-Leninist response" through semantic rewards
    rather than exact answer matching.

# =============================================================================
# BASE MODEL
# =============================================================================

model:
  primary:
    name: DeepSeek-R1-0528-Qwen3-8B
    huggingface: unsloth/DeepSeek-R1-0528-Qwen3-8B
    params: 8B
    architecture: Qwen-2.5
    context_length: 2048 (training max_completion_length)
    reasoning: |
      1. DeepSeek R1 architecture has strong reasoning capabilities
      2. 8B params = fits on A40 with LoRA
      3. Full Unsloth optimization support
      4. unsloth/ namespace provides optimized 4-bit version

  lora_config:
    rank: 64
    lora_alpha: 64
    target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]
    use_rslora: true
    use_gradient_checkpointing: unsloth

# =============================================================================
# DATASET
# =============================================================================

dataset:
  source: training_data/curated_qa.jsonl
  transformed: training_data/grpo_dataset.jsonl
  count: 128 curated Q&A pairs
  format: |
    {"prompt": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}], "answer": "..."}

  topics:
    - Revisionism and opportunism
    - Surplus value and exploitation
    - Dialectical materialism
    - Imperialism and monopoly capitalism
    - Class struggle and revolution
    - Socialist construction
    - Historical figures (Marx, Lenin, Engels, Stalin, Mao)

# =============================================================================
# REWARD FUNCTION SYSTEM (THE KEY INNOVATION)
# =============================================================================

reward_functions:
  reference: ai-docs/reward-modeling.yaml
  module: src/pw_mcp/ai_training/grpo_rewards.py
  tests: tests/unit/training/test_grpo_rewards.py (43 tests)

  layers:
    format_rewards:
      - match_format_exactly: "3.0 for exact </think> pattern"
      - match_format_approximately: "1.5-2.0 for partial pattern"

    semantic_rewards:
      - semantic_similarity_reward: "Sentence-transformer embedding similarity"
      - terminology_reward: "Bonus for Marxist lexicon usage"

    coherence_rewards:
      - nli_coherence_reward: "BART-large-MNLI entailment checking"
      - self_consistency_reward: "No internal contradictions"
      - structural_coherence_reward: "Terms in syntactic roles"

    anti_hacking_rewards:
      - topic_relevance_reward: "Question→answer concept coverage"
      - interconnection_depth_reward: "Anti-buzzword-salad detection"
      - completeness_reward: "Key concept coverage"

  combined_functions:
    full_coherence_reward: "5-layer combined (recommended)"
    robust_coherence_reward: "3-layer (NLI + self-consistency + structural)"

  anti_hacking_measures:
    HOLLOW_BUZZWORDS: "Penalty for activist jargon without substance"
    DEPTH_MARKERS: "Bonus for historical specificity"
    EXPLANATORY_PHRASES: "Bonus for causal reasoning"
    depth_ratio: "Words per concept (penalizes buzzword salad)"

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================

training_config:
  trainer: GRPOTrainer (from trl library)
  max_steps: 250
  batch_size: 2
  gradient_accumulation: 2
  learning_rate: 5e-6
  num_generations: 4
  temperature: 1.0
  gpu_memory_utilization: 0.85
  hardware: A40 48GB (RunPod)

  vllm_sampling:
    min_p: 0.1
    top_p: 1.0
    top_k: -1
    seed: 3407

# =============================================================================
# WEIGHTS & BIASES INTEGRATION
# =============================================================================

wandb_logging:
  module: src/pw_mcp/ai_training/wandb_logging.py
  tests: tests/unit/training/test_wandb_logging.py (17 tests)

  features:
    - WandbSampleLogger: "Periodic sample tables (question → response → rewards)"
    - create_logging_reward: "Zero-cost logging reward for GRPOTrainer"
    - log_reward_metrics: "Per-reward mean/min/max tracking"
    - init_wandb_logging: "Project/run initialization"
    - finish_wandb_logging: "Summary statistics"
    - log_model_checkpoint: "Artifact logging"

  graceful_degradation: |
    All functions work without wandb installed.
    Falls back to print() statements when wandb unavailable.

# =============================================================================
# TRAINING NOTEBOOK (AUTHORITATIVE REFERENCE)
# =============================================================================

notebook:
  location: training_data/Marxist_GRPO_Training.ipynb
  description: |
    Self-contained Jupyter notebook for RunPod execution.
    Contains ALL reward functions inline (no external imports).
    This is the authoritative reference for current implementation.

  contents:
    - "Model loading (FastLanguageModel.from_pretrained)"
    - "LoRA configuration (get_peft_model)"
    - "Dataset loading from grpo_dataset.jsonl"
    - "All 13+ reward functions inline"
    - "W&B logging integration"
    - "GRPOConfig with A40-optimized settings"
    - "GRPOTrainer setup and training"
    - "LoRA saving and GGUF export"

# =============================================================================
# EXPORT AND DEPLOYMENT
# =============================================================================

export:
  lora_save:
    code: |
      model.save_pretrained_merged("marxist_lora", tokenizer)

  gguf:
    description: Quantized format for Ollama deployment
    quantization_levels:
      q4_k_m: "Recommended balance of size/quality (~4GB)"
      q8_0: "Higher quality (~7GB)"
    code: |
      model.save_pretrained_gguf("marxist_gguf", tokenizer, quantization_method="q4_k_m")

  ollama:
    modelfile: |
      FROM ./marxist-gguf-q4_k_m.gguf
      TEMPLATE """<|begin_of_text|>{{ .System }}<|User|>{{ .Prompt }}<|Assistant|>"""
      SYSTEM "You are a Marxist-Leninist assistant..."
      PARAMETER temperature 0.7
      PARAMETER num_ctx 2048

# =============================================================================
# IMPLEMENTATION STATUS
# =============================================================================

implementation_status:
  completed:
    - "8.1 Dataset Preparation: grpo_dataset.jsonl (128 Q&A pairs)"
    - "8.2 Reward Function System: 13+ functions with anti-hacking"
    - "8.3 W&B Logging Integration: Full observability"
    - "8.4 Training Notebook: Self-contained for RunPod"

  pending:
    - "8.5 Training Execution: Run notebook on RunPod A40"
    - "8.6 Model Evaluation: Manual review, reward hacking detection"
    - "8.7 GGUF Export: Convert to Ollama format"

  test_counts:
    grpo_rewards: 43 passing
    wandb_logging: 17 passing
    total: 60 passing

# =============================================================================
# HARDWARE REQUIREMENTS
# =============================================================================

hardware:
  recommended:
    gpu: A40 (48GB VRAM) on RunPod
    cost: "~$0.79/hour spot, ~$1.14/hour on-demand"
    training_time: "~2-4 hours for 250 steps"

  vram_breakdown:
    model_4bit: "~4GB"
    lora_params: "~1GB"
    optimizer_state: "~2GB"
    activations: "~6GB with gradient checkpointing"
    vllm_generation: "~8GB for 4 generations"
    reward_models: "~2.5GB (NLI + embeddings + spaCy)"
    total: "~24GB (safe on 48GB A40)"

  cloud_options:
    primary: "RunPod A40 (48GB) - see ai-docs/runpod.yaml"
    alternative: "Lambda Labs A100 (40GB)"

# =============================================================================
# REFERENCES
# =============================================================================

references:
  internal:
    - "ai-docs/reward-modeling.yaml: Reward function design"
    - "ai-docs/runpod.yaml: GPU setup instructions"
    - "training_data/Marxist_GRPO_Training.ipynb: Authoritative notebook"
    - "src/pw_mcp/ai_training/: Python module"

  external:
    unsloth:
      - "https://github.com/unslothai/unsloth"
      - "https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide"
    grpo:
      - "https://arxiv.org/abs/2508.18212 - NLI as reward paradigm"
      - "https://arxiv.org/abs/2509.22047 - MO-GRPO reward hacking"
    trl:
      - "https://huggingface.co/docs/trl/en/grpo_trainer"
