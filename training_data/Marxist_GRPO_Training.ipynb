{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marxist-Leninist GRPO Training\n",
    "\n",
    "This notebook fine-tunes **DeepSeek-R1-0528-Qwen3-8B** on the ProleWiki corpus using GRPO (Group Relative Policy Optimization).\n",
    "\n",
    "**Goal:** Train the model to reason through political theory questions using dialectical materialist analysis, showing reasoning in `<think>` tags.\n",
    "\n",
    "**Hardware:** Optimized for A40 (48GB VRAM)\n",
    "\n",
    "**Dataset:** 1,058 Q&A pairs from ProleWiki covering:\n",
    "- Revisionism and opportunism\n",
    "- Dialectical and historical materialism\n",
    "- Anti-colonial theory (Fanon, Rodney, Nkrumah)\n",
    "- Revolutionary theory (Jackson, Sankara, PFLP)\n",
    "- Marxist political economy\n",
    "\n",
    "---\n",
    "\n",
    "**Based on:** [Unsloth GRPO Notebook](https://github.com/unslothai/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"  # Extra 30% context lengths!\n",
    "\n",
    "# Install dependencies\n",
    "!pip install unsloth vllm sentence-transformers numpy\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "\n",
    "# spaCy with TRANSFORMER model for better semantic understanding\n",
    "!pip install spacy spacy-curated-transformers\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Model\n\nWe load the DeepSeek-R1-0528-Qwen3-8B model with **16-bit precision** and vLLM fast inference.\n\n**GRPO Requirement:** Unlike SFT, GRPO training requires `load_in_4bit=False` because:\n1. LoRA adapters need 16-bit precision for proper gradient flow during policy optimization\n2. The vLLM generation pipeline handles memory efficiently with `fast_inference=True`\n3. `gpu_memory_utilization=0.6` is conservative to leave headroom for reward model inference"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "MAX_SEQ_LENGTH = 2048  # Longer for detailed political theory responses\n",
    "LORA_RANK = 32\n",
    "GPU_MEMORY_UTILIZATION = 0.6  # Conservative for 16-bit GRPO\n",
    "\n",
    "# IMPORTANT: For GRPO training, load_in_4bit must be False\n",
    "# GRPO requires 16-bit LoRA adapters for proper gradient flow\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=False,  # Must be False for GRPO LoRA 16bit\n",
    "    fast_inference=True,  # Enable vLLM\n",
    "    max_lora_rank=LORA_RANK,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    ")\n",
    "\n",
    "print(f\"Model type: {model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Apply LoRA\n\nApply LoRA adapters to attention and feed-forward layers for efficient fine-tuning.\n\n**GRPO Configuration:**\n- `lora_alpha = lora_rank` (same value, not doubled like some SFT configs)\n- `use_gradient_checkpointing = \"unsloth\"` for 30% VRAM reduction\n- Targets all attention + MLP layers for comprehensive adaptation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=LORA_RANK,  # Same as r for GRPO (not r*2)\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Template\n",
    "\n",
    "The DeepSeek-R1 distilled model uses a specific chat template. Let's verify it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find special tokens\n",
    "reasoning_start = None\n",
    "reasoning_end = None\n",
    "\n",
    "for token in tokenizer.get_added_vocab():\n",
    "    if \"think\" in token and \"/\" in token:\n",
    "        reasoning_end = token\n",
    "    elif \"think\" in token:\n",
    "        reasoning_start = token\n",
    "\n",
    "print(f\"Reasoning start: {reasoning_start}\")\n",
    "print(f\"Reasoning end: {reasoning_end}\")\n",
    "\n",
    "# Our system prompt for Marxist-Leninist reasoning\n",
    "SYSTEM_PROMPT = \"\"\"You are a Marxist-Leninist assistant trained on ProleWiki and critical theory.\n",
    "Think through political theory questions using dialectical materialist analysis.\n",
    "Show your reasoning in <think> tags, then provide a clear, well-sourced answer.\"\"\"\n",
    "\n",
    "print(f\"\\nSystem prompt:\\n{SYSTEM_PROMPT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chat template\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is revisionism?\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the GRPO-formatted dataset from `grpo_dataset.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "DATA_PATH = Path(\"grpo_dataset.jsonl\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found: {DATA_PATH}\\n\" \"Run 'python transform_to_grpo.py' first!\"\n",
    "    )\n",
    "\n",
    "dataset = Dataset.from_json(str(DATA_PATH))\n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Show sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nSample prompt: {sample['prompt'][1]['content'][:80]}...\")\n",
    "print(f\"Sample answer: {sample['answer'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reward Functions\n\nGRPO uses reward functions to guide the model toward desired behaviors:\n\n1. **Format rewards** - Encourage proper `<think>...</think>` structure\n2. **NLI coherence** - Check if response ENTAILS ground truth (defeats word soup)\n3. **Self-consistency** - Check for internal contradictions (no external ideology)\n4. **Structural coherence** - Verify terms in proper syntactic roles\n5. **Completeness reward** - Reward thorough, detailed responses\n\n### Research Basis\n- [NLI as reward paradigm](https://arxiv.org/abs/2508.18212)\n- [MO-GRPO mitigating reward hacking](https://arxiv.org/abs/2509.22047)\n- [Process-based rewards](https://arxiv.org/abs/2508.05170)\n\n### Why NLI instead of keyword matching?\nSimple terminology matching can be gamed with \"word soup\" - random Marxist terms without coherent meaning. NLI checks logical consistency, not word overlap."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Reasoning format tokens\n",
    "REASONING_START = \"<think>\"\n",
    "REASONING_END = \"</think>\"\n",
    "\n",
    "# Regex to match format\n",
    "SOLUTION_END_REGEX = re.compile(rf\"{REASONING_END}(.*)\", re.DOTALL)\n",
    "\n",
    "# Lazy-load models to avoid loading at import time\n",
    "_embedder = None\n",
    "_nli_pipeline = None\n",
    "_spacy_nlp = None\n",
    "\n",
    "\n",
    "def get_embedder():\n",
    "    \"\"\"Get or initialize the sentence transformer embedder.\"\"\"\n",
    "    global _embedder\n",
    "    if _embedder is None:\n",
    "        print(\"[Reward] Loading sentence-transformers embedder...\")\n",
    "        _embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    return _embedder\n",
    "\n",
    "\n",
    "def get_nli_pipeline():\n",
    "    \"\"\"Get or initialize the NLI pipeline (BART-large-MNLI).\"\"\"\n",
    "    global _nli_pipeline\n",
    "    if _nli_pipeline is None:\n",
    "        print(\"[Reward] Loading NLI model (bart-large-mnli)...\")\n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        _nli_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"facebook/bart-large-mnli\",\n",
    "            device=device,\n",
    "        )\n",
    "    return _nli_pipeline\n",
    "\n",
    "\n",
    "def get_spacy_nlp():\n",
    "    \"\"\"Get or initialize spaCy NLP pipeline.\n",
    "\n",
    "    Uses en_core_web_trf (transformer-based) for superior semantic understanding.\n",
    "    Falls back to en_core_web_md (word vectors) or en_core_web_sm if unavailable.\n",
    "    \"\"\"\n",
    "    global _spacy_nlp\n",
    "    if _spacy_nlp is None:\n",
    "        import spacy\n",
    "\n",
    "        # Try transformer model first (best semantic understanding)\n",
    "        models_to_try = [\"en_core_web_trf\", \"en_core_web_md\", \"en_core_web_sm\"]\n",
    "\n",
    "        for model_name in models_to_try:\n",
    "            try:\n",
    "                print(f\"[Reward] Loading spaCy model: {model_name}...\")\n",
    "                _spacy_nlp = spacy.load(model_name)\n",
    "                print(f\"[Reward] Loaded {model_name} successfully\")\n",
    "                break\n",
    "            except OSError:\n",
    "                print(f\"[Reward] {model_name} not found, trying next...\")\n",
    "                continue\n",
    "\n",
    "        if _spacy_nlp is None:\n",
    "            raise OSError(\"No spaCy model found!\")\n",
    "    return _spacy_nlp\n",
    "\n",
    "\n",
    "# Discourse connectives indicating logical structure\n",
    "DISCOURSE_CONNECTIVES = {\n",
    "    \"because\",\n",
    "    \"therefore\",\n",
    "    \"thus\",\n",
    "    \"hence\",\n",
    "    \"consequently\",\n",
    "    \"however\",\n",
    "    \"although\",\n",
    "    \"whereas\",\n",
    "    \"nevertheless\",\n",
    "    \"moreover\",\n",
    "    \"furthermore\",\n",
    "    \"additionally\",\n",
    "    \"specifically\",\n",
    "    \"namely\",\n",
    "    \"as a result\",\n",
    "    \"due to\",\n",
    "    \"in order to\",\n",
    "    \"so that\",\n",
    "    \"on the other hand\",\n",
    "    \"in contrast\",\n",
    "    \"similarly\",\n",
    "    \"likewise\",\n",
    "}\n",
    "\n",
    "# Marxist concept equivalences for topic matching\n",
    "CONCEPT_EQUIVALENCES = {\n",
    "    # Class terms\n",
    "    \"bourgeoisie\": {\"capitalist class\", \"ruling class\", \"capitalists\", \"bourgeois\", \"capital\"},\n",
    "    \"proletariat\": {\"working class\", \"workers\", \"wage laborers\", \"labor\", \"labourers\"},\n",
    "    \"petty bourgeoisie\": {\"petit bourgeoisie\", \"small business\", \"middle class\", \"petty bourgeois\"},\n",
    "    # Economic concepts\n",
    "    \"surplus value\": {\"unpaid labor\", \"profit\", \"extraction\", \"surplus labor\"},\n",
    "    \"means of production\": {\"productive forces\", \"capital goods\", \"factories\", \"industry\"},\n",
    "    \"exploitation\": {\"extraction\", \"appropriation\", \"expropriation\"},\n",
    "    # Political concepts\n",
    "    \"dictatorship of the proletariat\": {\n",
    "        \"workers state\",\n",
    "        \"proletarian dictatorship\",\n",
    "        \"workers government\",\n",
    "    },\n",
    "    \"vanguard party\": {\"vanguard\", \"communist party\", \"revolutionary party\"},\n",
    "    # Imperialism\n",
    "    \"imperialism\": {\"colonialism\", \"neo-colonialism\", \"empire\", \"colonial\"},\n",
    "    \"national liberation\": {\"decolonization\", \"anti-colonial\", \"liberation movement\"},\n",
    "    # Ideology\n",
    "    \"revisionism\": {\"opportunism\", \"reformism\", \"right deviation\"},\n",
    "    \"hegemony\": {\"ideological hegemony\", \"cultural hegemony\", \"domination\"},\n",
    "    # Philosophy\n",
    "    \"dialectical materialism\": {\"diamat\", \"materialist dialectics\", \"dialectics\"},\n",
    "    \"historical materialism\": {\"histmat\", \"materialist conception of history\"},\n",
    "}\n",
    "\n",
    "# Question words to ignore when extracting topics\n",
    "QUESTION_WORDS = {\"what\", \"how\", \"why\", \"who\", \"when\", \"where\", \"which\", \"whom\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marxist terminology for vocabulary reward\n",
    "MARXIST_TERMS = {\n",
    "    # Core concepts\n",
    "    \"dialectical\",\n",
    "    \"materialism\",\n",
    "    \"historical materialism\",\n",
    "    \"dialectical materialism\",\n",
    "    # Classes\n",
    "    \"bourgeoisie\",\n",
    "    \"proletariat\",\n",
    "    \"petty bourgeois\",\n",
    "    \"petty bourgeoisie\",\n",
    "    \"lumpenproletariat\",\n",
    "    \"working class\",\n",
    "    \"ruling class\",\n",
    "    # Class struggle\n",
    "    \"class struggle\",\n",
    "    \"class consciousness\",\n",
    "    \"class war\",\n",
    "    \"class conflict\",\n",
    "    # Political economy\n",
    "    \"surplus value\",\n",
    "    \"commodity\",\n",
    "    \"use value\",\n",
    "    \"exchange value\",\n",
    "    \"labor power\",\n",
    "    \"means of production\",\n",
    "    \"relations of production\",\n",
    "    \"forces of production\",\n",
    "    \"mode of production\",\n",
    "    \"primitive accumulation\",\n",
    "    \"exploitation\",\n",
    "    \"capital accumulation\",\n",
    "    # Imperialism\n",
    "    \"imperialism\",\n",
    "    \"colonialism\",\n",
    "    \"neo-colonialism\",\n",
    "    \"settler colonialism\",\n",
    "    \"national liberation\",\n",
    "    \"self-determination\",\n",
    "    # State and revolution\n",
    "    \"dictatorship of the proletariat\",\n",
    "    \"vanguard\",\n",
    "    \"vanguard party\",\n",
    "    \"democratic centralism\",\n",
    "    \"withering away of the state\",\n",
    "    \"proletarian dictatorship\",\n",
    "    # Ideology\n",
    "    \"hegemony\",\n",
    "    \"superstructure\",\n",
    "    \"base\",\n",
    "    \"ideology\",\n",
    "    \"false consciousness\",\n",
    "    # Revisionism\n",
    "    \"revisionism\",\n",
    "    \"opportunism\",\n",
    "    \"reformism\",\n",
    "    \"social democracy\",\n",
    "    \"ultra-leftism\",\n",
    "    # Alienation\n",
    "    \"alienation\",\n",
    "    \"fetishism\",\n",
    "    \"commodity fetishism\",\n",
    "    \"reification\",\n",
    "    # Historical\n",
    "    \"paris commune\",\n",
    "    \"october revolution\",\n",
    "    \"bolshevik\",\n",
    "    \"menshevik\",\n",
    "    # Anti-colonial\n",
    "    \"decolonization\",\n",
    "    \"third world\",\n",
    "    \"global south\",\n",
    "    \"national bourgeoisie\",\n",
    "    \"comprador\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEPTH ANALYSIS CONSTANTS (Anti-Buzzword-Salad)\n",
    "# =============================================================================\n",
    "\n",
    "# Explanatory phrases indicating concept is being explained, not just dropped\n",
    "EXPLANATORY_PHRASES = {\n",
    "    # Causal explanations\n",
    "    \"because the\",\n",
    "    \"because of\",\n",
    "    \"this is because\",\n",
    "    \"since the\",\n",
    "    \"due to the\",\n",
    "    \"as a result of\",\n",
    "    \"results from\",\n",
    "    \"caused by\",\n",
    "    \"leads to\",\n",
    "    \"results in\",\n",
    "    \"enables\",\n",
    "    \"produces\",\n",
    "    # Definitional explanations\n",
    "    \"is defined as\",\n",
    "    \"refers to\",\n",
    "    \"means that\",\n",
    "    \"denotes\",\n",
    "    \"that is\",\n",
    "    \"in other words\",\n",
    "    \"namely\",\n",
    "    \"i.e.\",\n",
    "    # Elaboration\n",
    "    \"specifically\",\n",
    "    \"in particular\",\n",
    "    \"for example\",\n",
    "    \"such as\",\n",
    "    \"this means\",\n",
    "    \"which means\",\n",
    "    \"this implies\",\n",
    "    \"therefore\",\n",
    "    # Mechanism explanations\n",
    "    \"this occurs when\",\n",
    "    \"this happens because\",\n",
    "    \"the mechanism\",\n",
    "    \"through the process of\",\n",
    "    \"by means of\",\n",
    "    \"works by\",\n",
    "}\n",
    "\n",
    "# Hollow buzzwords - activist jargon that substitutes for analysis\n",
    "HOLLOW_BUZZWORDS = {\n",
    "    # Vague connectors\n",
    "    \"interconnected\",\n",
    "    \"interrelated\",\n",
    "    \"intersects with\",\n",
    "    \"it's all connected\",\n",
    "    \"everything is connected\",\n",
    "    \"systemic\",\n",
    "    # Performative language\n",
    "    \"centered\",\n",
    "    \"centering\",\n",
    "    \"uplift\",\n",
    "    \"uplifting\",\n",
    "    \"do the work\",\n",
    "    \"the work\",\n",
    "    \"unpack\",\n",
    "    \"unpacking\",\n",
    "    \"unlearn\",\n",
    "    \"unlearning\",\n",
    "    \"hold space\",\n",
    "    \"sit with\",\n",
    "    \"lean into\",\n",
    "    \"problematic\",\n",
    "    \"harmful\",\n",
    "    \"toxic\",\n",
    "    # Vague abstractions\n",
    "    \"in a way\",\n",
    "    \"sort of\",\n",
    "    \"kind of\",\n",
    "    \"essentially\",\n",
    "    \"basically\",\n",
    "    \"generally speaking\",\n",
    "    \"broadly\",\n",
    "}\n",
    "\n",
    "# Depth markers - phrases indicating analytical depth\n",
    "DEPTH_MARKERS = {\n",
    "    # Historical specificity\n",
    "    \"in 1\",\n",
    "    \"in 2\",\n",
    "    \"during the\",\n",
    "    \"after the\",\n",
    "    \"before the\",\n",
    "    # Citations and references\n",
    "    \"marx argued\",\n",
    "    \"lenin wrote\",\n",
    "    \"engels noted\",\n",
    "    \"gramsci\",\n",
    "    \"according to\",\n",
    "    \"as marx\",\n",
    "    \"as lenin\",\n",
    "    # Concrete examples\n",
    "    \"for example\",\n",
    "    \"such as\",\n",
    "    \"in the case of\",\n",
    "    \"consider\",\n",
    "    # Definitions\n",
    "    \"defined as\",\n",
    "    \"meaning\",\n",
    "    \"specifically\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward +3.0 if response contains proper </think> tag.\n",
    "    This encourages the model to use the reasoning format.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0.0\n",
    "        response = completion[0][\"content\"]\n",
    "        if SOLUTION_END_REGEX.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward partial format matching.\n",
    "    +0.5 for exactly one <think> tag\n",
    "    +0.5 for exactly one </think> tag\n",
    "    -1.0 for multiple or missing tags\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0.0\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        start_count = response.count(REASONING_START)\n",
    "        end_count = response.count(REASONING_END)\n",
    "\n",
    "        score += 0.5 if start_count == 1 else -1.0\n",
    "        score += 0.5 if end_count == 1 else -1.0\n",
    "\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_reward(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward responses that are semantically similar to ground truth.\n",
    "    Uses sentence-transformers to compute cosine similarity.\n",
    "\n",
    "    Scoring:\n",
    "        > 0.75 similarity: +5.0\n",
    "        > 0.60 similarity: +3.0\n",
    "        > 0.45 similarity: +1.0\n",
    "        > 0.30 similarity: -1.0\n",
    "        <= 0.30 similarity: -3.0\n",
    "    \"\"\"\n",
    "    embedder = get_embedder()\n",
    "    scores = []\n",
    "\n",
    "    for completion, true_answer in zip(completions, answer, strict=False):\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        # Extract answer after </think> if present\n",
    "        if REASONING_END in response:\n",
    "            response = response.split(REASONING_END, 1)[1].strip()\n",
    "\n",
    "        # Handle empty response\n",
    "        if not response or len(response.strip()) < 10:\n",
    "            scores.append(-3.0)\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        emb_response = embedder.encode(response, normalize_embeddings=True)\n",
    "        emb_truth = embedder.encode(true_answer, normalize_embeddings=True)\n",
    "        similarity = float(np.dot(emb_response, emb_truth))\n",
    "\n",
    "        # Scale to reward\n",
    "        if similarity > 0.75:\n",
    "            score = 5.0\n",
    "        elif similarity > 0.60:\n",
    "            score = 3.0\n",
    "        elif similarity > 0.45:\n",
    "            score = 1.0\n",
    "        elif similarity > 0.30:\n",
    "            score = -1.0\n",
    "        else:\n",
    "            score = -3.0\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminology_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward use of proper Marxist terminology.\n",
    "    +0.3 per unique term found, capped at +2.0\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"].lower()\n",
    "\n",
    "        # Count unique terms present\n",
    "        term_count = sum(1 for term in MARXIST_TERMS if term in response)\n",
    "\n",
    "        # Reward: 0.3 per term, capped at 2.0\n",
    "        score = min(term_count * 0.3, 2.0)\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def completeness_reward(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward thorough, detailed responses.\n",
    "    Compares response length to ground truth length.\n",
    "\n",
    "    Scoring:\n",
    "        50-150% of target length: +2.0\n",
    "        30-200% of target length: +1.0\n",
    "        < 20% (too short): -2.0\n",
    "        > 200% (too verbose): -0.5\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for completion, true_answer in zip(completions, answer, strict=False):\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        # Extract answer after </think>\n",
    "        if REASONING_END in response:\n",
    "            answer_part = response.split(REASONING_END, 1)[1].strip()\n",
    "        else:\n",
    "            answer_part = response\n",
    "\n",
    "        answer_len = len(answer_part.split())\n",
    "        true_len = len(true_answer.split())\n",
    "\n",
    "        if true_len == 0:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        ratio = answer_len / true_len\n",
    "\n",
    "        if 0.5 <= ratio <= 1.5:\n",
    "            score = 2.0\n",
    "        elif 0.3 <= ratio <= 2.0:\n",
    "            score = 1.0\n",
    "        elif ratio < 0.2:\n",
    "            score = -2.0\n",
    "        else:\n",
    "            score = -0.5\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NLI-BASED COHERENCE REWARDS (Research-backed, defeats word soup)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def nli_coherence_reward(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward responses that logically ENTAIL the ground truth answer.\n",
    "\n",
    "    Uses Natural Language Inference (facebook/bart-large-mnli) to check\n",
    "    if the response is logically consistent with the expected answer.\n",
    "\n",
    "    This defeats \"word soup\" attacks because random terminology won't\n",
    "    logically entail anything - it will be classified as NEUTRAL.\n",
    "\n",
    "    Scoring:\n",
    "        entailment: +3.0 (response supports/implies ground truth)\n",
    "        neutral: -1.0 (off-topic or incoherent)\n",
    "        contradiction: -3.0 (contradicts ground truth)\n",
    "    \"\"\"\n",
    "    nli = get_nli_pipeline()\n",
    "    scores = []\n",
    "\n",
    "    for completion, true_answer in zip(completions, answer, strict=False):\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        # Extract answer part after </think>\n",
    "        if REASONING_END in response:\n",
    "            response = response.split(REASONING_END, 1)[1].strip()\n",
    "\n",
    "        # Handle empty or very short responses\n",
    "        if not response or len(response.strip()) < 20:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "\n",
    "        # Truncate to model max length\n",
    "        response_truncated = response[:512]\n",
    "        truth_truncated = true_answer[:512]\n",
    "\n",
    "        try:\n",
    "            input_text = f\"{response_truncated}</s></s>{truth_truncated}\"\n",
    "            result = nli(input_text)[0]\n",
    "            label = result[\"label\"].lower()\n",
    "\n",
    "            if label == \"entailment\":\n",
    "                score = 3.0\n",
    "            elif label == \"neutral\":\n",
    "                score = -1.0\n",
    "            else:  # contradiction\n",
    "                score = -3.0\n",
    "\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"[NLI Reward] Error: {e}\")\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def self_consistency_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward responses that are internally self-consistent.\n",
    "\n",
    "    Checks if any sentence in the response CONTRADICTS another sentence.\n",
    "    This avoids external ideological bias by only checking within-document\n",
    "    coherence.\n",
    "\n",
    "    Scoring:\n",
    "        No contradictions found: +1.0\n",
    "        Internal contradiction detected: -2.0\n",
    "    \"\"\"\n",
    "    nli = get_nli_pipeline()\n",
    "    nlp = get_spacy_nlp()\n",
    "    scores = []\n",
    "\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        # Parse into sentences\n",
    "        doc = nlp(response)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "\n",
    "        # Need at least 2 sentences to check consistency\n",
    "        if len(sentences) < 2:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Check pairs of sentences for contradictions\n",
    "        has_contradiction = False\n",
    "        max_pairs_to_check = 10\n",
    "        pairs_checked = 0\n",
    "\n",
    "        for i, sent_a in enumerate(sentences[:-1]):\n",
    "            if pairs_checked >= max_pairs_to_check:\n",
    "                break\n",
    "            for j in range(i + 1, min(i + 3, len(sentences))):\n",
    "                sent_b = sentences[j]\n",
    "                try:\n",
    "                    input_text = f\"{sent_a[:256]}</s></s>{sent_b[:256]}\"\n",
    "                    result = nli(input_text)[0]\n",
    "                    if result[\"label\"].lower() == \"contradiction\":\n",
    "                        has_contradiction = True\n",
    "                        break\n",
    "                    pairs_checked += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if has_contradiction:\n",
    "                break\n",
    "\n",
    "        scores.append(-2.0 if has_contradiction else 1.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def structural_coherence_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward responses with proper linguistic structure.\n",
    "\n",
    "    Uses spaCy dependency parsing to verify:\n",
    "    1. Marxist terms appear in meaningful syntactic roles (subject, object)\n",
    "    2. Response contains logical discourse connectives\n",
    "    3. Response has proper sentence structure (not word soup)\n",
    "\n",
    "    Scoring:\n",
    "        +0.3 per term in subject/object position (max +1.5)\n",
    "        +0.2 per discourse connective (max +1.0)\n",
    "        -1.0 if no complete sentences detected\n",
    "    \"\"\"\n",
    "    nlp = get_spacy_nlp()\n",
    "    scores = []\n",
    "\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        doc = nlp(response)\n",
    "        score = 0.0\n",
    "\n",
    "        # Check 1: Are there actual sentences?\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) < 1:\n",
    "            scores.append(-1.0)\n",
    "            continue\n",
    "\n",
    "        # Check 2: Marxist terms in meaningful syntactic roles\n",
    "        terms_in_context = 0\n",
    "        response_lower = response.lower()\n",
    "\n",
    "        for term in MARXIST_TERMS:\n",
    "            if term not in response_lower:\n",
    "                continue\n",
    "\n",
    "            for token in doc:\n",
    "                # Reward if term found in meaningful syntactic role\n",
    "                is_meaningful_role = token.dep_ in (\n",
    "                    \"nsubj\",\n",
    "                    \"nsubjpass\",\n",
    "                    \"dobj\",\n",
    "                    \"pobj\",\n",
    "                    \"attr\",\n",
    "                    \"appos\",\n",
    "                )\n",
    "                is_verb_root = token.head.pos_ == \"VERB\" and token.head.dep_ == \"ROOT\"\n",
    "                if term in token.text.lower() and (is_meaningful_role or is_verb_root):\n",
    "                    terms_in_context += 1\n",
    "                    break\n",
    "\n",
    "        score += min(terms_in_context * 0.3, 1.5)\n",
    "\n",
    "        # Check 3: Discourse connectives\n",
    "        connective_count = sum(1 for conn in DISCOURSE_CONNECTIVES if conn in response_lower)\n",
    "        score += min(connective_count * 0.2, 1.0)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def robust_coherence_reward(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Multi-layered coherence check combining NLI, self-consistency, and structure.\n",
    "\n",
    "    This is the RECOMMENDED reward function for robust evaluation that defeats\n",
    "    reward hacking via word soup or other adversarial strategies.\n",
    "\n",
    "    Layers:\n",
    "    1. NLI coherence: Does response entail ground truth?\n",
    "    2. Self-consistency: Does response contradict itself?\n",
    "    3. Structural coherence: Are terms used in meaningful syntactic roles?\n",
    "    \"\"\"\n",
    "    nli_scores = nli_coherence_reward(completions, answer, **kwargs)\n",
    "    consistency_scores = self_consistency_reward(completions, **kwargs)\n",
    "    structure_scores = structural_coherence_reward(completions, **kwargs)\n",
    "\n",
    "    combined = []\n",
    "    for nli, consistency, structure in zip(\n",
    "        nli_scores, consistency_scores, structure_scores, strict=False\n",
    "    ):\n",
    "        if nli <= -3.0:\n",
    "            combined.append(-3.0)  # Contradiction dominates\n",
    "        elif consistency <= -2.0:\n",
    "            combined.append(-2.0)  # Internal contradiction\n",
    "        else:\n",
    "            total = nli + (consistency * 0.5) + (structure * 0.5)\n",
    "            combined.append(total)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTERCONNECTION DEPTH REWARD (Anti-Buzzword-Salad)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def _compute_depth_ratio(text: str) -> float:\n",
    "    \"\"\"Compute words per unique Marxist concept (depth over breadth).\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    words = text_lower.split()\n",
    "    word_count = len(words)\n",
    "\n",
    "    if word_count < 20:\n",
    "        return 0.0\n",
    "\n",
    "    concepts_found = sum(1 for term in MARXIST_TERMS if term in text_lower)\n",
    "\n",
    "    if concepts_found == 0:\n",
    "        return float(word_count)\n",
    "\n",
    "    return word_count / concepts_found\n",
    "\n",
    "\n",
    "def _count_hollow_buzzwords(text: str) -> int:\n",
    "    \"\"\"Count hollow buzzwords in text.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for phrase in HOLLOW_BUZZWORDS if phrase in text_lower)\n",
    "\n",
    "\n",
    "def _count_depth_markers(text: str) -> int:\n",
    "    \"\"\"Count depth markers (historical specificity, citations, examples).\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for marker in DEPTH_MARKERS if marker in text_lower)\n",
    "\n",
    "\n",
    "def _count_explanatory_phrases(text: str) -> int:\n",
    "    \"\"\"Count explanatory phrases.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for phrase in EXPLANATORY_PHRASES if phrase in text_lower)\n",
    "\n",
    "\n",
    "def interconnection_depth_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward deep, meaningful interconnections; penalize buzzword salad.\n",
    "\n",
    "    Distinguishes:\n",
    "    - GOOD: \"Surplus value relates to imperialism BECAUSE capital export...\"\n",
    "    - BAD: \"Surplus value intersects with imperialism, colonialism, patriarchy...\"\n",
    "\n",
    "    Signals:\n",
    "    1. Depth ratio: words per unique Marxist concept\n",
    "       - High (>20): +1.0 (deep analysis - few concepts well-explained)\n",
    "       - Low (<5): -1.5 (severe buzzword soup)\n",
    "    2. Hollow buzzword density: -0.3 per additional above 2\n",
    "    3. Depth markers: +0.3 each (max +1.5)\n",
    "    4. Explanation ratio: +0.5 if >= 50% of concepts have explanations\n",
    "\n",
    "    Score range: -2.5 to +3.0\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        # Extract answer after </think>\n",
    "        if REASONING_END in response:\n",
    "            response = response.split(REASONING_END, 1)[1].strip()\n",
    "\n",
    "        if not response or len(response.strip()) < 20:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        score = 0.0\n",
    "\n",
    "        # Signal 1: Depth ratio (words per concept)\n",
    "        depth_ratio = _compute_depth_ratio(response)\n",
    "\n",
    "        if depth_ratio > 20:\n",
    "            score += 1.0  # Deep analysis\n",
    "        elif depth_ratio > 10:\n",
    "            score += 0.5  # Adequate depth\n",
    "        elif depth_ratio > 5:\n",
    "            score += -0.5  # Shallow\n",
    "        elif depth_ratio > 0:\n",
    "            score += -1.5  # Severe buzzword soup\n",
    "\n",
    "        # Signal 2: Hollow buzzword penalty\n",
    "        hollow_count = _count_hollow_buzzwords(response)\n",
    "        if hollow_count > 2:\n",
    "            penalty = min((hollow_count - 2) * 0.3, 1.5)\n",
    "            score -= penalty\n",
    "\n",
    "        # Signal 3: Depth markers bonus\n",
    "        depth_marker_count = _count_depth_markers(response)\n",
    "        score += min(depth_marker_count * 0.3, 1.5)\n",
    "\n",
    "        # Signal 4: Explanation ratio\n",
    "        text_lower = response.lower()\n",
    "        concepts_found = sum(1 for term in MARXIST_TERMS if term in text_lower)\n",
    "        explanations = _count_explanatory_phrases(response)\n",
    "\n",
    "        if concepts_found > 0:\n",
    "            explanation_ratio = explanations / concepts_found\n",
    "            if explanation_ratio >= 0.5:\n",
    "                score += 0.5  # Well-explained concepts\n",
    "            elif explanation_ratio < 0.1 and concepts_found > 3:\n",
    "                score -= 0.5  # Many concepts, no explanations\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOPIC RELEVANCE REWARD (Question-Answer Alignment)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def _extract_noun_with_preps(token):\n",
    "    \"\"\"\n",
    "    Extract a noun and its prepositional phrase children.\n",
    "    For \"dictatorship of the proletariat\", returns:\n",
    "    {\"dictatorship\", \"proletariat\", \"dictatorship of proletariat\"}\n",
    "    \"\"\"\n",
    "    topics = set()\n",
    "\n",
    "    if token.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "        topics.add(token.lemma_.lower())\n",
    "\n",
    "        # Check for compound modifiers (e.g., \"surplus value\")\n",
    "        modifiers = []\n",
    "        for child in token.children:\n",
    "            if child.dep_ in (\"compound\", \"amod\") and child.pos_ in (\"NOUN\", \"ADJ\"):\n",
    "                modifiers.append(child.text.lower())\n",
    "\n",
    "        if modifiers:\n",
    "            full_term = \" \".join([*modifiers, token.text.lower()])\n",
    "            topics.add(full_term)\n",
    "\n",
    "        # Follow prepositional phrases\n",
    "        for child in token.children:\n",
    "            if child.dep_ == \"prep\":\n",
    "                for pobj in child.children:\n",
    "                    if pobj.dep_ == \"pobj\":\n",
    "                        topics.add(pobj.lemma_.lower())\n",
    "                        full_phrase = f\"{token.text.lower()} {child.text} {pobj.text.lower()}\"\n",
    "                        topics.add(full_phrase)\n",
    "                        topics.update(_extract_noun_with_preps(pobj))\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "def _extract_question_topics(doc):\n",
    "    \"\"\"\n",
    "    Extract core topics from a question using spaCy dependency parsing.\n",
    "    For \"What is revisionism?\", extracts {\"revisionism\"}\n",
    "    \"\"\"\n",
    "    topics = set()\n",
    "\n",
    "    # Find the ROOT\n",
    "    root = None\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            root = token\n",
    "            break\n",
    "\n",
    "    if root:\n",
    "        for child in root.children:\n",
    "            if child.dep_ in (\"nsubj\", \"dobj\", \"attr\", \"nsubjpass\"):\n",
    "                if child.text.lower() in QUESTION_WORDS:\n",
    "                    continue\n",
    "                topics.update(_extract_noun_with_preps(child))\n",
    "\n",
    "            if child.dep_ == \"prep\":\n",
    "                for pobj in child.children:\n",
    "                    if pobj.dep_ == \"pobj\":\n",
    "                        topics.update(_extract_noun_with_preps(pobj))\n",
    "\n",
    "    # Fallback: noun chunks\n",
    "    if not topics:\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if chunk.root.text.lower() not in QUESTION_WORDS:\n",
    "                topics.add(chunk.root.lemma_.lower())\n",
    "\n",
    "    topics = {t for t in topics if t not in QUESTION_WORDS}\n",
    "    return topics\n",
    "\n",
    "\n",
    "def _extract_answer_topics(doc):\n",
    "    \"\"\"Extract topics from an answer. Strips determiners for better matching.\"\"\"\n",
    "    topics = set()\n",
    "    determiners = {\"the\", \"a\", \"an\", \"this\", \"that\", \"these\", \"those\"}\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        topics.add(chunk.root.lemma_.lower())\n",
    "\n",
    "        words = chunk.text.lower().strip().split()\n",
    "        if words and words[0] in determiners:\n",
    "            words = words[1:]\n",
    "        chunk_text = \" \".join(words)\n",
    "\n",
    "        if \" \" in chunk_text and len(chunk_text) < 50:\n",
    "            topics.add(chunk_text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        words = ent.text.lower().split()\n",
    "        if words and words[0] in determiners:\n",
    "            words = words[1:]\n",
    "        topics.add(\" \".join(words))\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "def _expand_with_synonyms(topics):\n",
    "    \"\"\"Expand topics with Marxist concept synonyms.\"\"\"\n",
    "    expanded = set(topics)\n",
    "\n",
    "    for topic in topics:\n",
    "        if topic in CONCEPT_EQUIVALENCES:\n",
    "            expanded.update(CONCEPT_EQUIVALENCES[topic])\n",
    "        for canonical, synonyms in CONCEPT_EQUIVALENCES.items():\n",
    "            if topic in synonyms or topic == canonical:\n",
    "                expanded.add(canonical)\n",
    "                expanded.update(synonyms)\n",
    "\n",
    "    return expanded\n",
    "\n",
    "\n",
    "def _compute_topic_coverage(q_topics, a_topics, nlp):\n",
    "    \"\"\"Compute how well answer topics cover question topics.\"\"\"\n",
    "    if not q_topics:\n",
    "        return 0.5\n",
    "\n",
    "    q_expanded = _expand_with_synonyms(q_topics)\n",
    "    matched = q_expanded & a_topics\n",
    "    direct_coverage = len(matched) / len(q_topics) if q_topics else 0\n",
    "\n",
    "    if direct_coverage >= 0.5:\n",
    "        return min(direct_coverage, 1.0)\n",
    "\n",
    "    # Fallback: semantic similarity\n",
    "    unmatched_q = q_topics - matched\n",
    "    semantic_matches = 0\n",
    "\n",
    "    for q_topic in unmatched_q:\n",
    "        q_token = nlp(q_topic)\n",
    "        if not q_token.has_vector:\n",
    "            continue\n",
    "\n",
    "        best_sim = 0.0\n",
    "        for a_topic in a_topics:\n",
    "            a_token = nlp(a_topic)\n",
    "            if a_token.has_vector:\n",
    "                sim = q_token.similarity(a_token)\n",
    "                best_sim = max(best_sim, sim)\n",
    "\n",
    "        if best_sim > 0.6:\n",
    "            semantic_matches += 1\n",
    "\n",
    "    total_matched = len(matched) + semantic_matches\n",
    "    return min(total_matched / len(q_topics), 1.0) if q_topics else 0.5\n",
    "\n",
    "\n",
    "def topic_relevance_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward answers that are ON-TOPIC with respect to the question.\n",
    "\n",
    "    Implements f(A) in f(Q) check where f extracts semantic topics.\n",
    "    This ensures the model answers WHAT WAS ASKED, not just generates\n",
    "    coherent Marxist text about something else.\n",
    "\n",
    "    Scoring:\n",
    "        > 80% coverage: +2.0 (answer fully addresses question topics)\n",
    "        > 60% coverage: +1.5 (answer mostly on-topic)\n",
    "        > 40% coverage: +1.0 (answer partially on-topic)\n",
    "        > 20% coverage: 0.0 (answer tangentially related)\n",
    "        <= 20% coverage: -1.5 (answer off-topic)\n",
    "    \"\"\"\n",
    "    nlp = get_spacy_nlp()\n",
    "    scores = []\n",
    "\n",
    "    for prompt, completion in zip(prompts, completions, strict=False):\n",
    "        question = prompt[-1][\"content\"]\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        if REASONING_END in response:\n",
    "            response = response.split(REASONING_END, 1)[1].strip()\n",
    "\n",
    "        if not response or len(response.strip()) < 20:\n",
    "            scores.append(-1.5)\n",
    "            continue\n",
    "\n",
    "        q_doc = nlp(question)\n",
    "        a_doc = nlp(response[:2000])\n",
    "\n",
    "        q_topics = _extract_question_topics(q_doc)\n",
    "        a_topics = _extract_answer_topics(a_doc)\n",
    "\n",
    "        if not q_topics:\n",
    "            scores.append(0.5 if len(a_topics) > 3 else 0.0)\n",
    "            continue\n",
    "\n",
    "        coverage = _compute_topic_coverage(q_topics, a_topics, nlp)\n",
    "\n",
    "        if coverage > 0.8:\n",
    "            score = 2.0\n",
    "        elif coverage > 0.6:\n",
    "            score = 1.5\n",
    "        elif coverage > 0.4:\n",
    "            score = 1.0\n",
    "        elif coverage > 0.2:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = -1.5\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def full_coherence_reward(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Complete coherence check: robust_coherence + topic_relevance + depth.\n",
    "\n",
    "    This is the MOST COMPREHENSIVE reward function, checking:\n",
    "    1. NLI coherence (A entails ground truth)\n",
    "    2. Self-consistency (A doesn't contradict itself)\n",
    "    3. Structural coherence (terms in proper syntactic roles)\n",
    "    4. Topic relevance (A addresses what Q asked about)\n",
    "    5. Interconnection depth (rewards deep analysis, penalizes buzzword salad)\n",
    "\n",
    "    Use this for maximum robustness against reward hacking.\n",
    "    \"\"\"\n",
    "    robust_scores = robust_coherence_reward(completions, answer, **kwargs)\n",
    "    relevance_scores = topic_relevance_reward(prompts, completions, **kwargs)\n",
    "    depth_scores = interconnection_depth_reward(completions, **kwargs)\n",
    "\n",
    "    combined = []\n",
    "    for robust, relevance, depth in zip(\n",
    "        robust_scores, relevance_scores, depth_scores, strict=False\n",
    "    ):\n",
    "        if relevance <= -1.5:\n",
    "            combined.append(-2.0)  # Severely off-topic\n",
    "        elif robust <= -2.0:\n",
    "            combined.append(robust)  # Robust check failed\n",
    "        elif depth <= -1.5:\n",
    "            combined.append(-1.5)  # Buzzword salad detected\n",
    "        else:\n",
    "            total = robust + (relevance * 0.4) + (depth * 0.3)\n",
    "            combined.append(total)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug reward function for monitoring during training\n",
    "_PRINT_COUNTER = 0\n",
    "_PRINT_EVERY = 10\n",
    "\n",
    "\n",
    "def debug_print_reward(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Print sample outputs periodically for monitoring.\n",
    "    Returns 0.0 (no effect on training).\n",
    "    \"\"\"\n",
    "    global _PRINT_COUNTER\n",
    "\n",
    "    if _PRINT_COUNTER % _PRINT_EVERY == 0:\n",
    "        question = prompts[0][-1][\"content\"]\n",
    "        response = completions[0][0][\"content\"]\n",
    "        true_answer = answer[0]\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Step {_PRINT_COUNTER}\")\n",
    "        print(f\"Question: {question[:100]}...\")\n",
    "        print(f\"Response: {response[:200]}...\")\n",
    "        print(f\"Expected: {true_answer[:100]}...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    _PRINT_COUNTER += 1\n",
    "\n",
    "    return [0.0] * len(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WEIGHTS & BIASES LOGGING\n",
    "# =============================================================================\n",
    "\n",
    "USE_WANDB = False  # Set to True to enable W&B logging\n",
    "\n",
    "# Lazy wandb import\n",
    "_wandb_module = None\n",
    "\n",
    "\n",
    "def _get_wandb():\n",
    "    \"\"\"Lazily import wandb module.\"\"\"\n",
    "    global _wandb_module\n",
    "    if _wandb_module is None:\n",
    "        try:\n",
    "            import wandb\n",
    "\n",
    "            _wandb_module = wandb\n",
    "        except ImportError:\n",
    "            print(\"[WandbLogging] wandb not installed. Install with: pip install wandb\")\n",
    "            _wandb_module = False\n",
    "    return _wandb_module if _wandb_module else None\n",
    "\n",
    "\n",
    "class WandbSampleLogger:\n",
    "    \"\"\"Logs sample tables to W&B for debugging reward functions.\"\"\"\n",
    "\n",
    "    def __init__(self, log_every_n_steps=10, max_samples_per_log=4):\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self.max_samples_per_log = max_samples_per_log\n",
    "        self._samples = []\n",
    "        self._table_columns = [\n",
    "            \"step\",\n",
    "            \"question\",\n",
    "            \"response\",\n",
    "            \"ground_truth\",\n",
    "            \"format_exact\",\n",
    "            \"nli_coherence\",\n",
    "            \"topic_relevance\",\n",
    "            \"depth\",\n",
    "            \"completeness\",\n",
    "            \"total\",\n",
    "        ]\n",
    "\n",
    "    def add_sample(self, step, question, response, ground_truth, rewards):\n",
    "        \"\"\"Add a sample to the buffer.\"\"\"\n",
    "        self._samples.append(\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"question\": question[:500],\n",
    "                \"response\": response[:500],\n",
    "                \"ground_truth\": ground_truth[:300],\n",
    "                \"rewards\": rewards,\n",
    "            }\n",
    "        )\n",
    "        # Keep only recent samples\n",
    "        max_buffer = self.max_samples_per_log * 3\n",
    "        if len(self._samples) > max_buffer:\n",
    "            self._samples = self._samples[-max_buffer:]\n",
    "\n",
    "    def should_log(self, step):\n",
    "        \"\"\"Check if we should log at this step.\"\"\"\n",
    "        return step > 0 and step % self.log_every_n_steps == 0\n",
    "\n",
    "    def log_table(self, step):\n",
    "        \"\"\"Log accumulated samples as a wandb.Table.\"\"\"\n",
    "        wandb = _get_wandb()\n",
    "        if wandb is None or not self._samples:\n",
    "            return\n",
    "\n",
    "        samples_to_log = self._samples[-self.max_samples_per_log :]\n",
    "        table = wandb.Table(columns=self._table_columns)\n",
    "\n",
    "        for sample in samples_to_log:\n",
    "            rewards = sample[\"rewards\"]\n",
    "            total = sum(rewards.values())\n",
    "            row = [\n",
    "                sample[\"step\"],\n",
    "                sample[\"question\"],\n",
    "                sample[\"response\"],\n",
    "                sample[\"ground_truth\"],\n",
    "                rewards.get(\"format_exact\", 0.0),\n",
    "                rewards.get(\"nli_coherence\", 0.0),\n",
    "                rewards.get(\"topic_relevance\", 0.0),\n",
    "                rewards.get(\"interconnection_depth\", 0.0),\n",
    "                rewards.get(\"completeness\", 0.0),\n",
    "                total,\n",
    "            ]\n",
    "            table.add_data(*row)\n",
    "\n",
    "        wandb.log({\"samples\": table}, step=step)\n",
    "        print(f\"[WandbLogging] Logged {len(samples_to_log)} samples at step {step}\")\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clear the sample buffer.\"\"\"\n",
    "        self._samples.clear()\n",
    "\n",
    "\n",
    "def log_reward_metrics(step, reward_scores):\n",
    "    \"\"\"Log reward metrics to wandb.\"\"\"\n",
    "    wandb = _get_wandb()\n",
    "    if wandb is None:\n",
    "        return\n",
    "\n",
    "    metrics = {}\n",
    "    for name, scores in reward_scores.items():\n",
    "        if not scores:\n",
    "            continue\n",
    "        metrics[f\"rewards/{name}\"] = sum(scores) / len(scores)\n",
    "        metrics[f\"rewards/{name}_min\"] = min(scores)\n",
    "        metrics[f\"rewards/{name}_max\"] = max(scores)\n",
    "\n",
    "    # Compute total\n",
    "    if reward_scores:\n",
    "        all_totals = []\n",
    "        num_samples = len(next(iter(reward_scores.values())))\n",
    "        for i in range(num_samples):\n",
    "            total = sum(scores[i] for scores in reward_scores.values() if i < len(scores))\n",
    "            all_totals.append(total)\n",
    "        if all_totals:\n",
    "            metrics[\"rewards/total\"] = sum(all_totals) / len(all_totals)\n",
    "\n",
    "    wandb.log(metrics, step=step)\n",
    "\n",
    "\n",
    "# Global step counter for logging reward\n",
    "_WANDB_LOGGING_STEP = 0\n",
    "\n",
    "\n",
    "def create_logging_reward(sample_logger=None, compute_all_rewards=True):\n",
    "    \"\"\"\n",
    "    Create a reward function that logs metrics and samples to wandb.\n",
    "    Returns [0.0] * len(completions) (no training effect).\n",
    "    \"\"\"\n",
    "    global _WANDB_LOGGING_STEP\n",
    "\n",
    "    def logging_reward(prompts, completions, answer, **kwargs):\n",
    "        global _WANDB_LOGGING_STEP\n",
    "        _WANDB_LOGGING_STEP += 1\n",
    "        step = _WANDB_LOGGING_STEP\n",
    "\n",
    "        wandb = _get_wandb()\n",
    "        if wandb is None or wandb.run is None:\n",
    "            return [0.0] * len(completions)\n",
    "\n",
    "        # Compute all reward scores if requested\n",
    "        if compute_all_rewards:\n",
    "            reward_scores = {}\n",
    "            try:\n",
    "                reward_scores[\"format_exact\"] = match_format_exactly(completions, **kwargs)\n",
    "            except Exception:\n",
    "                reward_scores[\"format_exact\"] = [0.0] * len(completions)\n",
    "            try:\n",
    "                reward_scores[\"nli_coherence\"] = nli_coherence_reward(completions, answer, **kwargs)\n",
    "            except Exception:\n",
    "                reward_scores[\"nli_coherence\"] = [0.0] * len(completions)\n",
    "            try:\n",
    "                reward_scores[\"topic_relevance\"] = topic_relevance_reward(\n",
    "                    prompts, completions, **kwargs\n",
    "                )\n",
    "            except Exception:\n",
    "                reward_scores[\"topic_relevance\"] = [0.0] * len(completions)\n",
    "            try:\n",
    "                reward_scores[\"interconnection_depth\"] = interconnection_depth_reward(\n",
    "                    completions, **kwargs\n",
    "                )\n",
    "            except Exception:\n",
    "                reward_scores[\"interconnection_depth\"] = [0.0] * len(completions)\n",
    "            try:\n",
    "                reward_scores[\"completeness\"] = completeness_reward(completions, answer, **kwargs)\n",
    "            except Exception:\n",
    "                reward_scores[\"completeness\"] = [0.0] * len(completions)\n",
    "\n",
    "            log_reward_metrics(step, reward_scores)\n",
    "        else:\n",
    "            reward_scores = {}\n",
    "\n",
    "        # Log samples periodically\n",
    "        if sample_logger and sample_logger.should_log(step):\n",
    "            for i in range(min(sample_logger.max_samples_per_log, len(prompts))):\n",
    "                question = prompts[i][-1][\"content\"]\n",
    "                response = completions[i][0][\"content\"]\n",
    "                truth = answer[i] if i < len(answer) else \"\"\n",
    "\n",
    "                sample_rewards = {\n",
    "                    name: scores[i] if i < len(scores) else 0.0\n",
    "                    for name, scores in reward_scores.items()\n",
    "                }\n",
    "\n",
    "                sample_logger.add_sample(\n",
    "                    step=step,\n",
    "                    question=question,\n",
    "                    response=response,\n",
    "                    ground_truth=truth,\n",
    "                    rewards=sample_rewards,\n",
    "                )\n",
    "\n",
    "            sample_logger.log_table(step)\n",
    "\n",
    "        return [0.0] * len(completions)\n",
    "\n",
    "    return logging_reward\n",
    "\n",
    "\n",
    "def init_wandb_logging(project, config, name=None, tags=None):\n",
    "    \"\"\"Initialize W&B logging for GRPO training.\"\"\"\n",
    "    wandb = _get_wandb()\n",
    "    if wandb is None:\n",
    "        return None\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=project,\n",
    "        config=config,\n",
    "        name=name,\n",
    "        tags=tags or [\"grpo\", \"marxist-leninist\"],\n",
    "    )\n",
    "    print(f\"[WandbLogging] Initialized run: {run.name}\")\n",
    "    print(f\"[WandbLogging] View at: {run.url}\")\n",
    "    return run\n",
    "\n",
    "\n",
    "def finish_wandb_logging(summary=None):\n",
    "    \"\"\"Finish the wandb run with optional summary statistics.\"\"\"\n",
    "    wandb = _get_wandb()\n",
    "    if wandb is None or wandb.run is None:\n",
    "        return\n",
    "\n",
    "    if summary:\n",
    "        for key, value in summary.items():\n",
    "            wandb.run.summary[key] = value\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"[WandbLogging] Run finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Weights & Biases Logging (Optional)\n\nEnable W&B logging for comprehensive training observability:\n- Per-step reward metrics (mean/min/max for each reward function)\n- Sample tables showing question  response  reward breakdowns\n- Training curves for debugging reward function behavior\n\n**To enable:** Set `USE_WANDB = True` below and run `wandb login` first."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Configure GRPO training with A40-optimized settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_STEPS = 250\n",
    "SAVE_STEPS = 50\n",
    "LEARNING_RATE = 5e-6\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# A40 optimized batch settings\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION = 2\n",
    "NUM_GENERATIONS = 4\n",
    "\n",
    "# Sequence lengths\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "MAX_COMPLETION_LENGTH = 1500\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = \"outputs/marxist-grpo\"\n",
    "LORA_OUTPUT = \"outputs/marxist-grpo-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM sampling parameters (temperature is set in GRPOConfig)\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p=0.1,\n",
    "    top_p=1.0,  # No nucleus sampling\n",
    "    top_k=-1,\n",
    "    seed=3407,\n",
    "    stop=[tokenizer.eos_token],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "# GRPO training configuration\n",
    "training_args = GRPOConfig(\n",
    "    # vLLM\n",
    "    vllm_sampling_params=vllm_sampling_params,\n",
    "    temperature=1.0,  # For GRPO training dynamics\n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    # Batch settings\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    # Sequence lengths\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    max_completion_length=MAX_COMPLETION_LENGTH,\n",
    "    # Training duration\n",
    "    max_steps=MAX_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    # Logging\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    # Output\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three reward configurations available:\n",
    "# 1. FULL (recommended) - Uses robust_coherence + topic_relevance + depth (maximum robustness)\n",
    "# 2. ROBUST - Uses NLI + self-consistency + structural analysis\n",
    "# 3. LEGACY - Original shallow rewards (vulnerable to word soup)\n",
    "\n",
    "REWARD_MODE = \"FULL\"  # Options: \"FULL\", \"ROBUST\", \"LEGACY\"\n",
    "\n",
    "# Initialize wandb logging if enabled\n",
    "wandb_run = None\n",
    "sample_logger = None\n",
    "logging_reward_func = None\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb_run = init_wandb_logging(\n",
    "        project=\"marxist-grpo\",\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"gradient_accumulation\": GRADIENT_ACCUMULATION,\n",
    "            \"num_generations\": NUM_GENERATIONS,\n",
    "            \"max_steps\": MAX_STEPS,\n",
    "            \"reward_mode\": REWARD_MODE,\n",
    "        },\n",
    "        tags=[\"grpo\", \"marxist-leninist\", \"prolewiki\"],\n",
    "    )\n",
    "    sample_logger = WandbSampleLogger(log_every_n_steps=10, max_samples_per_log=4)\n",
    "    logging_reward_func = create_logging_reward(sample_logger, compute_all_rewards=True)\n",
    "    print(\"[W&B] Logging enabled - reward metrics and sample tables will be logged\")\n",
    "\n",
    "if REWARD_MODE == \"FULL\":\n",
    "    print(\"Initializing GRPO trainer with FULL reward functions:\")\n",
    "    print(\"  - match_format_exactly (+3.0 for </think>)\")\n",
    "    print(\"  - match_format_approximately (tag validation)\")\n",
    "    print(\"  - full_coherence_reward (NLI + structure + topic + depth)\")\n",
    "    print(\"  - completeness_reward (length comparison)\")\n",
    "    print(\"  - debug_print_reward (monitoring)\")\n",
    "    if USE_WANDB:\n",
    "        print(\"  - logging_reward (W&B metrics + sample tables)\")\n",
    "    print(\"\\nNote: First run will download NLI model (~1.6GB) + spaCy transformer (~436MB)\")\n",
    "\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        full_coherence_reward,\n",
    "        completeness_reward,\n",
    "        debug_print_reward,\n",
    "    ]\n",
    "elif REWARD_MODE == \"ROBUST\":\n",
    "    print(\"Initializing GRPO trainer with ROBUST reward functions:\")\n",
    "    print(\"  - match_format_exactly (+3.0 for </think>)\")\n",
    "    print(\"  - match_format_approximately (tag validation)\")\n",
    "    print(\"  - robust_coherence_reward (NLI + self-consistency + structure)\")\n",
    "    print(\"  - completeness_reward (length comparison)\")\n",
    "    print(\"  - debug_print_reward (monitoring)\")\n",
    "    if USE_WANDB:\n",
    "        print(\"  - logging_reward (W&B metrics + sample tables)\")\n",
    "    print(\"\\nNote: First run will download NLI model (~1.6GB)\")\n",
    "\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        robust_coherence_reward,\n",
    "        completeness_reward,\n",
    "        debug_print_reward,\n",
    "    ]\n",
    "else:  # LEGACY\n",
    "    print(\"Initializing GRPO trainer with LEGACY reward functions:\")\n",
    "    print(\"  - match_format_exactly (+3.0 for </think>)\")\n",
    "    print(\"  - match_format_approximately (tag validation)\")\n",
    "    print(\"  - semantic_similarity_reward (+5.0 to -3.0)\")\n",
    "    print(\"  - terminology_reward (+0 to +2.0) [VULNERABLE TO WORD SOUP]\")\n",
    "    print(\"  - completeness_reward (length comparison)\")\n",
    "    print(\"  - debug_print_reward (monitoring)\")\n",
    "    if USE_WANDB:\n",
    "        print(\"  - logging_reward (W&B metrics + sample tables)\")\n",
    "\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        semantic_similarity_reward,\n",
    "        terminology_reward,\n",
    "        completeness_reward,\n",
    "        debug_print_reward,\n",
    "    ]\n",
    "\n",
    "# Add wandb logging reward if enabled\n",
    "if USE_WANDB and logging_reward_func is not None:\n",
    "    reward_funcs.append(logging_reward_func)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=reward_funcs,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "Run GRPO training. Monitor the `reward` column - it should increase over time.\n",
    "\n",
    "**Expected behavior:**\n",
    "- Steps 0-50: Format rewards stabilize\n",
    "- Steps 50-100: Semantic similarity improves\n",
    "- Steps 100-250: Content quality improves\n",
    "\n",
    "**Estimated time:** ~1-2 hours on A40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Steps: {MAX_STEPS}\")\n",
    "print(f\"Batch: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} x {NUM_GENERATIONS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "if USE_WANDB:\n",
    "    print(f\"W&B Run: {wandb_run.name if wandb_run else 'N/A'}\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb logging\n",
    "if USE_WANDB:\n",
    "    finish_wandb_logging(\n",
    "        summary={\n",
    "            \"final_step\": MAX_STEPS,\n",
    "            \"reward_mode\": REWARD_MODE,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LoRA\n",
    "\n",
    "Save the trained LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(LORA_OUTPUT, exist_ok=True)\n",
    "\n",
    "model.save_lora(LORA_OUTPUT)\n",
    "print(f\"LoRA saved to: {LORA_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify LoRA is actually trained (non-zero weights)\n",
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(f\"{LORA_OUTPUT}/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "    for key in f:\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert n_zeros.item() != tensor.numel(), f\"Layer {key} is all zeros!\"\n",
    "\n",
    "print(\"LoRA verification passed - all layers have non-zero weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing\n",
    "\n",
    "Test the model with and without the trained LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    \"What is revisionism in the Marxist sense?\",\n",
    "    \"Explain the concept of surplus value.\",\n",
    "    \"What is the dictatorship of the proletariat?\",\n",
    "    \"How does dialectical materialism differ from idealism?\",\n",
    "]\n",
    "\n",
    "test_sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TESTING WITHOUT LORA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in TEST_QUESTIONS[:2]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    output = (\n",
    "        model.fast_generate(text, sampling_params=test_sampling_params, lora_request=None)[0]\n",
    "        .outputs[0]\n",
    "        .text\n",
    "    )\n",
    "\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {output[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TESTING WITH LORA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in TEST_QUESTIONS[:2]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    output = (\n",
    "        model.fast_generate(\n",
    "            text,\n",
    "            sampling_params=test_sampling_params,\n",
    "            lora_request=model.load_lora(LORA_OUTPUT),\n",
    "        )[0]\n",
    "        .outputs[0]\n",
    "        .text\n",
    "    )\n",
    "\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {output[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Export\n",
    "\n",
    "Options for saving and exporting the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 16bit for VLLM deployment\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model-16bit\", tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "# Save to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model-4bit\", tokenizer, save_method=\"merged_4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to GGUF for llama.cpp / Ollama\n",
    "if False:\n",
    "    # Q8_0 - Fast conversion, high quality\n",
    "    model.save_pretrained_gguf(\"model-gguf\", tokenizer)\n",
    "\n",
    "if False:\n",
    "    # Q4_K_M - Recommended balance of size/quality\n",
    "    model.save_pretrained_gguf(\"model-gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "if False:\n",
    "    # Multiple quantizations at once\n",
    "    model.save_pretrained_gguf(\n",
    "        \"model-gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Integration\n",
    "\n",
    "To use with Ollama after GGUF export:\n",
    "\n",
    "```bash\n",
    "# Create Modelfile\n",
    "cat > Modelfile << 'EOF'\n",
    "FROM ./model-gguf-Q4_K_M.gguf\n",
    "\n",
    "SYSTEM \"\"\"You are a Marxist-Leninist assistant trained on ProleWiki and critical theory.\n",
    "Think through political theory questions using dialectical materialist analysis.\n",
    "Show your reasoning in <think> tags, then provide a clear, well-sourced answer.\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_k 50\n",
    "PARAMETER top_p 0.9\n",
    "EOF\n",
    "\n",
    "# Create and run\n",
    "ollama create marxist-assistant -f Modelfile\n",
    "ollama run marxist-assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "**Next steps:**\n",
    "1. Test the model with various political theory questions\n",
    "2. Export to GGUF if satisfied with results\n",
    "3. Create Ollama Modelfile for deployment\n",
    "4. Consider extended training (more steps) for better results\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [TRL GRPO Documentation](https://huggingface.co/docs/trl/main/en/grpo_trainer)\n",
    "- [ProleWiki](https://en.prolewiki.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
