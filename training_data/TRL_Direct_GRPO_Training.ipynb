{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marxist-Leninist GRPO Training (TRL Direct)\n",
    "\n",
    "This notebook fine-tunes **DeepSeek-R1-0528-Qwen3-8B** on the ProleWiki corpus using GRPO (Group Relative Policy Optimization).\n",
    "\n",
    "**This version uses TRL directly** without Unsloth, avoiding torch.compile issues that cause hangs on RunPod/Jupyter.\n",
    "\n",
    "**Goal:** Train the model to reason through political theory questions using dialectical materialist analysis, showing reasoning in `<think>` tags.\n",
    "\n",
    "**Hardware:** Optimized for A40 (48GB VRAM)\n",
    "\n",
    "**Dataset:** 1,058 Q&A pairs from ProleWiki covering:\n",
    "- Revisionism and opportunism\n",
    "- Dialectical and historical materialism\n",
    "- Anti-colonial theory (Fanon, Rodney, Nkrumah)\n",
    "- Revolutionary theory (Jackson, Sankara, PFLP)\n",
    "- Marxist political economy\n",
    "\n",
    "---\n",
    "\n",
    "**Stack:**\n",
    "- [TRL GRPOTrainer](https://huggingface.co/docs/trl/main/en/grpo_trainer) - HuggingFace's RL training library\n",
    "- [PEFT](https://huggingface.co/docs/peft) - Parameter-Efficient Fine-Tuning (LoRA)\n",
    "- [vLLM](https://github.com/vllm-project/vllm) - Fast inference for generation\n",
    "- [transformers](https://huggingface.co/docs/transformers) - Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers accelerate\n",
    "!pip install trl[vllm]  # TRL with vLLM support\n",
    "!pip install peft bitsandbytes\n",
    "!pip install datasets\n",
    "!pip install sentence-transformers numpy\n",
    "\n",
    "# spaCy with TRANSFORMER model for better semantic understanding\n",
    "!pip install spacy spacy-curated-transformers\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We load the DeepSeek-R1-0528-Qwen3-8B model using standard HuggingFace transformers.\n",
    "\n",
    "**Key differences from Unsloth:**\n",
    "- Uses `AutoModelForCausalLM` instead of `FastLanguageModel`\n",
    "- No torch.compile optimization (avoids hanging)\n",
    "- Standard PyTorch gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\"  # Or use deepseek-ai/ version\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LORA_RANK = 32\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"left\"  # Required for GRPO\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model in bfloat16 (16-bit for GRPO)\n",
    "# For 4-bit quantization, uncomment the bnb_config below\n",
    "\n",
    "# Optional: 4-bit quantization (saves VRAM but may affect quality)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=bnb_config,  # Uncomment for 4-bit\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config.model_type}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Using PEFT library directly instead of Unsloth's wrapper.\n",
    "\n",
    "**Gradient Checkpointing:** We use standard PyTorch `gradient_checkpointing_enable()` which is stable across all environments (no torch.compile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_RANK,  # Same as r for GRPO (scaling = 1.0)\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient checkpointing (standard PyTorch, no torch.compile)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Template\n",
    "\n",
    "Verify the chat template works correctly for DeepSeek-R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our system prompt for Marxist-Leninist reasoning\n",
    "SYSTEM_PROMPT = \"\"\"You are a Marxist-Leninist assistant trained on ProleWiki and critical theory.\n",
    "Think through political theory questions using dialectical materialist analysis.\n",
    "Show your reasoning in <think> tags, then provide a clear, well-sourced answer.\"\"\"\n",
    "\n",
    "# Test chat template\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is revisionism?\"},\n",
    "]\n",
    "\n",
    "print(\"Chat template test:\")\n",
    "print(tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the GRPO-formatted dataset from `grpo_dataset.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "DATA_PATH = Path(\"grpo_dataset.jsonl\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found: {DATA_PATH}\\n\" \"Run 'python transform_to_grpo.py' first!\"\n",
    "    )\n",
    "\n",
    "dataset = Dataset.from_json(str(DATA_PATH))\n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Show sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nSample prompt: {sample['prompt'][1]['content'][:80]}...\")\n",
    "print(f\"Sample answer: {sample['answer'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Functions\n",
    "\n",
    "GRPO uses reward functions to guide the model toward desired behaviors.\n",
    "\n",
    "These are the same reward functions from the Unsloth notebook - they're already TRL-compatible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Reasoning format tokens\n",
    "REASONING_START = \"<think>\"\n",
    "REASONING_END = \"</think>\"\n",
    "\n",
    "# Regex to match format\n",
    "SOLUTION_END_REGEX = re.compile(rf\"{REASONING_END}(.*)\", re.DOTALL)\n",
    "\n",
    "# Lazy-load models to avoid loading at import time\n",
    "_embedder = None\n",
    "_nli_pipeline = None\n",
    "_spacy_nlp = None\n",
    "\n",
    "\n",
    "def get_embedder():\n",
    "    \"\"\"Get or initialize the sentence transformer embedder.\"\"\"\n",
    "    global _embedder\n",
    "    if _embedder is None:\n",
    "        print(\"[Reward] Loading sentence-transformers embedder...\")\n",
    "        _embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    return _embedder\n",
    "\n",
    "\n",
    "def get_nli_pipeline():\n",
    "    \"\"\"Get or initialize the NLI pipeline (BART-large-MNLI).\"\"\"\n",
    "    global _nli_pipeline\n",
    "    if _nli_pipeline is None:\n",
    "        print(\"[Reward] Loading NLI model (bart-large-mnli)...\")\n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        _nli_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"facebook/bart-large-mnli\",\n",
    "            device=device,\n",
    "        )\n",
    "    return _nli_pipeline\n",
    "\n",
    "\n",
    "def get_spacy_nlp():\n",
    "    \"\"\"Get or initialize spaCy NLP pipeline.\"\"\"\n",
    "    global _spacy_nlp\n",
    "    if _spacy_nlp is None:\n",
    "        import spacy\n",
    "\n",
    "        models_to_try = [\"en_core_web_trf\", \"en_core_web_md\", \"en_core_web_sm\"]\n",
    "\n",
    "        for model_name in models_to_try:\n",
    "            try:\n",
    "                print(f\"[Reward] Loading spaCy model: {model_name}...\")\n",
    "                _spacy_nlp = spacy.load(model_name)\n",
    "                print(f\"[Reward] Loaded {model_name} successfully\")\n",
    "                break\n",
    "            except OSError:\n",
    "                print(f\"[Reward] {model_name} not found, trying next...\")\n",
    "                continue\n",
    "\n",
    "        if _spacy_nlp is None:\n",
    "            raise OSError(\"No spaCy model found!\")\n",
    "    return _spacy_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marxist terminology for vocabulary reward\n",
    "MARXIST_TERMS = {\n",
    "    # Core concepts\n",
    "    \"dialectical\",\n",
    "    \"materialism\",\n",
    "    \"historical materialism\",\n",
    "    \"dialectical materialism\",\n",
    "    # Classes\n",
    "    \"bourgeoisie\",\n",
    "    \"proletariat\",\n",
    "    \"petty bourgeois\",\n",
    "    \"petty bourgeoisie\",\n",
    "    \"lumpenproletariat\",\n",
    "    \"working class\",\n",
    "    \"ruling class\",\n",
    "    # Class struggle\n",
    "    \"class struggle\",\n",
    "    \"class consciousness\",\n",
    "    \"class war\",\n",
    "    \"class conflict\",\n",
    "    # Political economy\n",
    "    \"surplus value\",\n",
    "    \"commodity\",\n",
    "    \"use value\",\n",
    "    \"exchange value\",\n",
    "    \"labor power\",\n",
    "    \"means of production\",\n",
    "    \"relations of production\",\n",
    "    \"forces of production\",\n",
    "    \"mode of production\",\n",
    "    \"primitive accumulation\",\n",
    "    \"exploitation\",\n",
    "    \"capital accumulation\",\n",
    "    # Imperialism\n",
    "    \"imperialism\",\n",
    "    \"colonialism\",\n",
    "    \"neo-colonialism\",\n",
    "    \"settler colonialism\",\n",
    "    \"national liberation\",\n",
    "    \"self-determination\",\n",
    "    # State and revolution\n",
    "    \"dictatorship of the proletariat\",\n",
    "    \"vanguard\",\n",
    "    \"vanguard party\",\n",
    "    \"democratic centralism\",\n",
    "    \"withering away of the state\",\n",
    "    \"proletarian dictatorship\",\n",
    "    # Ideology\n",
    "    \"hegemony\",\n",
    "    \"superstructure\",\n",
    "    \"base\",\n",
    "    \"ideology\",\n",
    "    \"false consciousness\",\n",
    "    # Revisionism\n",
    "    \"revisionism\",\n",
    "    \"opportunism\",\n",
    "    \"reformism\",\n",
    "    \"social democracy\",\n",
    "    \"ultra-leftism\",\n",
    "    # Alienation\n",
    "    \"alienation\",\n",
    "    \"fetishism\",\n",
    "    \"commodity fetishism\",\n",
    "    \"reification\",\n",
    "    # Historical\n",
    "    \"paris commune\",\n",
    "    \"october revolution\",\n",
    "    \"bolshevik\",\n",
    "    \"menshevik\",\n",
    "    # Anti-colonial\n",
    "    \"decolonization\",\n",
    "    \"third world\",\n",
    "    \"global south\",\n",
    "    \"national bourgeoisie\",\n",
    "    \"comprador\",\n",
    "}\n",
    "\n",
    "# Discourse connectives indicating logical structure\n",
    "DISCOURSE_CONNECTIVES = {\n",
    "    \"because\",\n",
    "    \"therefore\",\n",
    "    \"thus\",\n",
    "    \"hence\",\n",
    "    \"consequently\",\n",
    "    \"however\",\n",
    "    \"although\",\n",
    "    \"whereas\",\n",
    "    \"nevertheless\",\n",
    "    \"moreover\",\n",
    "    \"furthermore\",\n",
    "    \"additionally\",\n",
    "    \"specifically\",\n",
    "    \"namely\",\n",
    "    \"as a result\",\n",
    "    \"due to\",\n",
    "    \"in order to\",\n",
    "    \"so that\",\n",
    "    \"on the other hand\",\n",
    "    \"in contrast\",\n",
    "    \"similarly\",\n",
    "    \"likewise\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward +3.0 if response contains proper </think> tag.\n",
    "    This encourages the model to use the reasoning format.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        # Handle both string and conversational formats\n",
    "        if isinstance(completion, list):\n",
    "            response = completion[0][\"content\"] if completion else \"\"\n",
    "        else:\n",
    "            response = completion\n",
    "\n",
    "        score = 3.0 if SOLUTION_END_REGEX.search(response) else 0.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward partial format matching.\n",
    "    +0.5 for exactly one <think> tag\n",
    "    +0.5 for exactly one </think> tag\n",
    "    -1.0 for multiple or missing tags\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if isinstance(completion, list):\n",
    "            response = completion[0][\"content\"] if completion else \"\"\n",
    "        else:\n",
    "            response = completion\n",
    "\n",
    "        score = 0.0\n",
    "        start_count = response.count(REASONING_START)\n",
    "        end_count = response.count(REASONING_END)\n",
    "\n",
    "        score += 0.5 if start_count == 1 else -1.0\n",
    "        score += 0.5 if end_count == 1 else -1.0\n",
    "\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def completeness_reward(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward thorough, detailed responses.\n",
    "    Compares response length to ground truth length.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for completion, true_answer in zip(completions, answer, strict=False):\n",
    "        if isinstance(completion, list):\n",
    "            response = completion[0][\"content\"] if completion else \"\"\n",
    "        else:\n",
    "            response = completion\n",
    "\n",
    "        # Extract answer after </think>\n",
    "        if REASONING_END in response:\n",
    "            answer_part = response.split(REASONING_END, 1)[1].strip()\n",
    "        else:\n",
    "            answer_part = response\n",
    "\n",
    "        answer_len = len(answer_part.split())\n",
    "        true_len = len(true_answer.split())\n",
    "\n",
    "        if true_len == 0:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        ratio = answer_len / true_len\n",
    "\n",
    "        if 0.5 <= ratio <= 1.5:\n",
    "            score = 2.0\n",
    "        elif 0.3 <= ratio <= 2.0:\n",
    "            score = 1.0\n",
    "        elif ratio < 0.2:\n",
    "            score = -2.0\n",
    "        else:\n",
    "            score = -0.5\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nli_coherence_reward(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward responses that logically ENTAIL the ground truth answer.\n",
    "    Uses Natural Language Inference (facebook/bart-large-mnli).\n",
    "    \"\"\"\n",
    "    nli = get_nli_pipeline()\n",
    "    scores = []\n",
    "\n",
    "    for completion, true_answer in zip(completions, answer, strict=False):\n",
    "        if isinstance(completion, list):\n",
    "            response = completion[0][\"content\"] if completion else \"\"\n",
    "        else:\n",
    "            response = completion\n",
    "\n",
    "        # Extract answer part after </think>\n",
    "        if REASONING_END in response:\n",
    "            response = response.split(REASONING_END, 1)[1].strip()\n",
    "\n",
    "        # Handle empty or very short responses\n",
    "        if not response or len(response.strip()) < 20:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "\n",
    "        # Truncate to model max length\n",
    "        response_truncated = response[:512]\n",
    "        truth_truncated = true_answer[:512]\n",
    "\n",
    "        try:\n",
    "            input_text = f\"{response_truncated}</s></s>{truth_truncated}\"\n",
    "            result = nli(input_text)[0]\n",
    "            label = result[\"label\"].lower()\n",
    "\n",
    "            if label == \"entailment\":\n",
    "                score = 3.0\n",
    "            elif label == \"neutral\":\n",
    "                score = -1.0\n",
    "            else:  # contradiction\n",
    "                score = -3.0\n",
    "\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"[NLI Reward] Error: {e}\")\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def structural_coherence_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward responses with proper linguistic structure.\n",
    "    Uses spaCy dependency parsing.\n",
    "    \"\"\"\n",
    "    nlp = get_spacy_nlp()\n",
    "    scores = []\n",
    "\n",
    "    for completion in completions:\n",
    "        if isinstance(completion, list):\n",
    "            response = completion[0][\"content\"] if completion else \"\"\n",
    "        else:\n",
    "            response = completion\n",
    "\n",
    "        doc = nlp(response)\n",
    "        score = 0.0\n",
    "\n",
    "        # Check 1: Are there actual sentences?\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) < 1:\n",
    "            scores.append(-1.0)\n",
    "            continue\n",
    "\n",
    "        # Check 2: Marxist terms in meaningful syntactic roles\n",
    "        terms_in_context = 0\n",
    "        response_lower = response.lower()\n",
    "\n",
    "        for term in MARXIST_TERMS:\n",
    "            if term not in response_lower:\n",
    "                continue\n",
    "\n",
    "            for token in doc:\n",
    "                is_meaningful_role = token.dep_ in (\n",
    "                    \"nsubj\",\n",
    "                    \"nsubjpass\",\n",
    "                    \"dobj\",\n",
    "                    \"pobj\",\n",
    "                    \"attr\",\n",
    "                    \"appos\",\n",
    "                )\n",
    "                is_verb_root = token.head.pos_ == \"VERB\" and token.head.dep_ == \"ROOT\"\n",
    "                if term in token.text.lower() and (is_meaningful_role or is_verb_root):\n",
    "                    terms_in_context += 1\n",
    "                    break\n",
    "\n",
    "        score += min(terms_in_context * 0.3, 1.5)\n",
    "\n",
    "        # Check 3: Discourse connectives\n",
    "        connective_count = sum(1 for conn in DISCOURSE_CONNECTIVES if conn in response_lower)\n",
    "        score += min(connective_count * 0.2, 1.0)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_coherence_reward(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Complete coherence check combining:\n",
    "    1. NLI coherence (response entails ground truth)\n",
    "    2. Structural coherence (terms in proper syntactic roles)\n",
    "\n",
    "    This is the RECOMMENDED reward function for robust evaluation.\n",
    "    \"\"\"\n",
    "    nli_scores = nli_coherence_reward(completions, answer, **kwargs)\n",
    "    structure_scores = structural_coherence_reward(completions, **kwargs)\n",
    "\n",
    "    combined = []\n",
    "    for nli, structure in zip(nli_scores, structure_scores, strict=False):\n",
    "        if nli <= -3.0:\n",
    "            combined.append(-3.0)  # Contradiction dominates\n",
    "        else:\n",
    "            total = nli + (structure * 0.5)\n",
    "            combined.append(total)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug reward function for monitoring during training\n",
    "_PRINT_COUNTER = 0\n",
    "_PRINT_EVERY = 10\n",
    "\n",
    "\n",
    "def debug_print_reward(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Print sample outputs periodically for monitoring.\n",
    "    Returns 0.0 (no effect on training).\n",
    "    \"\"\"\n",
    "    global _PRINT_COUNTER\n",
    "\n",
    "    if _PRINT_COUNTER % _PRINT_EVERY == 0:\n",
    "        # Handle conversational format (use ternary for ruff SIM108)\n",
    "        question = prompts[0][-1][\"content\"] if isinstance(prompts[0], list) else prompts[0]\n",
    "        response = (\n",
    "            completions[0][0][\"content\"] if isinstance(completions[0], list) else completions[0]\n",
    "        )\n",
    "        true_answer = answer[0]\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Step {_PRINT_COUNTER}\")\n",
    "        print(f\"Question: {question[:100]}...\")\n",
    "        print(f\"Response: {response[:200]}...\")\n",
    "        print(f\"Expected: {true_answer[:100]}...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    _PRINT_COUNTER += 1\n",
    "\n",
    "    return [0.0] * len(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Configure GRPO training using TRL's `GRPOConfig`.\n",
    "\n",
    "**Key differences from Unsloth:**\n",
    "- Uses `GRPOConfig` directly instead of wrapper\n",
    "- vLLM configured via `use_vllm` and `vllm_mode`\n",
    "- No torch.compile optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_STEPS = 250\n",
    "SAVE_STEPS = 50\n",
    "LEARNING_RATE = 5e-6\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# A40 optimized batch settings\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION = 2\n",
    "NUM_GENERATIONS = 4\n",
    "\n",
    "# Sequence lengths\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "MAX_COMPLETION_LENGTH = 1500\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = \"outputs/marxist-grpo-trl\"\n",
    "LORA_OUTPUT = \"outputs/marxist-grpo-trl-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO training configuration\n",
    "training_args = GRPOConfig(\n",
    "    # Output\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_torch\",  # Use standard AdamW (not 8bit without bitsandbytes setup)\n",
    "    # Batch settings\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    # Sequence lengths\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    max_completion_length=MAX_COMPLETION_LENGTH,\n",
    "    # Training duration\n",
    "    max_steps=MAX_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    # Logging\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    # GRPO specific\n",
    "    temperature=1.0,  # For GRPO training dynamics\n",
    "    beta=0.0,  # No KL penalty (modern default)\n",
    "    scale_rewards=True,\n",
    "    # vLLM for fast generation\n",
    "    use_vllm=True,\n",
    "    vllm_mode=\"colocate\",  # Share GPU with training model\n",
    "    vllm_gpu_memory_utilization=0.5,  # Conservative to leave room for training\n",
    "    # Precision\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(\n",
    "    f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION}\"\n",
    ")\n",
    "print(f\"  Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(\"  vLLM mode: colocate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reward functions\n",
    "print(\"Initializing GRPO trainer with reward functions:\")\n",
    "print(\"  - match_format_exactly (+3.0 for </think>)\")\n",
    "print(\"  - match_format_approximately (tag validation)\")\n",
    "print(\"  - full_coherence_reward (NLI + structure)\")\n",
    "print(\"  - completeness_reward (length comparison)\")\n",
    "print(\"  - debug_print_reward (monitoring)\")\n",
    "print(\"\\nNote: First run will download NLI model (~1.6GB) + spaCy transformer (~436MB)\")\n",
    "\n",
    "reward_funcs = [\n",
    "    match_format_exactly,\n",
    "    match_format_approximately,\n",
    "    full_coherence_reward,\n",
    "    completeness_reward,\n",
    "    debug_print_reward,\n",
    "]\n",
    "\n",
    "# Create trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=reward_funcs,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "Run GRPO training. Monitor the `reward` column - it should increase over time.\n",
    "\n",
    "**Expected behavior:**\n",
    "- Steps 0-50: Format rewards stabilize\n",
    "- Steps 50-100: Semantic similarity improves\n",
    "- Steps 100-250: Content quality improves\n",
    "\n",
    "**Estimated time:** ~1-2 hours on A40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Steps: {MAX_STEPS}\")\n",
    "print(f\"Batch: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} x {NUM_GENERATIONS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LoRA\n",
    "\n",
    "Save the trained LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(LORA_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Save the PEFT adapter\n",
    "model.save_pretrained(LORA_OUTPUT)\n",
    "tokenizer.save_pretrained(LORA_OUTPUT)\n",
    "\n",
    "print(f\"LoRA saved to: {LORA_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify LoRA is actually trained (non-zero weights)\n",
    "import os\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "adapter_path = os.path.join(LORA_OUTPUT, \"adapter_model.safetensors\")\n",
    "if os.path.exists(adapter_path):\n",
    "    with safe_open(adapter_path, framework=\"pt\") as f:\n",
    "        for key in list(f.keys())[:5]:  # Check first 5 layers\n",
    "            tensor = f.get_tensor(key)\n",
    "            n_nonzero = (tensor != 0).sum().item()\n",
    "            print(f\"{key}: {n_nonzero}/{tensor.numel()} non-zero\")\n",
    "    print(\"\\nLoRA verification passed!\")\n",
    "else:\n",
    "    print(f\"Adapter not found at {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing\n",
    "\n",
    "Test the model with the trained LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    \"What is revisionism in the Marxist sense?\",\n",
    "    \"Explain the concept of surplus value.\",\n",
    "    \"What is the dictatorship of the proletariat?\",\n",
    "    \"How does dialectical materialism differ from idealism?\",\n",
    "]\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING WITH TRAINED LORA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in TEST_QUESTIONS[:2]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and Export (Optional)\n",
    "\n",
    "Merge LoRA weights into base model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA into base model\n",
    "if False:  # Set to True to merge\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_model.save_pretrained(\"outputs/marxist-grpo-merged\")\n",
    "    tokenizer.save_pretrained(\"outputs/marxist-grpo-merged\")\n",
    "    print(\"Merged model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "**Next steps:**\n",
    "1. Test the model with various political theory questions\n",
    "2. Merge LoRA if satisfied with results\n",
    "3. Convert to GGUF for Ollama deployment (use `llama.cpp`)\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- [TRL GRPOTrainer Documentation](https://huggingface.co/docs/trl/main/en/grpo_trainer)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [ProleWiki](https://en.prolewiki.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
